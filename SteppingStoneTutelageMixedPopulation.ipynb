{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Lowered epoch count to 10, Switched to internal sin, alpha=1/(2+(self.age/4)), Hypers: (60, 1, agent_hyper, 1/4, 1/4, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsMixedPopulation/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if train:\n",
    "                outputs.append(out(x))\n",
    "            else:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.3\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(state_size * stack_size, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()#nn.MSELoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=10):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        self.alpha = 1/(2+(self.age/4))\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                #outputs = [torch.stack(o) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                # Peer inputs and outputs for peer loss function implementation.\n",
    "                peer_inputs = [b for b in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_outputs = [[a for a in o] for o in batches[i][1]]\n",
    "                for o in peer_outputs:\n",
    "                    rand.shuffle(o)\n",
    "                peer_outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs.to(self.t_device), train=True)\n",
    "                peer_outs = self.policy_map(peer_inputs.to(self.t_device), train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor)\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        tensor = torch.cat(tensors, 0)\n",
    "                    groups.append((previous_state, action_queue, tensor.detach().cpu().numpy()))\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor)\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score, overall_step\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff, alpha):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        good_examples = []\n",
    "        bad_examples = []\n",
    "        for exp in teach_pop:\n",
    "            good_examples += exp[1]\n",
    "        for exp in new_pop[int(len(new_pop) * self.teach_percent):]:\n",
    "            bad_examples += exp[1]\n",
    "        examples = good_examples\n",
    "        manager.print('BEGIN TRAINING POPULATION | TRAINING EXAMPLES {}'.format(len(examples)))\n",
    "        count = 0\n",
    "        losses = []\n",
    "        if len(examples) > 0:\n",
    "            for train in train_pop:\n",
    "                agent = train[0]\n",
    "                if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                    agent = self.replace_agent(agent)\n",
    "                trajectories = []\n",
    "                for _ in range(int(len(examples) / 2)):\n",
    "                    index = rand.randint(0, len(examples) - 1)\n",
    "                    trajectories.append(examples[index])\n",
    "                inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "                loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=1024, epochs=10)\n",
    "                losses.append(loss)\n",
    "                count += 1\n",
    "            print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses) if len(losses) > 0 else 'NA'))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 1, 0.001, 0.95, 5, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(60, 1, agent_hyper, 1/4, 1/4, 10, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_22 | STEP 1336 | SCORE 600.0 | AGE 1 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1168.3333333333333 | LOW SCORE 0.0 | HIGH SCORE 8400.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 30880\n",
      "AGENT_58 | EPOCH 10/10 | BATCH 16/16 | CURRENT BATCH COUNT 80 | LOSS 1.4060 | AGE 1 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4791895417372385\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_3 | STEP 1195 | SCORE 500.0 | AGE 1 | MEMBER 60/60 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1983.3333333333333 | LOW SCORE 0.0 | HIGH SCORE 8800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 32854\n",
      "AGENT_41 | EPOCH 10/10 | BATCH 17/17 | CURRENT BATCH COUNT 43 | LOSS 1.4057 | AGE 1 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4694297898984423\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_28 | STEP 1898 | SCORE 1900.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2573.3333333333335 | LOW SCORE 200.0 | HIGH SCORE 8800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 33442\n",
      "AGENT_10 | EPOCH 10/10 | BATCH 17/17 | CURRENT BATCH COUNT 337 | LOSS 1.4332 | AGE 1 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4747338596044803\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_48 | STEP 1983 | SCORE 2200.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3110.0 | LOW SCORE 700.0 | HIGH SCORE 8800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 34405\n",
      "AGENT_21 | EPOCH 10/10 | BATCH 17/17 | CURRENT BATCH COUNT 818 | LOSS 1.4246 | AGE 1 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.497790053592009\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_13 | STEP 2059 | SCORE 5800.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3688.3333333333335 | LOW SCORE 1200.0 | HIGH SCORE 9300.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 34947\n",
      "AGENT_2 | EPOCH 10/10 | BATCH 18/18 | CURRENT BATCH COUNT 65 | LOSS 1.4450 | AGE 1 | MEMBER 15/15\t\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.549855782058504\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_34 | STEP 2535 | SCORE 5300.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3871.6666666666665 | LOW SCORE 900.0 | HIGH SCORE 10600.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 37014\n",
      "AGENT_45 | EPOCH 10/10 | BATCH 19/19 | CURRENT BATCH COUNT 75 | LOSS 1.3846 | AGE 2 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5281252744323328\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_20 | STEP 1624 | SCORE 2200.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4290.0 | LOW SCORE 1400.0 | HIGH SCORE 12200.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 36951\n",
      "AGENT_7 | EPOCH 10/10 | BATCH 19/19 | CURRENT BATCH COUNT 43 | LOSS 1.7355 | AGE 2 | MEMBER 15/15\t\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5330178002725567\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_32 | STEP 2017 | SCORE 4400.0 | AGE 3 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4678.333333333333 | LOW SCORE 1600.0 | HIGH SCORE 12000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 39976\n",
      "AGENT_46 | EPOCH 10/10 | BATCH 20/20 | CURRENT BATCH COUNT 532 | LOSS 1.5430 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5063475469748178\n",
      "BEGIN RUNNING POPULATION | GENERATION 8\n",
      "AGENT_0 | STEP 2475 | SCORE 7100.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4941.666666666667 | LOW SCORE 1700.0 | HIGH SCORE 12800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 41894\n",
      "AGENT_56 | EPOCH 10/10 | BATCH 21/21 | CURRENT BATCH COUNT 467 | LOSS 1.4953 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4566796121521601\n",
      "BEGIN RUNNING POPULATION | GENERATION 9\n",
      "AGENT_43 | STEP 2032 | SCORE 6300.0 | AGE 3 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5196.666666666667 | LOW SCORE 1800.0 | HIGH SCORE 12700.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 39223\n",
      "AGENT_33 | EPOCH 10/10 | BATCH 20/20 | CURRENT BATCH COUNT 155 | LOSS 1.2178 | AGE 2 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.451502799709638\n",
      "BEGIN RUNNING POPULATION | GENERATION 10\n",
      "AGENT_49 | STEP 315 | SCORE 0.0 | AGE 4 | MEMBER 29/60 | GAME 1/1\t\t/1\t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-03d9f3ff67e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpopulation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_population\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmanager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-1d1bff41e59a>\u001b[0m in \u001b[0;36mrun_population\u001b[1;34m(self, env, render)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mcandidate_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber_of_attempts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[0mtotal_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mcandidate_runs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c38f29eb9979>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, env, render, extra_info)\u001b[0m\n\u001b[0;32m    129\u001b[0m                                 \u001b[0md\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                             \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                             \u001b[0maction_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverall_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max9613\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0msample_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mparam_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mprobs_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_events\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0msample_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible next test, try on trajectories before labeling rather than after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
