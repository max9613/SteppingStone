{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Cross Loss Test, Hypers: (40, 1, agent_hyper, 1/4, 1/4, 5)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experiments/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if not train:\n",
    "                outputs.append(self.sigmoid(out(x)))\n",
    "            else:\n",
    "                outputs.append(out(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.1\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(state_size * stack_size, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        #self.loss_func = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=50):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                self.optimizer.zero_grad()\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                #outputs = [torch.stack(o).to(self.t_device) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                # Peer inputs and outputs for peer loss function implementation.\n",
    "                peer_inputs = [b for b in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_outputs = [[a for a in o] for o in batches[i][1]]\n",
    "                for a in range(len(peer_outputs)):\n",
    "                    rand.shuffle(peer_outputs[a])\n",
    "                #peer_outputs = [torch.stack(o).to(self.t_device) for o in peer_outputs]\n",
    "                peer_outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs.to(self.t_device), train=True)\n",
    "                peer_outs = self.policy_map(peer_inputs.to(self.t_device), train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = observation\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor.to(self.t_device))\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = observation\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    groups.append((previous_state, action_queue))\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        tensor = torch.cat(tensors, 0)\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor.to(self.t_device))\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('\\n')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                agent = self.replace_agent(agent)\n",
    "            trajectories = []\n",
    "            for _ in range(int(len(examples) / 2)):\n",
    "                index = rand.randint(0, len(examples) - 1)\n",
    "                trajectories.append(examples[index])\n",
    "            inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "            loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=1024, epochs=50)\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('\\n')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 1, 0.0001, 0.95, 5, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(40, 1, agent_hyper, 1/4, 1/4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_3 | STEP 1366 | SCORE 500.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 530.0 | LOW SCORE 100.0 | HIGH SCORE 1500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_22 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 613 | LOSS 2.2001 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.333452543939863\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_0 | STEP 1308 | SCORE 500.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 612.5 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_19 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 546 | LOSS 1.9879 | AGE 2 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.31754156763213\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_34 | STEP 1445 | SCORE 1400.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 532.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_14 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 297 | LOSS 2.0227 | AGE 2 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.3267073347568514\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_20 | STEP 1565 | SCORE 1000.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 600.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_24 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 630 | LOSS 2.1912 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.2578100568907606\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_17 | STEP 1150 | SCORE 1100.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 467.5 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_39 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 916 | LOSS 2.1946 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.3045792046955653\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_20 | STEP 1368 | SCORE 200.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 545.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_26 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 233 | LOSS 2.2088 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.3291324058771137\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_16 | STEP 1165 | SCORE 300.0 | AGE 4 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 575.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_25 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 690 | LOSS 2.1704 | AGE 2 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.2396455556665145\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_4 | STEP 1247 | SCORE 700.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 560.0 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_14 | EPOCH 24/50 | BATCH 1/7 | CURRENT BATCH COUNT 1024 | LOSS 2.1432 | AGE 4 | MEMBER 10/10\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
