{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Hypers: (40, 1, agent_hypers, 1/4, 1/4, 1000)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsRNN/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN policy map.\n",
    "class POLICY_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_hidden_count = pre_hidden_count\n",
    "        self.post_hidden_count = post_hidden_count\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        weights = generate_weights(input_size, hidden_size, pre_hidden_count)\n",
    "        self.pre_hidden_layers = []\n",
    "        prev_weight = input_size\n",
    "        for w in weights:\n",
    "            self.pre_hidden_layers.append(nn.Linear(prev_weight, w).to(t_device))\n",
    "            prev_weight = w\n",
    "        self.pre_hidden_layers.append(nn.Linear(prev_weight, hidden_size).to(t_device))\n",
    "        self.post_hidden_layers = []\n",
    "        for _ in range(post_hidden_count):\n",
    "            self.post_hidden_layers.append(nn.Linear(hidden_size, hidden_size).to(t_device))\n",
    "        self.policy_out = nn.Linear(hidden_size, class_count).to(t_device)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.parameters = []\n",
    "        for layer in self.pre_hidden_layers + self.post_hidden_layers + [self.policy_out]:\n",
    "            self.parameters += list(layer.parameters())\n",
    "        \n",
    "    # Forward propogation.\n",
    "    def forward(self, states, hidden=None, train=False):\n",
    "        outs = []\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.hidden_size).to(self.t_device)\n",
    "        for x in states:\n",
    "            for layer in self.pre_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            x += hidden\n",
    "            for layer in self.post_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            hidden = x\n",
    "            if train:\n",
    "                outs.append(self.policy_out(x))\n",
    "            else:\n",
    "                outs.append(self.softmax(self.policy_out(x) * 10))\n",
    "        if train:\n",
    "            return outs\n",
    "        else:\n",
    "            return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Agent that plays the game.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, learning_rate, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.policy_map = POLICY_MAP(input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.parameters, lr=learning_rate)\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.age = 0\n",
    "        self.alpha = 1\n",
    "        \n",
    "    # Train the agent.\n",
    "    # Input data should be a list of trajectories, which should be of form [[states], [actions]].\n",
    "    def train(self, trajectories, unroll_depth, batch_size=32, epochs=50, extra_info=''):\n",
    "        batches = []\n",
    "        batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "        rand.shuffle(trajectories)\n",
    "        count = 0\n",
    "        for t in trajectories:\n",
    "            preds = self.policy_map(t[0], train=True)\n",
    "            for i in range(unroll_depth):\n",
    "                batch[0][i].append(t[0][i])\n",
    "                batch[1][i].append(t[1][i])\n",
    "            count += 1\n",
    "            if count >= batch_size:\n",
    "                batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "                batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "                preds = self.policy_map(batch[0], train=True)\n",
    "                #batch[2] = [torch.Tensor([np.argmax(vector.detach().cpu().numpy()) for vector in pred]).long().to(self.t_device) for pred in preds]\n",
    "                batch[2] = [torch.Tensor(np.argmax(pred.detach().cpu().numpy(), axis=1)).long().to(self.t_device) for pred in preds]\n",
    "                batches.append(batch)\n",
    "                batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "                count = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                inputs = batch[0]\n",
    "                targets = batch[1]\n",
    "                self_targets = batch[2]\n",
    "                outputs = self.policy_map(inputs, train=True)\n",
    "                loss = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    loss += self.cross_loss(outputs[i], targets[i])\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                for i in range(len(outputs)):\n",
    "                    loss -= self.alpha * self.cross_loss(outputs[i], self_targets[i])\n",
    "                self.optimizer.step()\n",
    "                print('\\rTRAINING {} | AGE {} | BATCH {}/{} | EPOCH {}/{} | LOSS {} | {}'.format(self.name, self.age, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "    \n",
    "    # Plays the game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        hidden_state = None\n",
    "        groups = []\n",
    "        group = []\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, step, score, self.age, extra_info), end = '')\n",
    "            score += reward\n",
    "            step += 1\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policies, hidden_state = self.policy_map([tensor], hidden_state)\n",
    "            policy = policies[0].to(self.s_device)\n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.class_count - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            group.append((tensor, action))\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                lives = info['ale.lives']\n",
    "                hidden_state = None\n",
    "                group = []\n",
    "            if render:\n",
    "                env.render()\n",
    "        return groups, score, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "    \n",
    "    # Sorts the trajectories into ones for the greedy head and non-greedy head.\n",
    "    def prepare_data(self, groups, unroll_depth):\n",
    "        trajectories = []\n",
    "        for total_run in groups:\n",
    "            index = unroll_depth\n",
    "            while index < len(total_run):\n",
    "                actions = []\n",
    "                states = []\n",
    "                for group in total_run[index-unroll_depth:index]:\n",
    "                    actions.append(group[1])\n",
    "                    states.append(group[0])\n",
    "                trajectories.append([states, actions])\n",
    "                index += 1\n",
    "        return trajectories\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, unroll_depth, epochs, batch_size, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        trajectories = self.prepare_data(examples, unroll_depth)\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            #if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "            #    agent = self.replace_agent(agent)\n",
    "            loss = agent.train(trajectories, unroll_depth, batch_size, epochs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}')\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        manager.save()\n",
    "        self.generation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hypers = (10**-4, 128, 80, 1, 4, 14, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(40, 1, agent_hypers, 1/4, 1/4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_13 | STEP 1027 | SCORE 0.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 732.5 | LOW SCORE 0.0 | HIGH SCORE 2200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_13 | AGE 0 | BATCH 56/56 | EPOCH 10/10 | LOSS 3.3383233547210693 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 2.773164701036045\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_26 | STEP 1260 | SCORE 100.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 695.0 | LOW SCORE 0.0 | HIGH SCORE 2700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_12 | AGE 0 | BATCH 61/61 | EPOCH 10/10 | LOSS 2.957956314086914 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 2.833677472052027\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_20 | STEP 1291 | SCORE 900.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 627.5 | LOW SCORE 0.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_14 | AGE 0 | BATCH 52/52 | EPOCH 10/10 | LOSS 3.2920234203338623 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 2.8999607998591204\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_16 | STEP 1809 | SCORE 1300.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 640.0 | LOW SCORE 0.0 | HIGH SCORE 2300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_29 | AGE 2 | BATCH 60/60 | EPOCH 10/10 | LOSS 2.2897183895111084 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 2.7678158322970075\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_28 | STEP 1347 | SCORE 300.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 702.5 | LOW SCORE 0.0 | HIGH SCORE 2300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_17 | AGE 1 | BATCH 61/61 | EPOCH 10/10 | LOSS 2.5901222229003906 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 2.9329631125340696\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_4 | STEP 1281 | SCORE 500.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 695.0 | LOW SCORE 0.0 | HIGH SCORE 4500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_28 | AGE 1 | BATCH 58/58 | EPOCH 10/10 | LOSS 3.510709047317505 | | MEMBER 7/100"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, 20, 10, 256, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
