{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Changed action map combination to subtraction from concatonation, Hypers: (32, 20, 128, 14, 1, (6, 1, 1, 2), 10**-4, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsHierarchy/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese embedding network at the heart of this model, that finds the controllable state.\n",
    "class EMBEDDING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        return self.sin(self.output_layer(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action predictor based on two embedded states.\n",
    "class ACTION_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, train=False):\n",
    "        #x = torch.cat([in_1, in_2], -1)\n",
    "        x = in_2 - in_1\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating network based on two embedded states.\n",
    "class RATING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, final_activation=False):\n",
    "        x = torch.cat([in_1, in_2], -1)\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if not final_activation:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given embedded state, outputs the desired next embedded state.\n",
    "class STRATEGY_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        return self.sin(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that plays a given game and attempts to achieve a maximum score.\n",
    "# This implementation will only use one agent that contains both selector and strategy networks.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, embedding_size, strategy_count, state_size, action_size, stack_size, layer_counts, learning_rate, gamma, teach_percent, s_device, t_device):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.strategy_count = strategy_count\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.stack_size = stack_size\n",
    "        self.layer_counts = layer_counts\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.teach_percent = teach_percent\n",
    "        self.age = 0\n",
    "        self.s_device = s_device\n",
    "        self.t_device = t_device\n",
    "        # Maps that form the model.\n",
    "        self.embedding_map = EMBEDDING_MAP(state_size * stack_size, embedding_size, layer_counts[0], t_device)\n",
    "        self.action_map = ACTION_MAP(embedding_size, action_size, layer_counts[1], t_device)\n",
    "        self.greedy_rating_map = RATING_MAP(embedding_size * 2, 2, layer_counts[2], t_device)\n",
    "        self.exploratory_rating_map = RATING_MAP(embedding_size * 2, 2, layer_counts[2], t_device)\n",
    "        self.strategies = [\n",
    "            STRATEGY_MAP(embedding_size, embedding_size, layer_counts[3], t_device) for _ in range(strategy_count)\n",
    "        ]\n",
    "        # Optimizers for maps.\n",
    "        self.action_embedding_optimizer = torch.optim.Adam(self.action_map.params + self.embedding_map.params, lr=learning_rate)\n",
    "        self.greedy_rating_optimizer = torch.optim.Adam(self.greedy_rating_map.params, lr=learning_rate)\n",
    "        self.exploratory_rating_optimizer = torch.optim.Adam(self.exploratory_rating_map.params, lr=learning_rate)\n",
    "        self.strategy_optimizers = [\n",
    "            torch.optim.Adam(s.params, lr=learning_rate) for s in self.strategies\n",
    "        ]\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.alpha = 1/self.action_size\n",
    "        self.exploitation_weight = 0.7\n",
    "        self.exploration_weight = 0.4\n",
    "        \n",
    "    # Trains the action and embedding maps together, this is the only place the embedding map should be trained.\n",
    "    # Groups should be of the form (state (i), state (i+1), action (i))\n",
    "    def train_action_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.action_map(self.embedding_map(inputs_1, True), self.embedding_map(inputs_2, True), True)\n",
    "                peer_out = self.action_map(self.embedding_map(peer_inputs_1, True), self.embedding_map(peer_inputs_2, True), True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rACTION MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.action_embedding_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the greedy rating map.\n",
    "    # Groups should be of the form (state (i), state (i+1), label (i))\n",
    "    def train_greedy_rating_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.greedy_rating_map(self.embedding_map(inputs_1, True).detach(), self.embedding_map(inputs_2, True).detach(), False)\n",
    "                peer_out = self.greedy_rating_map(self.embedding_map(peer_inputs_1, True).detach(), self.embedding_map(peer_inputs_2, True).detach(), False)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rGREEDY RATING MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.greedy_rating_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the exploratory rating map.\n",
    "    # Groups should be of the form (state (i), state (i+1), label (i))\n",
    "    def train_exploratory_rating_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.exploratory_rating_map(self.embedding_map(inputs_1, True).detach(), self.embedding_map(inputs_2, True).detach(), False)\n",
    "                peer_out = self.exploratory_rating_map(self.embedding_map(peer_inputs_1, True).detach(), self.embedding_map(peer_inputs_2, True).detach(), False)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rEXPLORATORY RATING MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.exploratory_rating_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the strategy at the passed index.\n",
    "    # Groups should be of the form (state (i), state (i+1))\n",
    "    def train_strategy_map(self, index, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            #batch[1].append(groups[position][1])\n",
    "            batch[1].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                inputs = self.embedding_map(inputs).detach()\n",
    "                peer_inputs = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_inputs = self.embedding_map(peer_inputs).detach()\n",
    "                peer_outputs = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                #peer_outputs = self.embedding_map(peer_outputs).detach()\n",
    "                outputs = torch.Tensor(batches[i][1]).long()\n",
    "                #outputs = self.embedding_map(outputs)\n",
    "                out = self.action_map(inputs, self.strategies[index](inputs), True)\n",
    "                peer_out = self.action_map(peer_inputs, self.strategies[index](peer_inputs), True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rSTRATEGY {}/{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(index+1, len(self.strategies), e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.strategy_optimizers[index].step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Returns labeled examples for a given set, based on a threshold.\n",
    "    def label_groups(self, groups):\n",
    "        groups.sort(key = lambda x: x[2], reverse=True)\n",
    "        good_examples = groups[:int(len(groups) * self.teach_percent)]\n",
    "        bad_examples = groups[int(len(groups) * self.teach_percent):]\n",
    "        examples = []\n",
    "        for group in good_examples:\n",
    "            examples.append((group[0], group[1], 1))\n",
    "        for group in bad_examples:\n",
    "            examples.append((group[0], group[1], 0))\n",
    "        rand.shuffle(examples)\n",
    "        return examples\n",
    "        \n",
    "    \n",
    "    # Plays a single game on a compatible enviroment and returns the score and trajectories.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_tensor = None\n",
    "        previous_action = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            previous_action = action\n",
    "            state = observation / 255\n",
    "            frames.append(torch.Tensor.float(torch.from_numpy(state)))\n",
    "            if len(frames) < self.stack_size:\n",
    "                action = rand.randint(0, self.action_size - 1)    \n",
    "            else:\n",
    "                tensor = torch.cat(frames[-self.stack_size:], 0)\n",
    "                embedding = self.embedding_map(tensor).detach()\n",
    "                suggestions = torch.stack([strat(embedding) for strat in self.strategies])\n",
    "                current_copies = torch.stack([embedding for _ in range(len(self.strategies))])\n",
    "                greedy_ratings = self.greedy_rating_map(current_copies, suggestions, False).detach().cpu().numpy()\n",
    "                exploratory_ratings = self.exploratory_rating_map(current_copies, suggestions, False).detach().cpu().numpy()\n",
    "                ratings = []\n",
    "                for i in range(len(greedy_ratings)):\n",
    "                    rating = 0\n",
    "                    if np.argmax(greedy_ratings[i]) == 1:\n",
    "                        rating += self.exploitation_weight\n",
    "                    if np.argmax(exploratory_ratings[i]) == 1:\n",
    "                        rating += self.exploration_weight\n",
    "                    ratings.append(rating)\n",
    "                chosen = suggestions[np.argmax(ratings) if sum(ratings) > 0 else rand.randint(0,len(suggestions)-1)]\n",
    "                policy = self.action_map(embedding, chosen)\n",
    "                if sum(policy) <= 0 or min(policy) < 0 or rand.uniform(0,1) < self.gamma:\n",
    "                    action = rand.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                    action = (int(distribution.sample()))\n",
    "                if previous_tensor is not None:\n",
    "                    groups.append((previous_tensor, tensor, previous_action))\n",
    "                previous_tensor = tensor\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_tensor = None\n",
    "                previous_action = None\n",
    "                action = 0\n",
    "                frames = []\n",
    "            step += 1\n",
    "            score += reward\n",
    "            print('\\rSTEP {} | SCORE {} | {}\\t\\t'.format(step, score, extra_info), end = '')\n",
    "        return groups, score\n",
    "    \n",
    "    # Plays a single game on a compatible enviroment with the provided strategy and returns the score and trajectories.\n",
    "    def play_game_with_strategy(self, env, strategy, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_tensor = None\n",
    "        previous_action = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            previous_action = action\n",
    "            state = observation / 255\n",
    "            frames.append(torch.Tensor.float(torch.from_numpy(state)))\n",
    "            if len(frames) < self.stack_size:\n",
    "                action = rand.randint(0, self.action_size - 1)    \n",
    "            else:\n",
    "                tensor = torch.cat(frames[-self.stack_size:], 0)\n",
    "                embedding = self.embedding_map(tensor).detach()\n",
    "                suggestion = self.strategies[strategy](embedding)\n",
    "                policy = self.action_map(embedding, suggestion)\n",
    "                if sum(policy) <= 0 or min(policy) < 0 or rand.uniform(0,1) < self.gamma:\n",
    "                    action = rand.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                    action = (int(distribution.sample()))\n",
    "                if previous_tensor is not None:\n",
    "                    groups.append((previous_tensor, tensor, previous_action))\n",
    "                previous_tensor = tensor\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_tensor = None\n",
    "                previous_action = None\n",
    "                action = 0\n",
    "                frames = []\n",
    "            step += 1\n",
    "            score += reward\n",
    "            print('\\rSTEP {} | SCORE {} | {}\\t\\t'.format(step, score, extra_info), end = '')\n",
    "        return groups, score\n",
    "    \n",
    "    # Runs the agent on the provided enviroment.\n",
    "    def run(self, env, games, loop, render = False):\n",
    "        running = True\n",
    "        while running:\n",
    "            if not loop:\n",
    "                running = False\n",
    "            # Game playing section.\n",
    "            initial_groups = []\n",
    "            total_score = 0\n",
    "            high_score = None\n",
    "            low_score = None\n",
    "            if games is not None:\n",
    "                for g in range(games):\n",
    "                    group, score = self.play_game(env,render,f'GAME {g+1}/{games}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            else:\n",
    "                for s in range(len(self.strategies)):\n",
    "                    group, score = self.play_game_with_strategy(env,s,render,f'GAME {s+1}/{len(self.strategies)}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            avg_score = total_score / len(initial_groups)\n",
    "            print('')\n",
    "            manager.print(f'END PLAY | AVERAGE SCORE {avg_score} | LOW SCORE {low_score} | HIGH SCORE {high_score}')\n",
    "            manager.save()\n",
    "            _, s_score = self.play_game(env,render,'')\n",
    "            print('')\n",
    "            manager.print(f'SELECTOR SCORE {s_score}')\n",
    "            manager.save()\n",
    "            # Training action and embedding maps section.\n",
    "            action_groups = []\n",
    "            for group in initial_groups:\n",
    "                action_groups += group[0]\n",
    "            a_e_loss = self.train_action_map(action_groups)\n",
    "            print('')\n",
    "            manager.print(f'END ACTION/EMBEDDING TRAINING | AVERAGE LOSS {a_e_loss}')\n",
    "            manager.save()\n",
    "            # Training greedy and exploratory rating maps section. \n",
    "            score_groups = [] # Groups should be of form (state (i), state (i+1), score)\n",
    "            loss_groups = [] # Groups should be of form (state (i), state (i+1), loss)\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    score_groups.append((pairs[0], pairs[1], group[1]))\n",
    "                    in_1 = self.embedding_map(pairs[0])\n",
    "                    in_2 = self.embedding_map(pairs[1])\n",
    "                    out = torch.stack([self.action_map(in_1, in_2, True)])\n",
    "                    # This might not run due to using cross loss comparing a 1D array to a single scalar target.\n",
    "                    label = torch.Tensor([pairs[2]]).long()\n",
    "                    loss = self.cross_loss(out, label).detach().cpu().numpy()\n",
    "                    loss_groups.append((pairs[0], pairs[1], loss))\n",
    "            score_groups = self.label_groups(score_groups)\n",
    "            loss_groups = self.label_groups(loss_groups)\n",
    "            g_r_loss = self.train_greedy_rating_map(score_groups)\n",
    "            print('')\n",
    "            manager.print(f'END GREEDY RATING TRAINING | AVERAGE LOSS {g_r_loss}')\n",
    "            manager.save()\n",
    "            e_r_loss = self.train_exploratory_rating_map(loss_groups)\n",
    "            print('')\n",
    "            manager.print(f'END EXPLORATORY RATING TRAINING | AVERAGE LOSS {e_r_loss}')\n",
    "            manager.save()\n",
    "            # Training strategies section.\n",
    "            strategies_training_groups = [] # Groups should be of form (state (i), state (i+1))\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    in_1 = self.embedding_map(pairs[0])\n",
    "                    in_2 = self.embedding_map(pairs[1])\n",
    "                    greedy_rating = self.greedy_rating_map(in_1, in_2, True).detach().cpu().numpy()\n",
    "                    exploratory_rating = self.exploratory_rating_map(in_1, in_2, True).detach().cpu().numpy()\n",
    "                    if np.argmax(greedy_rating) == 1: #or np.argmax(exploratory_rating) == 1:\n",
    "                        strategies_training_groups.append((pairs[0], pairs[1], pairs[2]))\n",
    "            losses = []\n",
    "            if len(strategies_training_groups) > 0:\n",
    "                for i in range(len(self.strategies)):\n",
    "                    training_groups = []\n",
    "                    while len(training_groups) < len(strategies_training_groups) / 3:\n",
    "                        index = rand.randint(0, len(strategies_training_groups) - 1)\n",
    "                        training_groups.append(strategies_training_groups[index])\n",
    "                    loss = self.train_strategy_map(i, training_groups)\n",
    "                    losses.append(loss)\n",
    "            avg_strat_loss = sum(losses)/len(losses) if len(losses) > 0 else 'NA'\n",
    "            print('')\n",
    "            manager.print(f'END STRATEGY TRAINING | AVERAGE LOSS {avg_strat_loss}')\n",
    "            manager.save()\n",
    "            self.age += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AGENT(32, 20, 128, 14, 1, (6, 1, 1, 2), 10**-4, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1345 | SCORE 800.0 | GAME 20/20\t\t\t\n",
      "END PLAY | AVERAGE SCORE 530.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "STEP 1262 | SCORE 200.0 | \t\t\n",
      "SELECTOR SCORE 200.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 26/26 | CURRENT BATCH COUNT 751 | LOSS 2.1496\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.3383417256061847\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 26/26 | CURRENT BATCH COUNT 751 | LOSS 0.5895\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.5979389434135877\n",
      "EXPLORATORY RATING MAP | EPOCH 50/50 | BATCH 26/26 | CURRENT BATCH COUNT 751 | LOSS 0.5815\t\t\t\n",
      "END EXPLORATORY RATING TRAINING | AVERAGE LOSS 0.6004140280301754\n",
      "STRATEGY 20/20 | EPOCH 50/50 | BATCH 1/1 | CURRENT BATCH COUNT 9 | LOSS 14.2940\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 11.420226209640502\n",
      "STEP 1285 | SCORE 400.0 | GAME 20/20\t\t\t\n",
      "END PLAY | AVERAGE SCORE 515.0 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "STEP 1054 | SCORE 600.0 | \t\t\n",
      "SELECTOR SCORE 600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 23/23 | CURRENT BATCH COUNT 1021 | LOSS 1.9460\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.0050600102673406\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 23/23 | CURRENT BATCH COUNT 1021 | LOSS 0.5899\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6119972850447115\n",
      "EXPLORATORY RATING MAP | EPOCH 4/50 | BATCH 13/23 | CURRENT BATCH COUNT 1024 | LOSS 0.6064\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    agent.run(env, None, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
