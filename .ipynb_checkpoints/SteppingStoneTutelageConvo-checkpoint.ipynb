{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Cross loss test, Hypers: (20, 1, agent_hyper, 1/4, 1/4, 5)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsConvolutional/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps input frames to hidden state.\n",
    "class STATE_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, frame_count):\n",
    "        super().__init__()\n",
    "        # Residual conv (1)\n",
    "        self.conv1 = nn.Conv2d(frame_count, 16, 3, padding=1)  \n",
    "        # First set of hidden non-residual layers.\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        # Residual conv (2)\n",
    "        self.conv4 = nn.Conv2d(16, 4, 3, padding=1)  \n",
    "        # Second set of hidden non-residual layers.\n",
    "        self.conv5 = nn.Conv2d(4, 4, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(4, 4, 3, padding=1)\n",
    "        # Final convolution.\n",
    "        self.conv7 = nn.Conv2d(4, 1, 3, padding=1)\n",
    "        \n",
    "        # Pooling operation.\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Initialized sigmoid.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    # Propogates input thru layers.\n",
    "    def forward(self, x):\n",
    "        # Activation for hidden layers.\n",
    "        c_a = F.relu\n",
    "        \n",
    "        # First residual layer.\n",
    "        x = c_a(self.conv1(x))\n",
    "        # First non-residual layers.\n",
    "        x_prime = c_a(self.conv2(x))\n",
    "        x_prime = c_a(self.conv3(x_prime))\n",
    "        # First combination\n",
    "        x = x + x_prime\n",
    "        # First pool.\n",
    "        x = self.pool(x)\n",
    "        # Second residual layer.\n",
    "        x = c_a(self.conv4(x))\n",
    "        # Second non-residual layers.\n",
    "        x_prime = c_a(self.conv5(x))\n",
    "        x_prime = c_a(self.conv6(x_prime))\n",
    "        # Second combination\n",
    "        x = x + x_prime\n",
    "        # Second pool.\n",
    "        x = self.pool(x)\n",
    "        # Final conv.\n",
    "        x = c_a(self.conv7(x))\n",
    "        # Final pool.\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Reshape into 1d tensor.\n",
    "        x = x.view(x.size(0),-1)\n",
    "        \n",
    "        # Return hidden state.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, conv_map, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        self.conv_map = conv_map\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "        self.params += list(conv_map.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        x = self.conv_map(x)\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if not train:\n",
    "                outputs.append(self.sigmoid(out(x)))\n",
    "            else:\n",
    "                outputs.append(out(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses the game frame so the agent can use it.\n",
    "def process_frame(frame):\n",
    "    frame = frame.mean(axis=2)\n",
    "    #frame = frame[95:155, 8:]\n",
    "    frame = frame * (1/255)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.1\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(STATE_MAP(stack_size).to(self.t_device), 520, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        #self.loss_func = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=50):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                self.optimizer.zero_grad()\n",
    "                #inputs = torch.stack(batches[i][0])\n",
    "                inputs = torch.cat(batches[i][0], 0)\n",
    "                #outputs = [torch.stack(o).to(self.t_device) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long().to(self.t_device) for o in batches[i][1]]\n",
    "                #peer_outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs.to(self.t_device), train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j])\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j])\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = process_frame(observation)\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                #tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                tensor = torch.stack([torch.stack([tensor for _ in range(self.stack_size)])])\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor.to(self.t_device))\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    d = d[0]\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = process_frame(observation)\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    groups.append((previous_state, action_queue))\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        #tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                        tensor = torch.stack([torch.stack([tensor for _ in range(self.stack_size)])])\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        #tensor = torch.cat(tensors, 0)\n",
    "                        tensor = torch.stack([torch.stack(tensors)])\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor.to(self.t_device))\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        d = d[0]\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('\\n')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                agent = self.replace_agent(agent)\n",
    "            trajectories = []\n",
    "            for _ in range(int(len(examples) / 2)):\n",
    "                index = rand.randint(0, len(examples) - 1)\n",
    "                trajectories.append(examples[index])\n",
    "            inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "            loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=128, epochs=50)\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('\\n')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 1, 0.001, 0.95, 5, torch.device('cuda'), torch.device('cpu'))\n",
    "population = POPULATION(20, 1, agent_hyper, 1/4, 1/4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_18 | STEP 1800 | SCORE 700.0 | AGE 1 | MEMBER 20/20 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 630.0 | LOW SCORE 200.0 | HIGH SCORE 1300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_3 | EPOCH 50/50 | BATCH 27/27 | CURRENT BATCH COUNT 120 | LOSS 2.6407 | AGE 1 | MEMBER 5/5\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.6367879356808137\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_11 | STEP 1337 | SCORE 900.0 | AGE 1 | MEMBER 20/20 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 530.0 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | EPOCH 50/50 | BATCH 28/28 | CURRENT BATCH COUNT 126 | LOSS 2.6355 | AGE 1 | MEMBER 5/5\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 2.6372040938309262\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_13 | STEP 1323 | SCORE 700.0 | AGE 2 | MEMBER 20/20 | GAME 1/1\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 600.0 | LOW SCORE 100.0 | HIGH SCORE 1300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | EPOCH 27/50 | BATCH 20/29 | CURRENT BATCH COUNT 128 | LOSS 2.6366 | AGE 2 | MEMBER 2/5\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
