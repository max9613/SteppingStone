{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Switched end to relu, Hypers: (10**-4, 128, 64, 4, 2, 14, torch.device('cpu'), torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsHydra/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action predictor based on two embedded states.\n",
    "class ACTION_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.relu(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Hydra map.\n",
    "class HYDRA_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, number_of_heads, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device):\n",
    "        super().__init__()\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_hidden_count = pre_hidden_count\n",
    "        self.post_hidden_count = post_hidden_count\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        weights = generate_weights(input_size, hidden_size, pre_hidden_count)\n",
    "        self.pre_hidden_layers = []\n",
    "        prev_weight = input_size\n",
    "        for w in weights:\n",
    "            self.pre_hidden_layers.append(nn.Linear(prev_weight, w).to(t_device))\n",
    "            prev_weight = w\n",
    "        self.pre_hidden_layers.append(nn.Linear(prev_weight, hidden_size).to(t_device))\n",
    "        self.heads = []\n",
    "        for _ in range(number_of_heads):\n",
    "            self.heads.append(ACTION_MAP(hidden_size, class_count, post_hidden_count, t_device))\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for layer in self.pre_hidden_layers:\n",
    "            self.params += list(layer.parameters())\n",
    "        \n",
    "    # Forward propogation.\n",
    "    def forward(self, states, heads, hidden=None, train=False, train_rnn=True):\n",
    "        outs = [[] for _ in range(len(heads))]\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.hidden_size).to(self.t_device)\n",
    "        for x in states:\n",
    "            for layer in self.pre_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            x += hidden\n",
    "            if not train_rnn:\n",
    "                x = x.detach()\n",
    "            for i in range(len(heads)):\n",
    "                outs[i].append(self.heads[heads[i]](x, train=train))\n",
    "            hidden = x\n",
    "        if train:\n",
    "            return outs\n",
    "        else:\n",
    "            return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Hydra that plays the game.\n",
    "class DUAL_HYDRA:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, learning_rate, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device, s_device):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.policy_map = HYDRA_MAP(2, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device)\n",
    "        self.greedy_optimizer = torch.optim.Adam(self.policy_map.params + self.policy_map.heads[0].params, lr=learning_rate)\n",
    "        self.selfless_optimizer = torch.optim.Adam(self.policy_map.heads[1].params, lr=learning_rate)\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.age = 0\n",
    "        \n",
    "    # Train the agent.\n",
    "    # Input data should be a list of trajectories, which should be of form [[states], [actions]].\n",
    "    def train_head(self, optimizer, head, train_rnn, trajectories, unroll_depth, batch_size=32, epochs=50, extra_info=''):\n",
    "        batches = []\n",
    "        batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "        rand.shuffle(trajectories)\n",
    "        count = 0\n",
    "        for t in trajectories:\n",
    "            for i in range(unroll_depth):\n",
    "                batch[0][i].append(t[0][i])\n",
    "                batch[1][i].append(t[1][i])\n",
    "            count += 1\n",
    "            if count >= batch_size:\n",
    "                batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "                batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "                batches.append(batch)\n",
    "                batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "                count = 0\n",
    "        if count > 0:\n",
    "            batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "            batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "            batches.append(batch)\n",
    "            batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "            count = 0\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                inputs = batch[0]\n",
    "                targets = batch[1]\n",
    "                outputs = self.policy_map(inputs, heads=[head], train=True, train_rnn=train_rnn)[0]\n",
    "                loss = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    loss += self.cross_loss(outputs[i], targets[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                print('\\rTRAINING HEAD {} | BATCH {}/{} | EPOCH {}/{} | LOSS {}'.format(head, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "    \n",
    "    # Plays the game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        hidden_state = None\n",
    "        groups = []\n",
    "        group = []\n",
    "        env.reset()\n",
    "        tensor = None\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            print('\\rSTEP {} | SCORE {} | AGE {} {}\\t\\t'.format(step, score, self.age, extra_info), end = '')\n",
    "            score += reward\n",
    "            if tensor is not None:\n",
    "                group.append((tensor, action, reward))\n",
    "            step += 1\n",
    "            # Need to update to support multi-head\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policies, hidden_state = self.policy_map([tensor], hidden=hidden_state, heads=[0,1])\n",
    "            policy = policies[0][0].to(self.s_device) + policies[1][0].to(self.s_device)\n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.class_count - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                tensor = None\n",
    "                lives = info['ale.lives']\n",
    "                hidden_state = None\n",
    "                group = []\n",
    "        return groups, score, step\n",
    "    \n",
    "    # Sorts the trajectories into ones for the greedy head and non-greedy head.\n",
    "    def prepare_data(self, groups, unroll_depth):\n",
    "        greedy_trajectories = []\n",
    "        non_greedy_trajectories = []\n",
    "        for total_run in groups:\n",
    "            index = unroll_depth\n",
    "            total_reward = 0\n",
    "            while index < len(total_run):\n",
    "                actions = []\n",
    "                states = []\n",
    "                for group in total_run[index-unroll_depth:index]:\n",
    "                    total_reward += group[2]\n",
    "                    actions.append(group[1])\n",
    "                    states.append(group[0])\n",
    "                if total_reward > 0:\n",
    "                    greedy_trajectories.append([states, actions])\n",
    "                else:\n",
    "                    non_greedy_trajectories.append([states, actions])\n",
    "                index += 1\n",
    "        return greedy_trajectories, non_greedy_trajectories\n",
    "    \n",
    "    # Collects data from n games and trains on it.\n",
    "    def collect_and_train(self, env, number_of_games, unroll_depth, render = False):\n",
    "        all_groups = []\n",
    "        total_score = 0\n",
    "        low_score = None\n",
    "        high_score = None\n",
    "        for g in range(number_of_games):\n",
    "            groups, score, step = self.play_game(env, render, f'| GAME {g+1}/{number_of_games}')\n",
    "            all_groups += groups\n",
    "            total_score += score\n",
    "            if low_score is None or score < low_score:\n",
    "                low_score = score\n",
    "            if high_score is None or score > high_score:\n",
    "                high_score = score\n",
    "        print()\n",
    "        manager.print(f'FINISHED {number_of_games} GAMES | AVERAGE SCORE {total_score/number_of_games} | LOW SCORE {low_score} | HIGH SCORE {high_score}')\n",
    "        manager.save()\n",
    "        greedy, selfless = self.prepare_data(all_groups, unroll_depth)\n",
    "        greedy_loss = self.train_head(self.greedy_optimizer, 0, True, greedy, unroll_depth, batch_size=256, epochs=50, extra_info='')\n",
    "        print()\n",
    "        manager.print(f'FINISHED TRAINING GREEDY HEAD | LOSS {greedy_loss}')\n",
    "        manager.save()\n",
    "        selfless_loss = self.train_head(self.selfless_optimizer, 1, False, selfless, unroll_depth, batch_size=256, epochs=50, extra_info='')\n",
    "        print()\n",
    "        manager.print(f'FINISHED TRAINING SELFLESS HEAD | LOSS {selfless_loss}')\n",
    "        manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra = DUAL_HYDRA(10**-4, 128, 64, 4, 2, 14, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1454 | SCORE 1200.0 | AGE 0 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 640.0 | LOW SCORE 100.0 | HIGH SCORE 1300.0\n",
      "TRAINING HEAD 0 | BATCH 42/42 | EPOCH 50/50 | LOSS 50.253055572509766\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 51.11092952546619\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 50.171531677246094\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 50.257630197557354\n",
      "STEP 1290 | SCORE 400.0 | AGE 2 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 395.0 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "TRAINING HEAD 0 | BATCH 26/26 | EPOCH 50/50 | LOSS 47.178356170654325\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 47.26047481536865\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 45.789443969726566\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 46.82081136395854\n",
      "STEP 1337 | SCORE 300.0 | AGE 4 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 495.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 44.140087127685554\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.14260809659958\n",
      "TRAINING HEAD 1 | BATCH 61/61 | EPOCH 50/50 | LOSS 43.896724700927734\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 44.109545908443266\n",
      "STEP 1207 | SCORE 400.0 | AGE 6 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 520.0 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "TRAINING HEAD 0 | BATCH 31/31 | EPOCH 50/50 | LOSS 44.313846588134766\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.07655392308389\n",
      "TRAINING HEAD 1 | BATCH 63/63 | EPOCH 50/50 | LOSS 43.980861663818364\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 44.01587037101625\n",
      "STEP 1138 | SCORE 700.0 | AGE 8 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 530.0 | LOW SCORE 0.0 | HIGH SCORE 1800.0\n",
      "TRAINING HEAD 0 | BATCH 38/38 | EPOCH 50/50 | LOSS 44.139625549316406\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.114769720780224\n",
      "TRAINING HEAD 1 | BATCH 57/57 | EPOCH 50/50 | LOSS 43.834053039550786\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.98462153250711\n",
      "STEP 970 | SCORE 100.0 | AGE 10 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 400.0 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "TRAINING HEAD 0 | BATCH 31/31 | EPOCH 50/50 | LOSS 44.118999481201174\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.09037191083355\n",
      "TRAINING HEAD 1 | BATCH 58/58 | EPOCH 50/50 | LOSS 43.734714508056645\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.94759691567256\n",
      "STEP 1260 | SCORE 700.0 | AGE 12 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 450.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 28/28 | EPOCH 50/50 | LOSS 44.034091949462894\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.04174513680594\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 43.795963287353516\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.95231120324904\n",
      "STEP 1461 | SCORE 700.0 | AGE 14 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 425.0 | LOW SCORE 100.0 | HIGH SCORE 800.0\n",
      "TRAINING HEAD 0 | BATCH 27/27 | EPOCH 50/50 | LOSS 43.993587493896484\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.000672793918184\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 43.797286987304696\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.91269355527816\n",
      "STEP 1720 | SCORE 1200.0 | AGE 16 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 590.0 | LOW SCORE 100.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 40/40 | EPOCH 50/50 | LOSS 44.051158905029356\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 44.00382291984558\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 43.708118438720706\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.9210541689598\n",
      "STEP 1111 | SCORE 500.0 | AGE 18 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 525.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 44.057235717773446\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.98693411588669\n",
      "TRAINING HEAD 1 | BATCH 61/61 | EPOCH 50/50 | LOSS 43.332687377929696\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.900700816170115\n",
      "STEP 956 | SCORE 0.0 | AGE 20 | GAME 20/20\t\t0\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 410.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 43.985507965087896\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.98874266147614\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 41.544876098632814\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89574770006083\n",
      "STEP 1797 | SCORE 500.0 | AGE 22 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 415.0 | LOW SCORE 100.0 | HIGH SCORE 900.0\n",
      "TRAINING HEAD 0 | BATCH 35/35 | EPOCH 50/50 | LOSS 43.958946228027344\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.977125941685266\n",
      "TRAINING HEAD 1 | BATCH 56/56 | EPOCH 50/50 | LOSS 43.765647888183594\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.916024958746775\n",
      "STEP 1204 | SCORE 300.0 | AGE 24 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 435.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 44.016803741455086\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.96053927898407\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 42.342441558837894\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.88909539130426\n",
      "STEP 1357 | SCORE 400.0 | AGE 26 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 535.0 | LOW SCORE 100.0 | HIGH SCORE 1500.0\n",
      "TRAINING HEAD 0 | BATCH 33/33 | EPOCH 50/50 | LOSS 43.977657318115234\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.973079216697\n",
      "TRAINING HEAD 1 | BATCH 60/60 | EPOCH 50/50 | LOSS 42.302776336669925\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89091610081991\n",
      "STEP 1442 | SCORE 700.0 | AGE 28 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 520.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 31/31 | EPOCH 50/50 | LOSS 43.866867065429694\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.98116268281014\n",
      "TRAINING HEAD 1 | BATCH 60/60 | EPOCH 50/50 | LOSS 43.570705413818365\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90084724299113\n",
      "STEP 943 | SCORE 300.0 | AGE 30 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 350.0 | LOW SCORE 0.0 | HIGH SCORE 900.0\n",
      "TRAINING HEAD 0 | BATCH 24/24 | EPOCH 50/50 | LOSS 43.911937713623054\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.976829792658485\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 41.465068817138675\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.87566022442233\n",
      "STEP 1276 | SCORE 400.0 | AGE 32 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 425.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 28/28 | EPOCH 50/50 | LOSS 43.954895019531254\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.991440508706226\n",
      "TRAINING HEAD 1 | BATCH 62/62 | EPOCH 50/50 | LOSS 43.875335693359375\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.921861302775724\n",
      "STEP 1106 | SCORE 300.0 | AGE 34 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 420.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 25/25 | EPOCH 50/50 | LOSS 43.892105102539066\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.941192459106446\n",
      "TRAINING HEAD 1 | BATCH 63/63 | EPOCH 50/50 | LOSS 43.859184265136726\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.909050633263966\n",
      "STEP 999 | SCORE 100.0 | AGE 36 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 510.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 39/39 | EPOCH 50/50 | LOSS 43.974658966064455\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.96718434064816\n",
      "TRAINING HEAD 1 | BATCH 57/57 | EPOCH 50/50 | LOSS 43.871593475341856\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90192614906712\n",
      "STEP 1377 | SCORE 800.0 | AGE 38 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 525.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "TRAINING HEAD 0 | BATCH 34/34 | EPOCH 50/50 | LOSS 43.980182647705085\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.964566964542165\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 43.299423217773446\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89654996193062\n",
      "STEP 867 | SCORE 800.0 | AGE 40 | GAME 20/20\t\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 430.0 | LOW SCORE 100.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 30/30 | EPOCH 50/50 | LOSS 43.946823120117194\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.96277199808757\n",
      "TRAINING HEAD 1 | BATCH 56/56 | EPOCH 50/50 | LOSS 43.838565826416016\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90479556083679\n",
      "STEP 1370 | SCORE 200.0 | AGE 42 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 505.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 30/30 | EPOCH 50/50 | LOSS 43.927093505859375\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.955560628255206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING HEAD 1 | BATCH 65/65 | EPOCH 50/50 | LOSS 42.883655548095734\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.9048105867826\n",
      "STEP 975 | SCORE 600.0 | AGE 44 | GAME 20/20\t\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 545.0 | LOW SCORE 200.0 | HIGH SCORE 1500.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 43.975051879882816\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94736580371857\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 43.786132812526956\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89768683094089\n",
      "STEP 1299 | SCORE 700.0 | AGE 46 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 635.0 | LOW SCORE 100.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 42/42 | EPOCH 50/50 | LOSS 43.980480194091845\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95015682038807\n",
      "TRAINING HEAD 1 | BATCH 53/53 | EPOCH 50/50 | LOSS 43.849441528320314\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.892862672625846\n",
      "STEP 1071 | SCORE 200.0 | AGE 48 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 635.0 | LOW SCORE 200.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 39/39 | EPOCH 50/50 | LOSS 43.942710876464844\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94767235584748\n",
      "TRAINING HEAD 1 | BATCH 58/58 | EPOCH 50/50 | LOSS 43.715110778808594\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.904967140987004\n",
      "STEP 2076 | SCORE 800.0 | AGE 50 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 590.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "TRAINING HEAD 0 | BATCH 38/38 | EPOCH 50/50 | LOSS 43.969924926757816\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.96277947476036\n",
      "TRAINING HEAD 1 | BATCH 61/61 | EPOCH 50/50 | LOSS 43.166202545166016\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.903500876504864\n",
      "STEP 1210 | SCORE 0.0 | AGE 52 | GAME 20/20\t\t\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 450.0 | LOW SCORE 0.0 | HIGH SCORE 2300.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 43.948600769042975\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95583840847016\n",
      "TRAINING HEAD 1 | BATCH 64/64 | EPOCH 50/50 | LOSS 43.621849060058594\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.908995012044905\n",
      "STEP 1415 | SCORE 400.0 | AGE 54 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 465.0 | LOW SCORE 0.0 | HIGH SCORE 900.0\n",
      "TRAINING HEAD 0 | BATCH 34/34 | EPOCH 50/50 | LOSS 43.984680175781254\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95561077791102\n",
      "TRAINING HEAD 1 | BATCH 60/60 | EPOCH 50/50 | LOSS 43.798141479492194\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89458022689819\n",
      "STEP 1167 | SCORE 200.0 | AGE 56 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 360.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 27/27 | EPOCH 50/50 | LOSS 43.934566497802734\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94806931672273\n",
      "TRAINING HEAD 1 | BATCH 67/67 | EPOCH 50/50 | LOSS 43.659378051757816\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89596804149115\n",
      "STEP 1472 | SCORE 600.0 | AGE 58 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 460.0 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "TRAINING HEAD 0 | BATCH 31/31 | EPOCH 50/50 | LOSS 43.967803955078125\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.96452487576392\n",
      "TRAINING HEAD 1 | BATCH 63/63 | EPOCH 50/50 | LOSS 43.901607513427734\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89721940782335\n",
      "STEP 1019 | SCORE 600.0 | AGE 60 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 495.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 39/39 | EPOCH 50/50 | LOSS 43.983318328857426\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.966799877851436\n",
      "TRAINING HEAD 1 | BATCH 53/53 | EPOCH 50/50 | LOSS 43.834815979003906\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89019228809285\n",
      "STEP 1164 | SCORE 400.0 | AGE 62 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 540.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 36/36 | EPOCH 50/50 | LOSS 44.004096984863286\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95775588141547\n",
      "TRAINING HEAD 1 | BATCH 57/57 | EPOCH 50/50 | LOSS 43.879623413085944\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89984698513098\n",
      "STEP 1023 | SCORE 300.0 | AGE 64 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 610.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 40/40 | EPOCH 50/50 | LOSS 43.995952606201175\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.97079432487488\n",
      "TRAINING HEAD 1 | BATCH 57/57 | EPOCH 50/50 | LOSS 43.165996551513676\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.88985466538814\n",
      "STEP 1351 | SCORE 500.0 | AGE 66 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 545.0 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "TRAINING HEAD 0 | BATCH 40/40 | EPOCH 50/50 | LOSS 43.973804473876956\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95104253959656\n",
      "TRAINING HEAD 1 | BATCH 57/57 | EPOCH 50/50 | LOSS 43.048267364501956\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.895535940872996\n",
      "STEP 1307 | SCORE 800.0 | AGE 68 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 510.0 | LOW SCORE 0.0 | HIGH SCORE 900.0\n",
      "TRAINING HEAD 0 | BATCH 33/33 | EPOCH 50/50 | LOSS 43.948074340820314\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.956434497255266\n",
      "TRAINING HEAD 1 | BATCH 56/56 | EPOCH 50/50 | LOSS 43.765724182128906\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.906058916364394\n",
      "STEP 1091 | SCORE 300.0 | AGE 70 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 495.0 | LOW SCORE 100.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 34/34 | EPOCH 50/50 | LOSS 43.948303222656256\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94637081819422\n",
      "TRAINING HEAD 1 | BATCH 55/55 | EPOCH 50/50 | LOSS 43.851577758789065\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.89930180913752\n",
      "STEP 1405 | SCORE 500.0 | AGE 72 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 525.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "TRAINING HEAD 0 | BATCH 33/33 | EPOCH 50/50 | LOSS 43.965965270996094\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94678423563639\n",
      "TRAINING HEAD 1 | BATCH 59/59 | EPOCH 50/50 | LOSS 43.023040771484375\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.904288555080605\n",
      "STEP 1066 | SCORE 0.0 | AGE 74 | GAME 20/20\t\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 400.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 28/28 | EPOCH 50/50 | LOSS 43.935424804687525\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.947656383514406\n",
      "TRAINING HEAD 1 | BATCH 65/65 | EPOCH 50/50 | LOSS 43.046966552734375\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.91607486783541\n",
      "STEP 1390 | SCORE 400.0 | AGE 76 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 455.0 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "TRAINING HEAD 0 | BATCH 30/30 | EPOCH 50/50 | LOSS 43.960124969482424\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.954636644999184\n",
      "TRAINING HEAD 1 | BATCH 64/64 | EPOCH 50/50 | LOSS 43.889152526855474\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.91506527900696\n",
      "STEP 1225 | SCORE 200.0 | AGE 78 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 570.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "TRAINING HEAD 0 | BATCH 34/34 | EPOCH 50/50 | LOSS 43.975173950195314\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.944650120454675\n",
      "TRAINING HEAD 1 | BATCH 61/61 | EPOCH 50/50 | LOSS 43.918857574462895\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.908877575983766\n",
      "STEP 1116 | SCORE 100.0 | AGE 80 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 555.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 32/32 | EPOCH 50/50 | LOSS 43.945335388183594\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94257625818253\n",
      "TRAINING HEAD 1 | BATCH 61/61 | EPOCH 50/50 | LOSS 43.885589599609375\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90735428231662\n",
      "STEP 1341 | SCORE 100.0 | AGE 82 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 385.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "TRAINING HEAD 0 | BATCH 29/29 | EPOCH 50/50 | LOSS 43.888458251953125\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.932706101516196\n",
      "TRAINING HEAD 1 | BATCH 63/63 | EPOCH 50/50 | LOSS 43.816585540771484\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90871834285676\n",
      "STEP 1526 | SCORE 700.0 | AGE 84 | GAME 20/20\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 425.0 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "TRAINING HEAD 0 | BATCH 31/31 | EPOCH 50/50 | LOSS 43.953567504882816\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.94059057666409\n",
      "TRAINING HEAD 1 | BATCH 56/56 | EPOCH 50/50 | LOSS 43.915588378906256\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90385216440473\n",
      "STEP 1007 | SCORE 200.0 | AGE 86 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 325.0 | LOW SCORE 0.0 | HIGH SCORE 600.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING HEAD 0 | BATCH 24/24 | EPOCH 50/50 | LOSS 43.937763214111335\n",
      "FINISHED TRAINING GREEDY HEAD | LOSS 43.95064398765564\n",
      "TRAINING HEAD 1 | BATCH 56/56 | EPOCH 50/50 | LOSS 43.767303466796875\n",
      "FINISHED TRAINING SELFLESS HEAD | LOSS 43.90226565633501\n",
      "STEP 7 | SCORE 0.0 | AGE 88 | GAME 12/20\t\t1/20\t\t"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DUAL_HYDRA' object has no attribute 'class_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-27346f7d52ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mhydra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_and_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-819dad20170b>\u001b[0m in \u001b[0;36mcollect_and_train\u001b[1;34m(self, env, number_of_games, unroll_depth, render)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mhigh_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'| GAME {g+1}/{number_of_games}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0mall_groups\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mtotal_score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-819dad20170b>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(self, env, render, extra_info)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_device\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DUAL_HYDRA' object has no attribute 'class_count'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    hydra.collect_and_train(env, 20, 20, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
