{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Added training data subsampling and gave each agent its own random threshold, Hypers: (80, 1, agent_params, 1/4, 1/4, 1000)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsDiscriminator/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single LSTM cell.\n",
    "class LSTM_CELL(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, cell_size, hidden_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.t_device = t_device\n",
    "        self.cell_forget_gate = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.cell_update_gate_sigmoid = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.cell_update_gate_tanh = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.hidden_dim_reduce = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    # Forward propogates the input through the cell and produces a new cell and hidden state.\n",
    "    def forward(self, x, cell_state, hidden_state):\n",
    "        x = torch.cat([hidden_state, x], dim=-1)\n",
    "        cell_state *= self.sigmoid(self.cell_forget_gate(x))\n",
    "        cell_state += self.sigmoid(self.cell_update_gate_sigmoid(x)) + self.tanh(self.cell_update_gate_tanh(x))\n",
    "        hidden_state = self.sigmoid(self.hidden_dim_reduce(x)) * self.tanh(cell_state)\n",
    "        return cell_state, hidden_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based discriminator.\n",
    "class DISCRIMINATOR(nn.Module):\n",
    "    \n",
    "    # The input thrown into the LSTM will be the concat of the distribution and state.\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, cell_size, hidden_size, distribution_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.distribution_size = distribution_size\n",
    "        self.t_device = t_device\n",
    "        self.lstm_cell = LSTM_CELL(input_size + distribution_size, cell_size, hidden_size, t_device)\n",
    "        self.hidden_1 = nn.Linear(hidden_size, int(hidden_size * (3/4)))\n",
    "        self.hidden_2 = nn.Linear(int(hidden_size * (3/4)), int(hidden_size * (1/2)))\n",
    "        self.hidden_3 = nn.Linear(int(hidden_size * (1/2)), int(hidden_size * (1/4)))\n",
    "        self.output = nn.Linear(int(hidden_size * (1/4)), 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.act_func = self.relu\n",
    "        \n",
    "    # Forward progogates the provided input through the network and returns the corresponding labels and inner states.\n",
    "    # Cell and hidden states should be pre-stacked to the correct sizing to match the state and distribution batch size.\n",
    "    # The inner states will be a stack of tensors with sizing equal to the batch sizing.\n",
    "    def forward(self, states, distributions, cell_states, hidden_states, training=False):\n",
    "        outputs = []\n",
    "        for index in range(len(states)):\n",
    "            x = torch.cat([states[index], distributions[index]], dim=-1)\n",
    "            cell_state, hidden_state = self.lstm_cell(x, cell_states, hidden_states)\n",
    "            x = self.act_func(self.hidden_1(hidden_state))\n",
    "            x = self.act_func(self.hidden_2(x))\n",
    "            x = self.act_func(self.hidden_3(x))\n",
    "            x = self.output(x)\n",
    "            if not training:\n",
    "                x = self.sigmoid(x)\n",
    "            outputs.append(x)\n",
    "        return outputs, cell_states, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feedforward generator.\n",
    "class GENERATOR(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, distribution_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.distribution_size = distribution_size\n",
    "        self.t_device = t_device\n",
    "        self.sin = torch.sin\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        weights = generate_weights(self.input_size, self.distribution_size, 2)\n",
    "        self.hidden_1 = nn.Linear(input_size, weights[0])\n",
    "        self.hidden_2 = nn.Linear(weights[0], weights[1])\n",
    "        self.output = nn.Linear(weights[1], distribution_size)\n",
    "        \n",
    "    # Forward propogate input states.\n",
    "    def forward(self, x):\n",
    "        x = self.sin(self.hidden_1(x))\n",
    "        x = self.sin(self.hidden_2(x))\n",
    "        return self.sigmoid(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that combines a discriminator and generator to play a game.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    # learning_rates[0] = discriminator learning rate & learning_rate[1] = generator learning_rate.\n",
    "    def __init__(self, name, learning_rates, input_size, hidden_size, action_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.learning_rates = learning_rates\n",
    "        self.input_size = input_size\n",
    "        self.cell_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.action_size = action_size\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.discriminator = DISCRIMINATOR(input_size, self.cell_size, hidden_size, action_size, t_device)\n",
    "        self.generator = GENERATOR(input_size, action_size, t_device)\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=learning_rates[0])\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=learning_rates[1])\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.age = 0\n",
    "        self.threshold = rand.uniform(0.5, 1)\n",
    "        \n",
    "    # Trains the generator to maximize the discriminator's output on the provided sequence of states.\n",
    "    def train_generator(self, states, cell_state, hidden_state, epochs):\n",
    "        self.generator_optimizer.zero_grad()\n",
    "        for e in range(epochs):\n",
    "            current_cell_state = cell_state\n",
    "            current_hidden_state = hidden_state\n",
    "            loss = 0\n",
    "            for state in states:\n",
    "                distribution = self.generator(state)\n",
    "                out, current_cell_state, current_hidden_state = self.discriminator([state], [distribution], current_cell_state, current_hidden_state, True)\n",
    "                loss += self.bce_loss(out[0], torch.ones(1))\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.generator_optimizer.step()\n",
    "            \n",
    "    # Trains the discriminator on the provided trajectories.\n",
    "    # Trajectories should be of the form (label, groups)\n",
    "    # Groups should just be a list of sequential gameplay generated pairings of size = unroll_size.\n",
    "    def train_discriminator(self, trajectories, unroll_size, batch_size=64, epochs=50, extra_info=''):\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "        batches = []\n",
    "        states = [[] for _ in range(unroll_size)]\n",
    "        distributions = [[] for _ in range(unroll_size)]\n",
    "        labels = [[] for _ in range(unroll_size)]\n",
    "        rand.shuffle(trajectories)\n",
    "        for trajectory in trajectories:\n",
    "            label = trajectory[0]\n",
    "            groups = trajectory[1]\n",
    "            for i in range(len(groups)):\n",
    "                states[i].append(groups[i][0])\n",
    "                distributions[i].append(groups[i][1])\n",
    "                labels[i].append(torch.zeros(1) if label == 0 else torch.ones(1))\n",
    "            if len(states[0]) >= batch_size:\n",
    "                states = [torch.stack(state) for state in states]\n",
    "                distributions = [torch.stack(dist) for dist in distributions]\n",
    "                labels = [torch.stack(label) for label in labels]\n",
    "                batches.append((states, distributions, labels, len(states[0])))\n",
    "                states = [[] for _ in range(unroll_size)]\n",
    "                distributions = [[] for _ in range(unroll_size)]\n",
    "                labels = [[] for _ in range(unroll_size)]\n",
    "        if len(states[0]) > 0:\n",
    "            states = [torch.stack(state) for state in states]\n",
    "            distributions = [torch.stack(dist) for dist in distributions]\n",
    "            labels = [torch.stack(label) for label in labels]\n",
    "            batches.append((states, distributions, labels, len(states[0])))\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                states = batch[0]\n",
    "                distributions = batch[1]\n",
    "                labels = batch[2]\n",
    "                actual_size = batch[3]\n",
    "                # These two assignments might need to be messed with to get everything working.\n",
    "                cell_state = torch.zeros(actual_size, self.cell_size)\n",
    "                hidden_state = torch.zeros(actual_size, self.hidden_size)\n",
    "                outputs, cell_state, hidden_state = self.discriminator(states, distributions, cell_state, hidden_state, True)\n",
    "                loss = 0\n",
    "                for o in range(len(outputs)):\n",
    "                    loss += self.bce_loss(outputs[o], labels[o])\n",
    "                loss.backward()\n",
    "                self.discriminator_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                print('\\rTRAINING {} | AGE {} | BATCH {}/{} | EPOCH {}/{} | LOSS {} {}'.format(self.name, self.age, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "                \n",
    "            \n",
    "    # Plays the provided game.\n",
    "    # A group has the following content: (state, distribution, verdict, cell_state, hidden_state)\n",
    "    def play_game(self, env, generator_epochs, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        cell_state = torch.zeros(self.cell_size)\n",
    "        hidden_state = torch.zeros(self.hidden_size)\n",
    "        groups = []\n",
    "        group = []\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} | THRESHOLD {:0.3f} {}\\t\\t'.format(self.name, step, score, self.age, self.threshold, extra_info), end = '')\n",
    "            score += reward\n",
    "            step += 1\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policy = self.generator(tensor)\n",
    "            verdict, possible_cell_state, possible_hidden_state = self.discriminator([tensor], [policy], cell_state, hidden_state)\n",
    "            if verdict[0] < self.threshold:\n",
    "                states = [tensor]\n",
    "                past_cell_state = cell_state.detach()\n",
    "                past_hidden_state = hidden_state.detach()\n",
    "                states = states[::-1]\n",
    "                self.train_generator(states, past_cell_state, past_hidden_state, generator_epochs)\n",
    "                policy = self.generator(tensor)\n",
    "                group.append((tensor.detach(), policy.detach(), 0, cell_state.detach(), hidden_state.detach()))\n",
    "                verdict, cell_state, hidden_state = self.discriminator([tensor], [policy], cell_state, hidden_state)\n",
    "            else:\n",
    "                group.append((tensor.detach(), policy.detach(), 1, cell_state.detach(), hidden_state.detach()))\n",
    "                cell_state = possible_cell_state\n",
    "                hidden_state = possible_hidden_state            \n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.action_size - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                lives = info['ale.lives']\n",
    "                cell_state = torch.zeros(self.cell_size)\n",
    "                hidden_state = torch.zeros(self.hidden_size)\n",
    "                group = []\n",
    "            if render:\n",
    "                env.render()\n",
    "        return groups, score, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        \n",
    "    # Discretizes and labels the provided trajectories.\n",
    "    def prepare_data(self, positive_examples, negative_examples, unroll_depth):\n",
    "        trajectories = []\n",
    "        for groups in positive_examples:\n",
    "            index = unroll_depth\n",
    "            while index < len(groups):\n",
    "                trajectories.append((1, groups[index-unroll_depth:index]))\n",
    "                index += 1\n",
    "        for groups in negative_examples:\n",
    "            index = unroll_depth\n",
    "            while index < len(groups):\n",
    "                trajectories.append((0, groups[index-unroll_depth:index]))\n",
    "                index += 1\n",
    "        rand.shuffle(trajectories)\n",
    "        return trajectories\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, generator_epochs, unroll_depth, epochs, batch_size, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, generator_epochs, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        for exp in teach_pop:\n",
    "            positive_examples += exp[1]\n",
    "        for exp in train_pop:\n",
    "            negative_examples += exp[1]\n",
    "        trajectories = self.prepare_data(positive_examples, negative_examples, unroll_depth)\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            #if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "            #    agent = self.replace_agent(agent)\n",
    "            rand.shuffle(trajectories)\n",
    "            loss = agent.train_discriminator(trajectories[:int(len(trajectories) / 4)], unroll_depth, batch_size, epochs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}')\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        manager.save()\n",
    "        self.generation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = ([0.0001,0.1], 128, 100, 14, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = POPULATION(80, 1, agent_params, 1/4, 1/4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_45 | STEP 1184 | SCORE 300.0 | AGE 0 | THRESHOLD 0.813 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 628.75 | LOW SCORE 0.0 | HIGH SCORE 5400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_52 | AGE 0 | BATCH 197/197 | EPOCH 10/10 | LOSS 13.624397277832031 | MEMBER 20/20\n",
      "END TRAINING POPULATION | AVG LOSS 9.72182724150153\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_14 | STEP 1642 | SCORE 1700.0 | AGE 0 | THRESHOLD 0.765 | MEMBER 80/80 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1150.0 | LOW SCORE 0.0 | HIGH SCORE 5200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_54 | AGE 0 | BATCH 226/226 | EPOCH 10/10 | LOSS 8.080606460571289 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 9.532543609631537\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_69 | STEP 991 | SCORE 0.0 | AGE 0 | THRESHOLD 0.794 | MEMBER 80/80 | GAME 1/1\t\t/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1972.5 | LOW SCORE 0.0 | HIGH SCORE 8400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_69 | AGE 0 | BATCH 255/255 | EPOCH 10/10 | LOSS 5.097788333892822 | MEMBER 20/2000\n",
      "END TRAINING POPULATION | AVG LOSS 7.432759711853722\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_51 | STEP 1474 | SCORE 800.0 | AGE 0 | THRESHOLD 0.930 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2672.5 | LOW SCORE 0.0 | HIGH SCORE 8900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_30 | AGE 3 | BATCH 278/278 | EPOCH 10/10 | LOSS 5.256945610046387 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 9.26994378074789\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_60 | STEP 1666 | SCORE 1900.0 | AGE 1 | THRESHOLD 0.760 | MEMBER 80/80 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3438.75 | LOW SCORE 0.0 | HIGH SCORE 9900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_30 | AGE 4 | BATCH 308/308 | EPOCH 10/10 | LOSS 8.233470916748047 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 10.23654810780248\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_65 | STEP 1816 | SCORE 5000.0 | AGE 1 | THRESHOLD 0.603 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4195.0 | LOW SCORE 200.0 | HIGH SCORE 13900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_5 | AGE 2 | BATCH 339/339 | EPOCH 10/10 | LOSS 14.973633766174316 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.270829608883039\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_75 | STEP 3380 | SCORE 9600.0 | AGE 2 | THRESHOLD 0.589 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4350.0 | LOW SCORE 900.0 | HIGH SCORE 12600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 4 | BATCH 339/339 | EPOCH 10/10 | LOSS 12.597063064575195 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.639987890291355\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_57 | STEP 2414 | SCORE 6100.0 | AGE 2 | THRESHOLD 0.576 | MEMBER 80/80 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4300.0 | LOW SCORE 1300.0 | HIGH SCORE 8600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_51 | AGE 2 | BATCH 334/334 | EPOCH 10/10 | LOSS 12.788995742797852 | MEMBER 20/20\n",
      "END TRAINING POPULATION | AVG LOSS 13.27103131200739\n",
      "BEGIN RUNNING POPULATION | GENERATION 8\n",
      "AGENT_46 | STEP 3512 | SCORE 9500.0 | AGE 3 | THRESHOLD 0.599 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5117.5 | LOW SCORE 1500.0 | HIGH SCORE 11800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_38 | AGE 2 | BATCH 358/358 | EPOCH 10/10 | LOSS 12.003877639770508 | MEMBER 20/20\n",
      "END TRAINING POPULATION | AVG LOSS 12.425330910949068\n",
      "BEGIN RUNNING POPULATION | GENERATION 9\n",
      "AGENT_19 | STEP 1542 | SCORE 1900.0 | AGE 2 | THRESHOLD 0.544 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5332.5 | LOW SCORE 700.0 | HIGH SCORE 13200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 7 | BATCH 353/353 | EPOCH 10/10 | LOSS 13.177472114562988 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.893899398170854\n",
      "BEGIN RUNNING POPULATION | GENERATION 10\n",
      "AGENT_29 | STEP 2182 | SCORE 4900.0 | AGE 3 | THRESHOLD 0.907 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5341.25 | LOW SCORE 700.0 | HIGH SCORE 11300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 8 | BATCH 350/350 | EPOCH 10/10 | LOSS 14.818615913391113 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.60691121393953\n",
      "BEGIN RUNNING POPULATION | GENERATION 11\n",
      "AGENT_33 | STEP 4275 | SCORE 7400.0 | AGE 2 | THRESHOLD 0.527 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5003.75 | LOW SCORE 600.0 | HIGH SCORE 10700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_59 | AGE 4 | BATCH 341/341 | EPOCH 10/10 | LOSS 13.068829536437988 | MEMBER 20/20\n",
      "END TRAINING POPULATION | AVG LOSS 12.75286949770192\n",
      "BEGIN RUNNING POPULATION | GENERATION 12\n",
      "AGENT_4 | STEP 2682 | SCORE 8400.0 | AGE 1 | THRESHOLD 0.693 | MEMBER 80/80 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5653.75 | LOW SCORE 900.0 | HIGH SCORE 12600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 10 | BATCH 372/372 | EPOCH 10/10 | LOSS 17.21526527404785 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.437886781445755\n",
      "BEGIN RUNNING POPULATION | GENERATION 13\n",
      "AGENT_29 | STEP 3335 | SCORE 8800.0 | AGE 3 | THRESHOLD 0.907 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5368.75 | LOW SCORE 400.0 | HIGH SCORE 13600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_3 | AGE 10 | BATCH 366/366 | EPOCH 10/10 | LOSS 13.662028312683105 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.33666570595705\n",
      "BEGIN RUNNING POPULATION | GENERATION 14\n",
      "AGENT_16 | STEP 2189 | SCORE 4300.0 | AGE 1 | THRESHOLD 0.800 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5683.75 | LOW SCORE 400.0 | HIGH SCORE 10200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_28 | AGE 6 | BATCH 349/349 | EPOCH 10/10 | LOSS 15.900740623474121 | MEMBER 20/200\n",
      "END TRAINING POPULATION | AVG LOSS 12.193185816678755\n",
      "BEGIN RUNNING POPULATION | GENERATION 15\n",
      "AGENT_56 | STEP 2484 | SCORE 7300.0 | AGE 2 | THRESHOLD 0.903 | MEMBER 80/80 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5697.5 | LOW SCORE 800.0 | HIGH SCORE 11400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 13 | BATCH 351/351 | EPOCH 10/10 | LOSS 16.064664840698242 | MEMBER 20/20\n",
      "END TRAINING POPULATION | AVG LOSS 12.656575249532011\n",
      "BEGIN RUNNING POPULATION | GENERATION 16\n",
      "AGENT_23 | STEP 1973 | SCORE 3000.0 | AGE 10 | THRESHOLD 0.719 | MEMBER 80/80 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5285.0 | LOW SCORE 800.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_40 | AGE 4 | BATCH 39/355 | EPOCH 1/10 | LOSS 12.661493301391602 | MEMBER 1/20"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, 2, 20, 10, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
