{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Hypers: (32, 40, 128, 14, 1, (6, 1, 1, 2), 10**-3, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsHierarchy/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese embedding network at the heart of this model, that finds the controllable state.\n",
    "class EMBEDDING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        return self.sin(self.output_layer(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action predictor based on two embedded states.\n",
    "class ACTION_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, train=False):\n",
    "        #x = torch.cat([in_1, in_2], -1)\n",
    "        x = in_2 - in_1\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating network based on two embedded states.\n",
    "class RATING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, final_activation=False):\n",
    "        x = torch.cat([in_1, in_2], -1)\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if not final_activation:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given embedded state, outputs the desired next embedded state.\n",
    "class STRATEGY_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that plays a given game and attempts to achieve a maximum score.\n",
    "# This implementation will only use one agent that contains both selector and strategy networks.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, embedding_size, strategy_count, state_size, action_size, stack_size, layer_counts, learning_rate, gamma, teach_percent, s_device, t_device):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.strategy_count = strategy_count\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.stack_size = stack_size\n",
    "        self.layer_counts = layer_counts\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.teach_percent = teach_percent\n",
    "        self.age = 0\n",
    "        self.s_device = s_device\n",
    "        self.t_device = t_device\n",
    "        # Maps that form the model.\n",
    "        self.embedding_map = EMBEDDING_MAP(state_size * stack_size, embedding_size, layer_counts[0], t_device)\n",
    "        self.action_map = ACTION_MAP(embedding_size, action_size, layer_counts[1], t_device)\n",
    "        self.greedy_rating_map = RATING_MAP(embedding_size + action_size, 2, layer_counts[2], t_device)\n",
    "        self.strategies = [\n",
    "            STRATEGY_MAP(embedding_size, action_size, layer_counts[3], t_device) for _ in range(strategy_count)\n",
    "        ]\n",
    "        # Optimizers for maps.\n",
    "        self.action_embedding_optimizer = torch.optim.Adam(self.action_map.params + self.embedding_map.params, lr=learning_rate)\n",
    "        self.greedy_rating_optimizer = torch.optim.Adam(self.greedy_rating_map.params, lr=learning_rate)\n",
    "        self.strategy_optimizers = [\n",
    "            torch.optim.Adam(s.params, lr=learning_rate) for s in self.strategies\n",
    "        ]\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.alpha = 1/self.action_size\n",
    "        self.exploitation_weight = 0.7\n",
    "        self.exploration_weight = 0.4\n",
    "        \n",
    "    # Trains the action and embedding maps together, this is the only place the embedding map should be trained.\n",
    "    # Groups should be of the form (state (i), state (i+1), action (i))\n",
    "    def train_action_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.action_map(self.embedding_map(inputs_1, True), self.embedding_map(inputs_2, True), True)\n",
    "                peer_out = self.action_map(self.embedding_map(peer_inputs_1, True), self.embedding_map(peer_inputs_2, True), True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rACTION MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.action_embedding_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the greedy rating map.\n",
    "    # Groups should be of the form (state (i), state (i+1), label (i))\n",
    "    def train_greedy_rating_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(eye[groups[position][1]])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.greedy_rating_map(self.embedding_map(inputs_1, True).detach(), inputs_2, False)\n",
    "                peer_out = self.greedy_rating_map(self.embedding_map(peer_inputs_1, True).detach(), peer_inputs_2, False)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rGREEDY RATING MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.greedy_rating_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the strategy at the passed index.\n",
    "    # Groups should be of the form (state (i), state (i+1))\n",
    "    def train_strategy_map(self, index, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            #batch[1].append(groups[position][1])\n",
    "            batch[1].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                inputs = self.embedding_map(inputs).detach()\n",
    "                peer_inputs = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_inputs = self.embedding_map(peer_inputs).detach()\n",
    "                peer_outputs = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                #peer_outputs = self.embedding_map(peer_outputs).detach()\n",
    "                outputs = torch.Tensor(batches[i][1]).long()\n",
    "                #outputs = self.embedding_map(outputs)\n",
    "                out = self.strategies[index](inputs, True)\n",
    "                peer_out = self.strategies[index](peer_inputs, True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rSTRATEGY {}/{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(index+1, len(self.strategies), e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.strategy_optimizers[index].step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Returns labeled examples for a given set, based on a threshold.\n",
    "    def label_groups(self, groups):\n",
    "        groups.sort(key = lambda x: x[3], reverse=True)\n",
    "        good_examples = groups[:int(len(groups) * self.teach_percent)]\n",
    "        bad_examples = groups[int(len(groups) * self.teach_percent):]\n",
    "        examples = []\n",
    "        for group in good_examples:\n",
    "            examples.append((group[0], group[2], 1))\n",
    "        for group in bad_examples:\n",
    "            examples.append((group[0], group[2], 0))\n",
    "        rand.shuffle(examples)\n",
    "        return examples\n",
    "    \n",
    "    # Plays a single game on a compatible enviroment with the provided strategy and returns the score and trajectories.\n",
    "    def play_game_with_strategy(self, env, strategy, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_tensor = None\n",
    "        previous_action = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            previous_action = action\n",
    "            state = observation / 255\n",
    "            frames.append(torch.Tensor.float(torch.from_numpy(state)))\n",
    "            if len(frames) < self.stack_size:\n",
    "                action = rand.randint(0, self.action_size - 1)    \n",
    "            else:\n",
    "                tensor = torch.cat(frames[-self.stack_size:], 0)\n",
    "                embedding = self.embedding_map(tensor).detach()\n",
    "                policy = self.strategies[strategy](embedding)\n",
    "                if sum(policy) <= 0 or min(policy) < 0 or rand.uniform(0,1) < self.gamma:\n",
    "                    action = rand.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                    action = (int(distribution.sample()))\n",
    "                if previous_tensor is not None:\n",
    "                    groups.append((previous_tensor, tensor, previous_action))\n",
    "                previous_tensor = tensor\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_tensor = None\n",
    "                previous_action = None\n",
    "                action = 0\n",
    "                frames = []\n",
    "            step += 1\n",
    "            score += reward\n",
    "            print('\\rSTEP {} | SCORE {} | {}\\t\\t'.format(step, score, extra_info), end = '')\n",
    "        return groups, score\n",
    "    \n",
    "    # Runs the agent on the provided enviroment.\n",
    "    def run(self, env, games, loop, render = False):\n",
    "        running = True\n",
    "        while running:\n",
    "            if not loop:\n",
    "                running = False\n",
    "            # Game playing section.\n",
    "            initial_groups = []\n",
    "            total_score = 0\n",
    "            high_score = None\n",
    "            low_score = None\n",
    "            if games is not None:\n",
    "                for g in range(games):\n",
    "                    group, score = self.play_game(env,render,f'GAME {g+1}/{games}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            else:\n",
    "                for s in range(len(self.strategies)):\n",
    "                    group, score = self.play_game_with_strategy(env,s,render,f'GAME {s+1}/{len(self.strategies)}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            avg_score = total_score / len(initial_groups)\n",
    "            print('')\n",
    "            manager.print(f'END PLAY | AVERAGE SCORE {avg_score} | LOW SCORE {low_score} | HIGH SCORE {high_score}')\n",
    "            manager.save()\n",
    "            # Training action and embedding maps section.\n",
    "            action_groups = []\n",
    "            for group in initial_groups:\n",
    "                action_groups += group[0]\n",
    "            a_e_loss = self.train_action_map(action_groups)\n",
    "            print('')\n",
    "            manager.print(f'END ACTION/EMBEDDING TRAINING | AVERAGE LOSS {a_e_loss}')\n",
    "            manager.save()\n",
    "            # Training greedy and exploratory rating maps section. \n",
    "            score_groups = [] # Groups should be of form (state (i), state (i+1), score)\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    score_groups.append((pairs[0], pairs[1], pairs[2], group[1]))\n",
    "            score_groups = self.label_groups(score_groups)\n",
    "            g_r_loss = self.train_greedy_rating_map(score_groups)\n",
    "            print('')\n",
    "            manager.print(f'END GREEDY RATING TRAINING | AVERAGE LOSS {g_r_loss}')\n",
    "            manager.save()\n",
    "            # Training strategies section.\n",
    "            strategies_training_groups = [] # Groups should be of form (state (i), state (i+1))\n",
    "            eye = torch.eye(self.action_size)\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    in_1 = self.embedding_map(pairs[0])\n",
    "                    greedy_rating = self.greedy_rating_map(in_1, eye[pairs[2]], True).detach().cpu().numpy()\n",
    "                    if np.argmax(greedy_rating) == 1:\n",
    "                        strategies_training_groups.append((pairs[0], pairs[1], pairs[2]))\n",
    "            initial_groups.sort(key = lambda x: x[1], reverse=True)\n",
    "            teach_groups = initial_groups[:int(len(initial_groups) * self.teach_percent)]\n",
    "            for group in teach_groups:\n",
    "                for pairs in group[0]:\n",
    "                    strategies_training_groups.append((pairs[0], pairs[1], pairs[2]))\n",
    "            losses = []\n",
    "            if len(strategies_training_groups) > 0:\n",
    "                for i in range(len(self.strategies)):\n",
    "                    training_groups = []\n",
    "                    while len(training_groups) < len(strategies_training_groups) / 3:\n",
    "                        index = rand.randint(0, len(strategies_training_groups) - 1)\n",
    "                        training_groups.append(strategies_training_groups[index])\n",
    "                    loss = self.train_strategy_map(i, training_groups)\n",
    "                    losses.append(loss)\n",
    "            avg_strat_loss = sum(losses)/len(losses) if len(losses) > 0 else 'NA'\n",
    "            print('')\n",
    "            manager.print(f'END STRATEGY TRAINING | AVERAGE LOSS {avg_strat_loss}')\n",
    "            manager.save()\n",
    "            self.age += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AGENT(32, 40, 128, 14, 1, (6, 1, 1, 2), 10**-3, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1301 | SCORE 1000.0 | GAME 40/40\t\t\n",
      "END PLAY | AVERAGE SCORE 540.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 99 | LOSS 2.4489\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.454339648760282\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 99 | LOSS 1.0276\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.9417984543167628\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 13/13 | CURRENT BATCH COUNT 552 | LOSS 2.9206\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4470399898290633\n",
      "STEP 1015 | SCORE 900.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 567.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 90 | LOSS 2.5020\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4783575595342198\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 90 | LOSS 1.0898\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 1.207827760027005\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 11/11 | CURRENT BATCH COUNT 812 | LOSS 2.8938\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 3.155221911300312\n",
      "STEP 1512 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 645.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 300 | LOSS 2.4546\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4725814174256233\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 300 | LOSS 1.2626\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 1.3859464125363332\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 145 | LOSS 2.5477\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 3.8466071047037844\n",
      "STEP 1034 | SCORE 200.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 547.5 | LOW SCORE 100.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 556 | LOSS 2.4790\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.47319217934328\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 556 | LOSS 1.7837\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 1.415469772161222\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 13/13 | CURRENT BATCH COUNT 876 | LOSS 4.7086\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 4.279997942447663\n",
      "STEP 1284 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 655.0 | LOW SCORE 100.0 | HIGH SCORE 1600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 176 | LOSS 2.4912\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.471803315602816\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 176 | LOSS 1.2234\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 1.5222337764272322\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    agent.run(env, None, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
