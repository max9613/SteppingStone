{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Combined best examples with selected examples, Hypers: (32, 40, 128, 14, 1, (6, 1, 1, 2), 10**-4, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsHierarchy/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese embedding network at the heart of this model, that finds the controllable state.\n",
    "class EMBEDDING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        return self.sin(self.output_layer(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action predictor based on two embedded states.\n",
    "class ACTION_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, train=False):\n",
    "        #x = torch.cat([in_1, in_2], -1)\n",
    "        x = in_2 - in_1\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating network based on two embedded states.\n",
    "class RATING_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, in_1, in_2, final_activation=False):\n",
    "        x = torch.cat([in_1, in_2], -1)\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if not final_activation:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given embedded state, outputs the desired next embedded state.\n",
    "class STRATEGY_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.softmax(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that plays a given game and attempts to achieve a maximum score.\n",
    "# This implementation will only use one agent that contains both selector and strategy networks.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, embedding_size, strategy_count, state_size, action_size, stack_size, layer_counts, learning_rate, gamma, teach_percent, s_device, t_device):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.strategy_count = strategy_count\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.stack_size = stack_size\n",
    "        self.layer_counts = layer_counts\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.teach_percent = teach_percent\n",
    "        self.age = 0\n",
    "        self.s_device = s_device\n",
    "        self.t_device = t_device\n",
    "        # Maps that form the model.\n",
    "        self.embedding_map = EMBEDDING_MAP(state_size * stack_size, embedding_size, layer_counts[0], t_device)\n",
    "        self.action_map = ACTION_MAP(embedding_size, action_size, layer_counts[1], t_device)\n",
    "        self.greedy_rating_map = RATING_MAP(embedding_size + action_size, 2, layer_counts[2], t_device)\n",
    "        self.strategies = [\n",
    "            STRATEGY_MAP(embedding_size, action_size, layer_counts[3], t_device) for _ in range(strategy_count)\n",
    "        ]\n",
    "        # Optimizers for maps.\n",
    "        self.action_embedding_optimizer = torch.optim.Adam(self.action_map.params + self.embedding_map.params, lr=learning_rate)\n",
    "        self.greedy_rating_optimizer = torch.optim.Adam(self.greedy_rating_map.params, lr=learning_rate)\n",
    "        self.strategy_optimizers = [\n",
    "            torch.optim.Adam(s.params, lr=learning_rate) for s in self.strategies\n",
    "        ]\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.alpha = 1/self.action_size\n",
    "        self.exploitation_weight = 0.7\n",
    "        self.exploration_weight = 0.4\n",
    "        \n",
    "    # Trains the action and embedding maps together, this is the only place the embedding map should be trained.\n",
    "    # Groups should be of the form (state (i), state (i+1), action (i))\n",
    "    def train_action_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.action_map(self.embedding_map(inputs_1, True), self.embedding_map(inputs_2, True), True)\n",
    "                peer_out = self.action_map(self.embedding_map(peer_inputs_1, True), self.embedding_map(peer_inputs_2, True), True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rACTION MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.action_embedding_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the greedy rating map.\n",
    "    # Groups should be of the form (state (i), state (i+1), label (i))\n",
    "    def train_greedy_rating_map(self, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(eye[groups[position][1]])\n",
    "            batch[2].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs_1 = torch.stack(batches[i][0])\n",
    "                inputs_2 = torch.stack(batches[i][1])\n",
    "                peer_inputs_1 = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs_1)\n",
    "                peer_inputs_1 = torch.stack(peer_inputs_1)\n",
    "                peer_inputs_2 = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_inputs_2)\n",
    "                peer_inputs_2 = torch.stack(peer_inputs_2)\n",
    "                peer_outputs = [t for t in batches[i][2]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][2]).long()\n",
    "                out = self.greedy_rating_map(self.embedding_map(inputs_1, True).detach(), inputs_2, False)\n",
    "                peer_out = self.greedy_rating_map(self.embedding_map(peer_inputs_1, True).detach(), peer_inputs_2, False)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rGREEDY RATING MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.greedy_rating_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Trains the strategy at the passed index.\n",
    "    # Groups should be of the form (state (i), state (i+1))\n",
    "    def train_strategy_map(self, index, groups, batch_size=1024, epochs=50):\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            #batch[1].append(groups[position][1])\n",
    "            batch[1].append(groups[position][2])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                inputs = self.embedding_map(inputs).detach()\n",
    "                peer_inputs = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_inputs = self.embedding_map(peer_inputs).detach()\n",
    "                peer_outputs = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                #peer_outputs = self.embedding_map(peer_outputs).detach()\n",
    "                outputs = torch.Tensor(batches[i][1]).long()\n",
    "                #outputs = self.embedding_map(outputs)\n",
    "                out = self.strategies[index](inputs, True)\n",
    "                peer_out = self.strategies[index](peer_inputs, True)\n",
    "                loss = self.cross_loss(out, outputs) - (self.alpha * self.cross_loss(peer_out, peer_outputs))\n",
    "                print('\\rSTRATEGY {}/{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(index+1, len(self.strategies), e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.strategy_optimizers[index].step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "    \n",
    "    # Returns labeled examples for a given set, based on a threshold.\n",
    "    def label_groups(self, groups):\n",
    "        groups.sort(key = lambda x: x[3], reverse=True)\n",
    "        good_examples = groups[:int(len(groups) * self.teach_percent)]\n",
    "        bad_examples = groups[int(len(groups) * self.teach_percent):]\n",
    "        examples = []\n",
    "        for group in good_examples:\n",
    "            examples.append((group[0], group[2], 1))\n",
    "        for group in bad_examples:\n",
    "            examples.append((group[0], group[2], 0))\n",
    "        rand.shuffle(examples)\n",
    "        return examples\n",
    "    \n",
    "    # Plays a single game on a compatible enviroment with the provided strategy and returns the score and trajectories.\n",
    "    def play_game_with_strategy(self, env, strategy, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_tensor = None\n",
    "        previous_action = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            previous_action = action\n",
    "            state = observation / 255\n",
    "            frames.append(torch.Tensor.float(torch.from_numpy(state)))\n",
    "            if len(frames) < self.stack_size:\n",
    "                action = rand.randint(0, self.action_size - 1)    \n",
    "            else:\n",
    "                tensor = torch.cat(frames[-self.stack_size:], 0)\n",
    "                embedding = self.embedding_map(tensor).detach()\n",
    "                policy = self.strategies[strategy](embedding)\n",
    "                if sum(policy) <= 0 or min(policy) < 0 or rand.uniform(0,1) < self.gamma:\n",
    "                    action = rand.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                    action = (int(distribution.sample()))\n",
    "                if previous_tensor is not None:\n",
    "                    groups.append((previous_tensor, tensor, previous_action))\n",
    "                previous_tensor = tensor\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_tensor = None\n",
    "                previous_action = None\n",
    "                action = 0\n",
    "                frames = []\n",
    "            step += 1\n",
    "            score += reward\n",
    "            print('\\rSTEP {} | SCORE {} | {}\\t\\t'.format(step, score, extra_info), end = '')\n",
    "        return groups, score\n",
    "    \n",
    "    # Runs the agent on the provided enviroment.\n",
    "    def run(self, env, games, loop, render = False):\n",
    "        running = True\n",
    "        while running:\n",
    "            if not loop:\n",
    "                running = False\n",
    "            # Game playing section.\n",
    "            initial_groups = []\n",
    "            total_score = 0\n",
    "            high_score = None\n",
    "            low_score = None\n",
    "            if games is not None:\n",
    "                for g in range(games):\n",
    "                    group, score = self.play_game(env,render,f'GAME {g+1}/{games}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            else:\n",
    "                for s in range(len(self.strategies)):\n",
    "                    group, score = self.play_game_with_strategy(env,s,render,f'GAME {s+1}/{len(self.strategies)}')\n",
    "                    initial_groups.append((group, score))\n",
    "                    total_score += score\n",
    "                    if high_score is None or high_score < score:\n",
    "                        high_score = score\n",
    "                    if low_score is None or low_score > score:\n",
    "                        low_score = score\n",
    "            avg_score = total_score / len(initial_groups)\n",
    "            print('')\n",
    "            manager.print(f'END PLAY | AVERAGE SCORE {avg_score} | LOW SCORE {low_score} | HIGH SCORE {high_score}')\n",
    "            manager.save()\n",
    "            # Training action and embedding maps section.\n",
    "            action_groups = []\n",
    "            for group in initial_groups:\n",
    "                action_groups += group[0]\n",
    "            a_e_loss = self.train_action_map(action_groups)\n",
    "            print('')\n",
    "            manager.print(f'END ACTION/EMBEDDING TRAINING | AVERAGE LOSS {a_e_loss}')\n",
    "            manager.save()\n",
    "            # Training greedy and exploratory rating maps section. \n",
    "            score_groups = [] # Groups should be of form (state (i), state (i+1), score)\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    score_groups.append((pairs[0], pairs[1], pairs[2], group[1]))\n",
    "            score_groups = self.label_groups(score_groups)\n",
    "            g_r_loss = self.train_greedy_rating_map(score_groups)\n",
    "            print('')\n",
    "            manager.print(f'END GREEDY RATING TRAINING | AVERAGE LOSS {g_r_loss}')\n",
    "            manager.save()\n",
    "            # Training strategies section.\n",
    "            strategies_training_groups = [] # Groups should be of form (state (i), state (i+1))\n",
    "            eye = torch.eye(self.action_size)\n",
    "            for group in initial_groups:\n",
    "                for pairs in group[0]:\n",
    "                    in_1 = self.embedding_map(pairs[0])\n",
    "                    greedy_rating = self.greedy_rating_map(in_1, eye[pairs[2]], True).detach().cpu().numpy()\n",
    "                    if np.argmax(greedy_rating) == 1:\n",
    "                        strategies_training_groups.append((pairs[0], pairs[1], pairs[2]))\n",
    "            initial_groups.sort(key = lambda x: x[1], reverse=True)\n",
    "            teach_groups = initial_groups[:int(len(initial_groups) * self.teach_percent)]\n",
    "            for group in teach_groups:\n",
    "                for pairs in group[0]:\n",
    "                    strategies_training_groups.append((pairs[0], pairs[1], pairs[2]))\n",
    "            losses = []\n",
    "            if len(strategies_training_groups) > 0:\n",
    "                for i in range(len(self.strategies)):\n",
    "                    training_groups = []\n",
    "                    while len(training_groups) < len(strategies_training_groups) / 3:\n",
    "                        index = rand.randint(0, len(strategies_training_groups) - 1)\n",
    "                        training_groups.append(strategies_training_groups[index])\n",
    "                    loss = self.train_strategy_map(i, training_groups)\n",
    "                    losses.append(loss)\n",
    "            avg_strat_loss = sum(losses)/len(losses) if len(losses) > 0 else 'NA'\n",
    "            print('')\n",
    "            manager.print(f'END STRATEGY TRAINING | AVERAGE LOSS {avg_strat_loss}')\n",
    "            manager.save()\n",
    "            self.age += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AGENT(32, 40, 128, 14, 1, (6, 1, 1, 2), 10**-4, 0.9, 1/3, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1846 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 445.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 449 | LOSS 1.3830\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.0777186965942382\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 449 | LOSS 0.5864\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.5884303412717932\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 32 | LOSS 2.3661\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4479319419690544\n",
      "STEP 1093 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 700.0 | LOW SCORE 0.0 | HIGH SCORE 2400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 477 | LOSS -15.5485\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -3.6357748898176045\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 477 | LOSS 0.5935\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.625730375785094\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 942 | LOSS 2.4605\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4654085066715874\n",
      "STEP 1389 | SCORE 1100.0 | GAME 40/40\t\t\n",
      "END PLAY | AVERAGE SCORE 612.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 934 | LOSS -27.3678\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -20.017294459245644\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 934 | LOSS 0.6326\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6119866868184537\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 741 | LOSS 2.4563\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4561294978062307\n",
      "STEP 1391 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 695.0 | LOW SCORE 200.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 857 | LOSS -65.7367\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -42.85015436424399\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 857 | LOSS 0.6162\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6085530564695034\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 989 | LOSS 2.4577\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4568991601268455\n",
      "STEP 1280 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 497.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 647 | LOSS -108.9199\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -86.71695787429809\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 647 | LOSS 0.6033\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6165483008210476\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 20 | LOSS 2.2759\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4512327424628393\n",
      "STEP 987 | SCORE 900.0 | GAME 40/40\t\t\t\t\n",
      "END PLAY | AVERAGE SCORE 550.0 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 420 | LOSS -162.5552\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -142.10578174438476\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 420 | LOSS 0.5317\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.550645869898796\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 12/12 | CURRENT BATCH COUNT 401 | LOSS 2.3980\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4111639335155486\n",
      "STEP 1303 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 555.0 | LOW SCORE 200.0 | HIGH SCORE 1200.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 808 | LOSS -185.9578\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -176.85397503051757\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 808 | LOSS 0.6100\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6323299613237381\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 935 | LOSS 2.3970\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4314195578098294\n",
      "STEP 1321 | SCORE 900.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 502.5 | LOW SCORE 0.0 | HIGH SCORE 1000.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 261 | LOSS -280.0942\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -226.92180733032225\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 261 | LOSS 0.6173\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6011657527446747\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 295 | LOSS 2.5053\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4627404670034134\n",
      "STEP 1376 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 592.5 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 464 | LOSS -347.2208\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -205.24777364693435\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 464 | LOSS 0.6110\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.619754993354573\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 9/9 | CURRENT BATCH COUNT 592 | LOSS 2.4304\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4755544730689794\n",
      "STEP 1108 | SCORE 200.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 682.5 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 976 | LOSS 2.6754\t\t\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS -101.69832443761825\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 976 | LOSS 0.6462\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.624194251871109\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 494 | LOSS 2.4509\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.508773956318696\n",
      "STEP 1808 | SCORE 1000.0 | GAME 40/40\t\t\n",
      "END PLAY | AVERAGE SCORE 570.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 28 | LOSS 2.6298\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.6642258083343506\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 28 | LOSS 0.6333\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6002385571718216\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 838 | LOSS 2.4858\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4994825470646225\n",
      "STEP 1440 | SCORE 1300.0 | GAME 40/40\t\t\n",
      "END PLAY | AVERAGE SCORE 590.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 997 | LOSS 2.5297\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.5618213730812074\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 997 | LOSS 0.6082\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.654357379436493\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 31 | LOSS 2.3813\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.432928392916918\n",
      "STEP 1688 | SCORE 1000.0 | GAME 40/40\t\t\n",
      "END PLAY | AVERAGE SCORE 580.0 | LOW SCORE 0.0 | HIGH SCORE 2100.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 44 | LOSS 2.6022\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.542148728466034\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 44 | LOSS 0.6904\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6217867486476898\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 104 | LOSS 2.5175\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.5358153577021194\n",
      "STEP 1305 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 515.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 364 | LOSS 2.5528\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.597761041384477\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 364 | LOSS 0.5813\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6247019966290547\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 963 | LOSS 2.5006\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.5639893146753314\n",
      "STEP 1236 | SCORE 900.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 645.0 | LOW SCORE 100.0 | HIGH SCORE 2000.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 990 | LOSS 2.5518\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.539986631320073\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 990 | LOSS 0.6228\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6476997942649401\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 86 | LOSS 2.4632\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.5293788886581146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1343 | SCORE 100.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 595.0 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 87 | LOSS 2.5218\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.5606333453982484\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 87 | LOSS 0.5857\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6438460418640398\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 876 | LOSS 2.4030\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4953710993187768\n",
      "STEP 1296 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 585.0 | LOW SCORE 0.0 | HIGH SCORE 1700.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 205 | LOSS 2.5436\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.5041241199493407\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 205 | LOSS 0.6477\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6170453276872635\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 578 | LOSS 2.5658\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.581257408003012\n",
      "STEP 1288 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 545.0 | LOW SCORE 0.0 | HIGH SCORE 1100.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 40 | LOSS 2.4630\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.5049356345946974\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 40 | LOSS 0.5442\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6250153148403534\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 891 | LOSS 2.4964\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.549281317075094\n",
      "STEP 1181 | SCORE 100.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 500.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 254 | LOSS 2.4782\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.508836901187897\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 254 | LOSS 0.6206\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.665700923037529\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 741 | LOSS 2.5399\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.599495716929436\n",
      "STEP 1425 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 547.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 724 | LOSS 2.4987\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4937718686278987\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 724 | LOSS 0.5860\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6411252761860283\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 592 | LOSS 2.5920\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.609409825325012\n",
      "STEP 1275 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 597.5 | LOW SCORE 0.0 | HIGH SCORE 2000.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 628 | LOSS 2.4851\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.488054236056758\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 628 | LOSS 0.6011\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6381141655585345\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 204 | LOSS 2.3871\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.4976646654307837\n",
      "STEP 1371 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 537.5 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 894 | LOSS 2.4936\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4844214822769164\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 894 | LOSS 0.6212\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6370897512197494\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 145 | LOSS 2.6308\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.5180503605008124\n",
      "STEP 1026 | SCORE 100.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 602.5 | LOW SCORE 0.0 | HIGH SCORE 2000.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 364 | LOSS 2.5051\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.5035578930611706\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 364 | LOSS 0.6438\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6916654158807268\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 10/10 | CURRENT BATCH COUNT 449 | LOSS 2.2520\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.461320284688473\n",
      "STEP 1322 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 635.0 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 605 | LOSS 2.4698\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.491525722391465\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 605 | LOSS 0.6127\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6623535321273055\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 250 | LOSS 2.4372\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.592862919256091\n",
      "STEP 1247 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 535.0 | LOW SCORE 0.0 | HIGH SCORE 1300.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 17 | LOSS 2.4625\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4819641048174637\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 17 | LOSS 0.6352\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6346053303663547\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 52 | LOSS 2.3004\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.610277801424265\n",
      "STEP 1209 | SCORE 100.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 522.5 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 507 | LOSS 2.4707\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4725592930793763\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 507 | LOSS 0.6213\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6386463543176651\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 103 | LOSS 2.4248\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.6061190206557514\n",
      "STEP 1326 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 592.5 | LOW SCORE 100.0 | HIGH SCORE 1700.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 575 | LOSS 2.4701\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4644906498909\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 575 | LOSS 0.6192\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6609429112434387\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 943 | LOSS 2.4201\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.625839318854468\n",
      "STEP 1476 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 582.5 | LOW SCORE 100.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 865 | LOSS 2.4611\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.466713065642577\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 865 | LOSS 0.6219\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.7012449568510055\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 667 | LOSS 2.3949\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.6518774863992425\n",
      "STEP 1281 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 587.5 | LOW SCORE 100.0 | HIGH SCORE 2100.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 573 | LOSS 2.4853\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4720166235703687\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 573 | LOSS 0.6285\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6916033908495536\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 501 | LOSS 2.6360\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.665482916325332\n",
      "STEP 1167 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 550.0 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 997 | LOSS 2.4568\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4696691378593445\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 997 | LOSS 0.6071\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6160585270166397\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 12 | LOSS 2.7349\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.7678815154007506\n",
      "STEP 1161 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 435.0 | LOW SCORE 0.0 | HIGH SCORE 1700.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 900 | LOSS 2.4633\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.466770206781534\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 900 | LOSS 0.6001\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6166558841558604\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 837 | LOSS 2.9308\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.795104904992239\n",
      "STEP 1205 | SCORE 700.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 605.0 | LOW SCORE 0.0 | HIGH SCORE 1700.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 206 | LOSS 2.4683\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.465044337908427\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 206 | LOSS 0.7101\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6923783942297393\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 9/9 | CURRENT BATCH COUNT 32 | LOSS 2.3252\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.6204113487270146\n",
      "STEP 1296 | SCORE 0.0 | GAME 40/40\t\t\t\t\t\n",
      "END PLAY | AVERAGE SCORE 562.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 276 | LOSS 2.4802\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.463935268330124\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 53/53 | CURRENT BATCH COUNT 276 | LOSS 0.6041\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6986237385812796\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 224 | LOSS 2.6935\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.6979206699132923\n",
      "STEP 1159 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 585.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 49 | LOSS 2.4294\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.463124274740032\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 49 | LOSS 0.6633\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6910283360761754\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 897 | LOSS 2.5219\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.7483060098205288\n",
      "STEP 1322 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 547.5 | LOW SCORE 0.0 | HIGH SCORE 1800.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 414 | LOSS 2.4612\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4621294746398927\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 414 | LOSS 0.6563\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6733434191703797\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 914 | LOSS 2.9022\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.87433066034317\n",
      "STEP 1071 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 575.0 | LOW SCORE 200.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 41 | LOSS 2.4359\t\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.464275827033847\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 41 | LOSS 0.5536\t\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6624972064588585\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 757 | LOSS 2.7277\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.9088939055800442\n",
      "STEP 1182 | SCORE 600.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 445.0 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 48/48 | CURRENT BATCH COUNT 217 | LOSS 2.4576\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4624032441775006\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 48/48 | CURRENT BATCH COUNT 217 | LOSS 0.6132\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.7460911519825458\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 463 | LOSS 3.0042\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.8709266653855643\n",
      "STEP 1329 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 550.0 | LOW SCORE 0.0 | HIGH SCORE 2400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 637 | LOSS 2.4699\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4623581889959483\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 637 | LOSS 0.6138\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6481052605693157\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 80 | LOSS 2.7980\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.8158509630709885\n",
      "STEP 1110 | SCORE 200.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 517.5 | LOW SCORE 100.0 | HIGH SCORE 1600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 522 | LOSS 2.4677\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.462029759650137\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 51/51 | CURRENT BATCH COUNT 522 | LOSS 0.5958\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6663922748846166\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 9/9 | CURRENT BATCH COUNT 436 | LOSS 2.5460\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.724695050583946\n",
      "STEP 1395 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 547.5 | LOW SCORE 0.0 | HIGH SCORE 1400.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 346 | LOSS 2.4620\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.463720135980723\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 346 | LOSS 0.6147\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6968595540523529\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 10/10 | CURRENT BATCH COUNT 155 | LOSS 2.4295\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.6759961881518364\n",
      "STEP 1143 | SCORE 100.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 412.5 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 985 | LOSS 2.4568\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4642259066445487\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 985 | LOSS 0.6121\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.7120441650127878\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 701 | LOSS 2.5960\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.867615406053407\n",
      "STEP 1226 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 517.5 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 740 | LOSS 2.4752\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.464520344161987\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 740 | LOSS 0.6146\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6712865116834641\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 6/6 | CURRENT BATCH COUNT 977 | LOSS 2.8723\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.988557235697905\n",
      "STEP 1209 | SCORE 0.0 | GAME 40/40\t\t\t\t\t\n",
      "END PLAY | AVERAGE SCORE 530.0 | LOW SCORE 0.0 | HIGH SCORE 1800.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 935 | LOSS 2.4506\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4673643063581907\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 935 | LOSS 0.6321\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.7164189141301008\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 883 | LOSS 2.4982\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.8868682214170702\n",
      "STEP 1434 | SCORE 900.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 570.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 645 | LOSS 2.4515\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4662635518074034\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 50/50 | CURRENT BATCH COUNT 645 | LOSS 0.6074\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6675988632678985\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 8/8 | CURRENT BATCH COUNT 66 | LOSS 3.2760\t\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.8296000429391865\n",
      "STEP 1077 | SCORE 400.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 537.5 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 140 | LOSS 2.4721\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.469026996630889\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 52/52 | CURRENT BATCH COUNT 140 | LOSS 0.6399\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6640391185191962\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 9/9 | CURRENT BATCH COUNT 271 | LOSS 2.6097\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.87392604581515\n",
      "STEP 1046 | SCORE 200.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 555.0 | LOW SCORE 100.0 | HIGH SCORE 1400.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 835 | LOSS 2.4553\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.467666234483524\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 835 | LOSS 0.6450\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.761982296194349\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 9/9 | CURRENT BATCH COUNT 973 | LOSS 2.5973\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.8976746359798646\n",
      "STEP 1068 | SCORE 300.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 500.0 | LOW SCORE 0.0 | HIGH SCORE 1500.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 48/48 | CURRENT BATCH COUNT 818 | LOSS 2.4939\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.468165883918603\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 48/48 | CURRENT BATCH COUNT 818 | LOSS 0.6535\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.9357760482281446\n",
      "STRATEGY 40/40 | EPOCH 50/50 | BATCH 7/7 | CURRENT BATCH COUNT 938 | LOSS 2.5946\t\t\t\n",
      "END STRATEGY TRAINING | AVERAGE LOSS 2.960053628597941\n",
      "STEP 1194 | SCORE 500.0 | GAME 40/40\t\t\t\n",
      "END PLAY | AVERAGE SCORE 542.5 | LOW SCORE 0.0 | HIGH SCORE 1600.0\n",
      "ACTION MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 573 | LOSS 2.4572\t\t\t\n",
      "END ACTION/EMBEDDING TRAINING | AVERAGE LOSS 2.4671746121620646\n",
      "GREEDY RATING MAP | EPOCH 50/50 | BATCH 49/49 | CURRENT BATCH COUNT 573 | LOSS 0.6455\t\t\t\n",
      "END GREEDY RATING TRAINING | AVERAGE LOSS 0.6894236732988941\n",
      "STRATEGY 24/40 | EPOCH 48/50 | BATCH 3/6 | CURRENT BATCH COUNT 1024 | LOSS 2.7848\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    agent.run(env, None, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
