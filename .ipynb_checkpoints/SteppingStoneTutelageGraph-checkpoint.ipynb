{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the centroids and pairings for the given data.\n",
    "# Data is in the form (tensor, action).\n",
    "def get_centroids_and_assignments(data, mse, num_of_centroids=2):\n",
    "    attempts = 0\n",
    "    centroids = []\n",
    "    indexes = [i for i in range(len(data))]\n",
    "    rand.shuffle(indexes)\n",
    "    for i in range(num_of_centroids):\n",
    "        centroids.append(data[indexes[i]][0].detach())\n",
    "    running = True\n",
    "    groups = [[] for _ in range(num_of_centroids)]\n",
    "    while running:\n",
    "        groups = [[] for _ in range(num_of_centroids)]\n",
    "        for d in data:\n",
    "            dists = []\n",
    "            for c in centroids:\n",
    "                dists.append(mse(d[0], c).detach().cpu().numpy())\n",
    "            index = np.argmin(dists)\n",
    "            groups[index].append(d)\n",
    "        if attempts < 10000:\n",
    "            new_centroids = []\n",
    "            total_dist = 0\n",
    "            for g in range(len(groups)):\n",
    "                centroid = None\n",
    "                for d in groups[g]:\n",
    "                    if centroid is None:\n",
    "                        centroid = d[0].detach()\n",
    "                    else:\n",
    "                        centroid += d[0].detach()\n",
    "                if centroid is None:\n",
    "                    index = rand.randint(0, len(data) - 1)\n",
    "                    centroid = data[index][0].detach()\n",
    "                else:\n",
    "                    centroid /= len(groups[g])\n",
    "                total_dist += mse(centroids[g], centroid).detach().cpu().numpy()\n",
    "                new_centroids.append(centroid)\n",
    "            running = total_dist != 0\n",
    "            centroids = new_centroids\n",
    "            attempts += 1\n",
    "        else:\n",
    "            running = False\n",
    "    return centroids, groups\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Graph node, contains the two centroids of the decision point if non-leaf, else contains action distribution.\n",
    "class NODE:\n",
    "    \n",
    "    # Constructor.\n",
    "    # Data is in the form (tensor, action).\n",
    "    def __init__(self, data, number_of_actions, max_depth, depth=0, missing_actions=None, action_weight=0.9):\n",
    "        # MSE for determining centroid associations during policy fetch.\n",
    "        self.mse = nn.MSELoss()\n",
    "        action_groups = [[] for _ in range(number_of_actions)]\n",
    "        for d in data:\n",
    "            action_groups[d[1]].append(d)\n",
    "        group_sizes = [len(g) for g in action_groups]\n",
    "        non_empty_groups = [1 if g > 0 else 0 for g in group_sizes]\n",
    "        # On first node in tree find all missing actions so they can later be added in at very low probability to all\n",
    "        # leaf node policies.\n",
    "        if missing_actions is None:\n",
    "            missing_actions = [0 for _ in range(number_of_actions)]\n",
    "            for i in range(len(non_empty_groups)):\n",
    "                if non_empty_groups[i] == 0:\n",
    "                    missing_actions[i] = 1\n",
    "        # If data is empty make completely random.\n",
    "        if len(data) <= 0:\n",
    "            self.leaf = True\n",
    "            self.centroids = None\n",
    "            self.children = None\n",
    "            self.policy = [1/number_of_actions for _ in range(number_of_actions)]\n",
    "        # When only one action is present make this a leaf node.\n",
    "        elif sum(non_empty_groups) == 1:\n",
    "            self.leaf = True\n",
    "            self.centroids = None\n",
    "            self.children = None\n",
    "            self.policy = [0 for _ in range(number_of_actions)]\n",
    "            if sum(missing_actions) > 0:\n",
    "                self.policy[np.argmax(non_empty_groups)] = action_weight # Set probability of actual action to the action_weight.\n",
    "                # Distribute rest over the missing actions\n",
    "                total_missing = sum(missing_actions)\n",
    "                for i in range(len(self.policy)):\n",
    "                    if missing_actions[i] == 1:\n",
    "                        self.policy[i] = (1-action_weight) / total_missing\n",
    "            else:\n",
    "                self.policy[np.argmax(non_empty_groups)] = 1\n",
    "        # When there are only two actions, convert them into their respective centroids, \n",
    "        # and create the resulting children.\n",
    "        elif sum(non_empty_groups) == 2:\n",
    "            self.leaf = False\n",
    "            self.centroids = []\n",
    "            self.children = []\n",
    "            policy = None\n",
    "            for i in range(len(action_groups)):\n",
    "                if group_sizes[i] > 0:\n",
    "                    centroid = None\n",
    "                    for d in action_groups[i]:\n",
    "                        if centroid is None:\n",
    "                            centroid = d[0].detach()\n",
    "                        else:\n",
    "                            centroid += d[0].detach()\n",
    "                    centroid /= len(action_groups[i])\n",
    "                    self.centroids.append(centroid)\n",
    "                    self.children.append(NODE(action_groups[i], number_of_actions, max_depth, depth + 1, missing_actions, action_weight))\n",
    "        # When there are more than 2 action groups and max depth - 1 is reached create a child for each group.\n",
    "        elif depth == max_depth - 1:\n",
    "            self.leaf = False\n",
    "            self.centroids = []\n",
    "            self.children = []\n",
    "            policy = None\n",
    "            for i in range(len(action_groups)):\n",
    "                if group_sizes[i] > 0:\n",
    "                    centroid = None\n",
    "                    for d in action_groups[i]:\n",
    "                        if centroid is None:\n",
    "                            centroid = d[0].detach()\n",
    "                        else:\n",
    "                            centroid += d[0].detach()\n",
    "                    centroid /= len(action_groups[i])\n",
    "                    self.centroids.append(centroid)\n",
    "                    self.children.append(NODE(action_groups[i], number_of_actions, max_depth, depth + 1, missing_actions, action_weight))\n",
    "        # Otherwise just create a binary split point of centroids.\n",
    "        else:\n",
    "            self.leaf = False\n",
    "            self.children = []\n",
    "            policy = None\n",
    "            centroids, groups = get_centroids_and_assignments(data, self.mse, 2)\n",
    "            self.centroids = centroids\n",
    "            for g in groups:\n",
    "                self.children.append(NODE(g, number_of_actions, max_depth, depth + 1, missing_actions, action_weight))\n",
    "\n",
    "            \n",
    "    # Returns the policy for the passed tensor.\n",
    "    def get_policy(self, tensor):\n",
    "        if self.leaf:\n",
    "            return self.policy\n",
    "        else:\n",
    "            dists = []\n",
    "            for c in self.centroids:\n",
    "                dists.append(self.mse(tensor, c).detach().cpu().numpy())\n",
    "            return self.children[np.argmin(dists)].get_policy(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, stack_size, max_depth, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.stack_size = stack_size\n",
    "        self.max_depth = max_depth\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.graph = None\n",
    "    \n",
    "    # Generates a new graph based on the provided data.\n",
    "    # Data is in the form (tensor, action).\n",
    "    def generate_new_graph(self, data):\n",
    "        self.graph = NODE(data, self.action_size, self.max_depth)\n",
    "        self.age += 1\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        overall_step = 0\n",
    "        lives = 4\n",
    "        action = 0\n",
    "        groups = []\n",
    "        frames = []\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            frames.append(torch.Tensor.float(torch.from_numpy(observation)))\n",
    "            if len(frames) < self.stack_size:\n",
    "                action = rand.randint(0, self.action_size - 1)\n",
    "            else:\n",
    "                state = torch.cat(frames[-self.stack_size:], 0)\n",
    "                if self.graph is None:\n",
    "                    action = rand.randint(0, self.action_size - 1)\n",
    "                else:\n",
    "                    policy = torch.Tensor(self.graph.get_policy(state))\n",
    "                    policy[action] = 0\n",
    "                    if sum(policy) > 0:\n",
    "                        distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                        action = int(distribution.sample())\n",
    "                    else:\n",
    "                        action = rand.randint(0, self.action_size - 1)\n",
    "                groups.append((state, action))\n",
    "            score += reward\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                action = 0\n",
    "            overall_step += 1\n",
    "        return groups, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, cutoff, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.agent_params = agent_params\n",
    "        self.cutoff = cutoff\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('\\nEND RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        new_pop.sort(key = lambda x: x[2])\n",
    "        teach_pop = new_pop[int(len(new_pop) * self.cutoff):]\n",
    "        train_pop = new_pop[:int(len(new_pop) * (1-self.cutoff))]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            #if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "            #    agent = self.replace_agent(agent)\n",
    "            trajectories = []\n",
    "            for _ in range(int(len(examples) / 3)):\n",
    "                index = rand.randint(0, len(examples) - 1)\n",
    "                trajectories.append(examples[index])\n",
    "            print(f'\\r{agent.name} | {count+1}/{len(train_pop)}',end='')\n",
    "            agent.generate_new_graph(trajectories)\n",
    "            count += 1\n",
    "        print('\\nEND TRAINING POPULATION')\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 5, 20, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(30, 1, agent_hyper, 1/3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_17 | STEP 1229 | SCORE 500.0 | AGE 1 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 573.3333333333334 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_28 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_2 | STEP 1331 | SCORE 1100.0 | AGE 1 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 576.6666666666666 | LOW SCORE 0.0 | HIGH SCORE 2100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_12 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_20 | STEP 1018 | SCORE 0.0 | AGE 3 | MEMBER 30/30 | GAME 1/1\t\t1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 746.6666666666666 | LOW SCORE 0.0 | HIGH SCORE 2000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_28 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_2 | STEP 1203 | SCORE 300.0 | AGE 1 | MEMBER 30/30 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 610.0 | LOW SCORE 0.0 | HIGH SCORE 1900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_17 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_10 | STEP 1318 | SCORE 1400.0 | AGE 2 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 893.3333333333334 | LOW SCORE 0.0 | HIGH SCORE 3200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_5 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_14 | STEP 1442 | SCORE 600.0 | AGE 4 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 843.3333333333334 | LOW SCORE 200.0 | HIGH SCORE 2000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_0 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_9 | STEP 1207 | SCORE 300.0 | AGE 5 | MEMBER 30/30 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 790.0 | LOW SCORE 0.0 | HIGH SCORE 2500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_19 | STEP 1430 | SCORE 1100.0 | AGE 7 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 890.0 | LOW SCORE 0.0 | HIGH SCORE 3000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 8\n",
      "AGENT_15 | STEP 1333 | SCORE 300.0 | AGE 7 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 993.3333333333334 | LOW SCORE 0.0 | HIGH SCORE 2400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_25 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 9\n",
      "AGENT_19 | STEP 1506 | SCORE 2000.0 | AGE 8 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 980.0 | LOW SCORE 0.0 | HIGH SCORE 2200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_5 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 10\n",
      "AGENT_25 | STEP 1174 | SCORE 1000.0 | AGE 6 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1036.6666666666667 | LOW SCORE 100.0 | HIGH SCORE 2300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_1 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 11\n",
      "AGENT_28 | STEP 1187 | SCORE 400.0 | AGE 8 | MEMBER 30/30 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 930.0 | LOW SCORE 0.0 | HIGH SCORE 2700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_24 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 12\n",
      "AGENT_20 | STEP 1051 | SCORE 300.0 | AGE 9 | MEMBER 30/30 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 906.6666666666666 | LOW SCORE 100.0 | HIGH SCORE 2100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 13\n",
      "AGENT_5 | STEP 1703 | SCORE 1000.0 | AGE 11 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 946.6666666666666 | LOW SCORE 200.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_1 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 14\n",
      "AGENT_28 | STEP 1486 | SCORE 2000.0 | AGE 11 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 823.3333333333334 | LOW SCORE 0.0 | HIGH SCORE 2100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_25 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 15\n",
      "AGENT_3 | STEP 1698 | SCORE 1700.0 | AGE 12 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1066.6666666666667 | LOW SCORE 0.0 | HIGH SCORE 2900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_10 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 16\n",
      "AGENT_28 | STEP 1664 | SCORE 1000.0 | AGE 12 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1143.3333333333333 | LOW SCORE 0.0 | HIGH SCORE 4600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_29 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 17\n",
      "AGENT_10 | STEP 1570 | SCORE 800.0 | AGE 12 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1133.3333333333333 | LOW SCORE 0.0 | HIGH SCORE 4600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_26 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 18\n",
      "AGENT_18 | STEP 1171 | SCORE 500.0 | AGE 11 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 963.3333333333334 | LOW SCORE 0.0 | HIGH SCORE 2500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_13 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 19\n",
      "AGENT_12 | STEP 1595 | SCORE 400.0 | AGE 15 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1070.0 | LOW SCORE 200.0 | HIGH SCORE 2900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_7 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 20\n",
      "AGENT_26 | STEP 1564 | SCORE 500.0 | AGE 12 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1050.0 | LOW SCORE 0.0 | HIGH SCORE 3700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_21 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 21\n",
      "AGENT_21 | STEP 1602 | SCORE 1000.0 | AGE 15 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1090.0 | LOW SCORE 100.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_15 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 22\n",
      "AGENT_25 | STEP 1270 | SCORE 500.0 | AGE 17 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1070.0 | LOW SCORE 200.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_8 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 23\n",
      "AGENT_16 | STEP 1617 | SCORE 1300.0 | AGE 17 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 900.0 | LOW SCORE 0.0 | HIGH SCORE 2700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_7 | 20/200\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 24\n",
      "AGENT_19 | STEP 1116 | SCORE 1000.0 | AGE 17 | MEMBER 30/30 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1090.0 | LOW SCORE 0.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_18 | 20/20\n",
      "END TRAINING POPULATION\n",
      "BEGIN RUNNING POPULATION | GENERATION 25\n",
      "AGENT_4 | STEP 1827 | SCORE 2200.0 | AGE 16 | MEMBER 30/30 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1000.0 | LOW SCORE 0.0 | HIGH SCORE 2800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_1 | 2/200"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
