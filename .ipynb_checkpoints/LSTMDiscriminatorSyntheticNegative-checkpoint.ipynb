{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Hypers: (40, 1, agent_params, 1/10, 1/5, 1000)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsDiscriminator/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single LSTM cell.\n",
    "class LSTM_CELL(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, cell_size, hidden_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.cell_size = cell_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.t_device = t_device\n",
    "        self.cell_forget_gate = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.cell_update_gate_sigmoid = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.cell_update_gate_tanh = nn.Linear(input_size + hidden_size, cell_size)\n",
    "        self.hidden_dim_reduce = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    # Forward propogates the input through the cell and produces a new cell and hidden state.\n",
    "    def forward(self, x, cell_state, hidden_state):\n",
    "        x = torch.cat([hidden_state, x], dim=-1)\n",
    "        cell_state *= self.sigmoid(self.cell_forget_gate(x))\n",
    "        cell_state += self.sigmoid(self.cell_update_gate_sigmoid(x)) + self.tanh(self.cell_update_gate_tanh(x))\n",
    "        hidden_state = self.sigmoid(self.hidden_dim_reduce(x)) * self.tanh(cell_state)\n",
    "        return cell_state, hidden_state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based discriminator.\n",
    "class DISCRIMINATOR(nn.Module):\n",
    "    \n",
    "    # The input thrown into the LSTM will be the concat of the distribution and state.\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, cell_size, hidden_size, distribution_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.distribution_size = distribution_size\n",
    "        self.t_device = t_device\n",
    "        self.lstm_cell = LSTM_CELL(input_size + distribution_size, cell_size, hidden_size, t_device)\n",
    "        self.hidden_1 = nn.Linear(hidden_size, int(hidden_size * (3/4)))\n",
    "        self.hidden_2 = nn.Linear(int(hidden_size * (3/4)), int(hidden_size * (1/2)))\n",
    "        self.hidden_3 = nn.Linear(int(hidden_size * (1/2)), int(hidden_size * (1/4)))\n",
    "        self.output = nn.Linear(int(hidden_size * (1/4)), 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.act_func = self.relu\n",
    "        \n",
    "    # Forward progogates the provided input through the network and returns the corresponding labels and inner states.\n",
    "    # Cell and hidden states should be pre-stacked to the correct sizing to match the state and distribution batch size.\n",
    "    # The inner states will be a stack of tensors with sizing equal to the batch sizing.\n",
    "    def forward(self, states, distributions, cell_states, hidden_states, training=False):\n",
    "        outputs = []\n",
    "        for index in range(len(states)):\n",
    "            x = torch.cat([states[index], distributions[index]], dim=-1)\n",
    "            cell_state, hidden_state = self.lstm_cell(x, cell_states, hidden_states)\n",
    "            x = self.act_func(self.hidden_1(hidden_state))\n",
    "            x = self.act_func(self.hidden_2(x))\n",
    "            x = self.act_func(self.hidden_3(x))\n",
    "            x = self.output(x)\n",
    "            if not training:\n",
    "                x = self.sigmoid(x)\n",
    "            outputs.append(x)\n",
    "        return outputs, cell_states, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feedforward generator.\n",
    "class GENERATOR(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, distribution_size, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.distribution_size = distribution_size\n",
    "        self.t_device = t_device\n",
    "        self.sin = torch.sin\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        weights = generate_weights(self.input_size, self.distribution_size, 2)\n",
    "        self.hidden_1 = nn.Linear(input_size, weights[0])\n",
    "        self.hidden_2 = nn.Linear(weights[0], weights[1])\n",
    "        self.output = nn.Linear(weights[1], distribution_size)\n",
    "        \n",
    "    # Forward propogate input states.\n",
    "    def forward(self, x):\n",
    "        x = self.sin(self.hidden_1(x))\n",
    "        x = self.sin(self.hidden_2(x))\n",
    "        return self.sigmoid(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that combines a discriminator and generator to play a game.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    # learning_rates[0] = discriminator learning rate & learning_rate[1] = generator learning_rate.\n",
    "    def __init__(self, name, learning_rates, input_size, hidden_size, action_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.learning_rates = learning_rates\n",
    "        self.input_size = input_size\n",
    "        self.cell_size = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.action_size = action_size\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.discriminator = DISCRIMINATOR(input_size, self.cell_size, hidden_size, action_size, t_device)\n",
    "        self.generator = GENERATOR(input_size, action_size, t_device)\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=learning_rates[0])\n",
    "        self.generator_optimizer = torch.optim.Adam(self.generator.parameters(), lr=learning_rates[1])\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.age = 0\n",
    "        self.threshold = rand.uniform(0.5, 1)\n",
    "    # Trains the generator to maximize the discriminator's output on the provided sequence of states.\n",
    "    def train_generator(self, states, cell_state, hidden_state, epochs):\n",
    "        self.generator_optimizer.zero_grad()\n",
    "        for e in range(epochs):\n",
    "            current_cell_state = cell_state\n",
    "            current_hidden_state = hidden_state\n",
    "            loss = 0\n",
    "            for state in states:\n",
    "                distribution = self.generator(state)\n",
    "                out, current_cell_state, current_hidden_state = self.discriminator([state], [distribution], current_cell_state, current_hidden_state, True)\n",
    "                loss += self.bce_loss(out[0], torch.ones(1))\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.generator_optimizer.step()\n",
    "            \n",
    "    # Trains the discriminator on the provided trajectories.\n",
    "    # Trajectories should be of the form (label, groups)\n",
    "    # Groups should just be a list of sequential gameplay generated pairings of size = unroll_size.\n",
    "    def train_discriminator(self, trajectories, unroll_size, batch_size=64, epochs=50, extra_info=''):\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "        batches = []\n",
    "        states = [[] for _ in range(unroll_size)]\n",
    "        distributions_positive = [[] for _ in range(unroll_size)]\n",
    "        distributions_negative = [[] for _ in range(unroll_size)]\n",
    "        rand.shuffle(trajectories)\n",
    "        for trajectory in trajectories:\n",
    "            groups = trajectory\n",
    "            for i in range(len(groups)):\n",
    "                states[i].append(groups[i][0])\n",
    "                distributions_positive[i].append(groups[i][1])\n",
    "                distributions_negative[i].append(torch.rand(self.action_size))\n",
    "            if len(states[0]) >= batch_size:\n",
    "                states = [torch.stack(state) for state in states]\n",
    "                distributions_positive = [torch.stack(dist) for dist in distributions_positive]\n",
    "                distributions_negative = [torch.stack(dist) for dist in distributions_negative]\n",
    "                batches.append((states, distributions_positive, distributions_negative, len(states[0])))\n",
    "                states = [[] for _ in range(unroll_size)]\n",
    "                distributions_positive = [[] for _ in range(unroll_size)]\n",
    "                distributions_negative = [[] for _ in range(unroll_size)]\n",
    "        if len(states[0]) > 0:\n",
    "            states = [torch.stack(state) for state in states]\n",
    "            distributions_positive = [torch.stack(dist) for dist in distributions_positive]\n",
    "            distributions_negative = [torch.stack(dist) for dist in distributions_negative]\n",
    "            batches.append((states, distributions_positive, distributions_negative, len(states[0])))\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            losses = []\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                states = batch[0]\n",
    "                positive_distributions = batch[1]\n",
    "                negative_distributions = batch[2]\n",
    "                actual_size = batch[3]\n",
    "                # These two assignments might need to be messed with to get everything working.\n",
    "                cell_state = torch.zeros(actual_size, self.cell_size)\n",
    "                hidden_state = torch.zeros(actual_size, self.hidden_size)\n",
    "                positive_outputs, _, _ = self.discriminator(states, positive_distributions, cell_state, hidden_state, True)\n",
    "                negative_outputs, _, _ = self.discriminator(states, negative_distributions, cell_state, hidden_state, True)\n",
    "                loss = 0\n",
    "                for o in range(len(positive_outputs)):\n",
    "                    loss += self.bce_loss(positive_outputs[o], torch.ones(actual_size, 1)) + self.bce_loss(negative_outputs[o], torch.zeros(actual_size, 1))\n",
    "                loss.backward()\n",
    "                self.discriminator_optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                print('\\rTRAINING {} | AGE {} | BATCH {}/{} | EPOCH {}/{} | LOSS {} {}'.format(self.name, self.age, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "                \n",
    "            \n",
    "    # Plays the provided game.\n",
    "    # A group has the following content: (state, distribution, verdict, cell_state, hidden_state)\n",
    "    def play_game(self, env, generator_epochs, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        cell_state = torch.zeros(self.cell_size)\n",
    "        hidden_state = torch.zeros(self.hidden_size)\n",
    "        groups = []\n",
    "        group = []\n",
    "        approves = []\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} | THRESHOLD {:0.2f} | APPROVAL {} {}\\t\\t'.format(self.name, step, score, self.age, self.threshold, sum(approves)/len(approves) if len(approves) > 0 else 0, extra_info), end = '')\n",
    "            score += reward\n",
    "            step += 1\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policy = self.generator(tensor)\n",
    "            verdict, possible_cell_state, possible_hidden_state = self.discriminator([tensor], [policy], cell_state, hidden_state)\n",
    "            if verdict[0] < self.threshold:\n",
    "                approves.append(0)\n",
    "                states = [tensor]\n",
    "                past_cell_state = cell_state.detach()\n",
    "                past_hidden_state = hidden_state.detach()\n",
    "                states = states[::-1]\n",
    "                self.train_generator(states, past_cell_state, past_hidden_state, generator_epochs)\n",
    "                policy = self.generator(tensor)\n",
    "                group.append((tensor.detach(), policy.detach(), 0, cell_state.detach(), hidden_state.detach()))\n",
    "                verdict, cell_state, hidden_state = self.discriminator([tensor], [policy], cell_state, hidden_state)\n",
    "            else:\n",
    "                approves.append(1)\n",
    "                group.append((tensor.detach(), policy.detach(), 1, cell_state.detach(), hidden_state.detach()))\n",
    "                cell_state = possible_cell_state\n",
    "                hidden_state = possible_hidden_state            \n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.action_size - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                lives = info['ale.lives']\n",
    "                cell_state = torch.zeros(self.cell_size)\n",
    "                hidden_state = torch.zeros(self.hidden_size)\n",
    "                group = []\n",
    "            if render:\n",
    "                env.render()\n",
    "        return groups, score, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        \n",
    "    # Discretizes and labels the provided trajectories.\n",
    "    def prepare_data(self, positive_examples, negative_examples, unroll_depth):\n",
    "        trajectories = []\n",
    "        for groups in positive_examples:\n",
    "            index = unroll_depth\n",
    "            while index < len(groups):\n",
    "                trajectories.append(groups[index-unroll_depth:index])\n",
    "                index += 1\n",
    "        rand.shuffle(trajectories)\n",
    "        return trajectories\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, generator_epochs, unroll_depth, epochs, batch_size, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, generator_epochs, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        for exp in teach_pop:\n",
    "            positive_examples += exp[1]\n",
    "        trajectories = self.prepare_data(positive_examples, negative_examples, unroll_depth)\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            # ADD BACK IN AGENT DEATH.\n",
    "            \n",
    "            #if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "            #    agent = self.replace_agent(agent)\n",
    "            rand.shuffle(trajectories)\n",
    "            loss = agent.train_discriminator(trajectories, unroll_depth, batch_size, epochs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}')\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        manager.save()\n",
    "        self.generation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = ([0.0001,0.1], 128, 100, 14, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = POPULATION(40, 1, agent_params, 1/10, 1/5, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_8 | STEP 1124 | SCORE 800.0 | AGE 0 | THRESHOLD 0.77 | APPROVAL 0.0 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1027.5 | LOW SCORE 0.0 | HIGH SCORE 8400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_33 | AGE 0 | BATCH 125/125 | EPOCH 10/10 | LOSS 68.03379821777344 | MEMBER 8/88\n",
      "END TRAINING POPULATION | AVG LOSS 69.41746123695373\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_27 | STEP 1936 | SCORE 2300.0 | AGE 0 | THRESHOLD 0.89 | APPROVAL 0.0 | MEMBER 40/40 | GAME 1/1\t\t40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2050.0 | LOW SCORE 0.0 | HIGH SCORE 8200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_14 | AGE 0 | BATCH 129/129 | EPOCH 10/10 | LOSS 27.748254776000977 | MEMBER 8/8\n",
      "END TRAINING POPULATION | AVG LOSS 43.001579354437744\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_2 | STEP 733 | SCORE 500.0 | AGE 0 | THRESHOLD 0.93 | APPROVAL 0.0 | MEMBER 37/40 | GAME 1/1\t\t1\t\t0 | GAME 1/1\t\t\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, 2, 50, 10, 64, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
