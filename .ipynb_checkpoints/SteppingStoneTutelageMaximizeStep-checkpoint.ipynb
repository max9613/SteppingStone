{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Switched metric to score*step, Hypers: (60, 1, agent_hyper, 1/4, 1/4, 10, 0.3)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsSTEP/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if train:\n",
    "                outputs.append(out(x))\n",
    "            else:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.3\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(state_size * stack_size, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()#nn.MSELoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=10):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        self.alpha = 1/(2+(self.age/4))\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                #outputs = [torch.stack(o) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                # Peer inputs and outputs for peer loss function implementation.\n",
    "                peer_inputs = [b for b in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_outputs = [[a for a in o] for o in batches[i][1]]\n",
    "                for o in peer_outputs:\n",
    "                    rand.shuffle(o)\n",
    "                peer_outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs.to(self.t_device), train=True)\n",
    "                peer_outs = self.policy_map(peer_inputs.to(self.t_device), train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor)\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        tensor = torch.cat(tensors, 0)\n",
    "                    groups.append((previous_state, action_queue, tensor.detach().cpu().numpy()))\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor)\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score, overall_step\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff, alpha):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            # Sort by step.\n",
    "            candidate_runs.sort(key = lambda x: x[1]*x[2], reverse=True)\n",
    "            # Use step as sorting value later.\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]*candidate_runs[0][2]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        good_examples = []\n",
    "        bad_examples = []\n",
    "        for exp in teach_pop:\n",
    "            good_examples += exp[1]\n",
    "        for exp in new_pop[int(len(new_pop) * self.teach_percent):]:\n",
    "            bad_examples += exp[1]\n",
    "        examples = good_examples\n",
    "        manager.print('BEGIN TRAINING POPULATION | TRAINING EXAMPLES {}'.format(len(examples)))\n",
    "        count = 0\n",
    "        losses = []\n",
    "        if len(examples) > 0:\n",
    "            for train in train_pop:\n",
    "                agent = train[0]\n",
    "                if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                    agent = self.replace_agent(agent)\n",
    "                trajectories = []\n",
    "                for _ in range(int(len(examples) / 2)):\n",
    "                    index = rand.randint(0, len(examples) - 1)\n",
    "                    trajectories.append(examples[index])\n",
    "                inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "                loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=1024, epochs=10)\n",
    "                losses.append(loss)\n",
    "                count += 1\n",
    "            print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses) if len(losses) > 0 else 'NA'))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 1, 0.001, 0.95, 5, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(60, 1, agent_hyper, 1/4, 1/4, 10, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_48 | STEP 1148 | SCORE 0.0 | AGE 1 | MEMBER 60/60 | GAME 1/1\t\t1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 780.0 | LOW SCORE 0.0 | HIGH SCORE 3700.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 26696\n",
      "AGENT_48 | EPOCH 10/10 | BATCH 14/14 | CURRENT BATCH COUNT 36 | LOSS 1.5779 | AGE 1 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4937839195841833\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_30 | STEP 2054 | SCORE 3300.0 | AGE 1 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1316.6666666666667 | LOW SCORE 0.0 | HIGH SCORE 5600.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 30835\n",
      "AGENT_36 | EPOCH 10/10 | BATCH 16/16 | CURRENT BATCH COUNT 57 | LOSS 1.5866 | AGE 1 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4757989456752936\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_51 | STEP 1725 | SCORE 2000.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1910.0 | LOW SCORE 0.0 | HIGH SCORE 4500.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 32513\n",
      "AGENT_53 | EPOCH 10/10 | BATCH 16/16 | CURRENT BATCH COUNT 896 | LOSS 1.4425 | AGE 1 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.4763118207951391\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_48 | STEP 1385 | SCORE 1400.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2246.6666666666665 | LOW SCORE 400.0 | HIGH SCORE 5200.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 32000\n",
      "AGENT_37 | EPOCH 10/10 | BATCH 16/16 | CURRENT BATCH COUNT 640 | LOSS 1.4899 | AGE 1 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5308859115839006\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_14 | STEP 1655 | SCORE 800.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2343.3333333333335 | LOW SCORE 600.0 | HIGH SCORE 5800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 34110\n",
      "AGENT_2 | EPOCH 10/10 | BATCH 17/17 | CURRENT BATCH COUNT 671 | LOSS 1.6428 | AGE 3 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5334839157964666\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_2 | STEP 2085 | SCORE 1900.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2833.3333333333335 | LOW SCORE 900.0 | HIGH SCORE 6000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 35821\n",
      "AGENT_33 | EPOCH 10/10 | BATCH 18/18 | CURRENT BATCH COUNT 502 | LOSS 1.4870 | AGE 2 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5523760299770921\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_52 | STEP 2031 | SCORE 3000.0 | AGE 2 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2850.0 | LOW SCORE 800.0 | HIGH SCORE 6000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 36344\n",
      "AGENT_30 | EPOCH 10/10 | BATCH 18/18 | CURRENT BATCH COUNT 764 | LOSS 1.5029 | AGE 2 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.549861809765851\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_50 | STEP 1894 | SCORE 2400.0 | AGE 3 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3298.3333333333335 | LOW SCORE 700.0 | HIGH SCORE 6200.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 42372\n",
      "AGENT_14 | EPOCH 10/10 | BATCH 21/21 | CURRENT BATCH COUNT 706 | LOSS 1.5327 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5429939904288643\n",
      "BEGIN RUNNING POPULATION | GENERATION 8\n",
      "AGENT_48 | STEP 2660 | SCORE 3700.0 | AGE 3 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3836.6666666666665 | LOW SCORE 900.0 | HIGH SCORE 6400.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 39691\n",
      "AGENT_22 | EPOCH 10/10 | BATCH 20/20 | CURRENT BATCH COUNT 389 | LOSS 1.4952 | AGE 2 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5276931695540745\n",
      "BEGIN RUNNING POPULATION | GENERATION 9\n",
      "AGENT_59 | STEP 2680 | SCORE 5100.0 | AGE 3 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3618.3333333333335 | LOW SCORE 1700.0 | HIGH SCORE 5700.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 40348\n",
      "AGENT_55 | EPOCH 10/10 | BATCH 20/20 | CURRENT BATCH COUNT 718 | LOSS 1.5508 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5262882358233134\n",
      "BEGIN RUNNING POPULATION | GENERATION 10\n",
      "AGENT_18 | STEP 2341 | SCORE 3700.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4543.333333333333 | LOW SCORE 1900.0 | HIGH SCORE 8900.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 46836\n",
      "AGENT_39 | EPOCH 10/10 | BATCH 23/23 | CURRENT BATCH COUNT 890 | LOSS 1.4687 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5277739423599797\n",
      "BEGIN RUNNING POPULATION | GENERATION 11\n",
      "AGENT_5 | STEP 1697 | SCORE 3400.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4521.666666666667 | LOW SCORE 1300.0 | HIGH SCORE 12900.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 50267\n",
      "AGENT_32 | EPOCH 10/10 | BATCH 25/25 | CURRENT BATCH COUNT 557 | LOSS 1.5295 | AGE 3 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.516972100321452\n",
      "BEGIN RUNNING POPULATION | GENERATION 12\n",
      "AGENT_1 | STEP 2276 | SCORE 3700.0 | AGE 5 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4823.333333333333 | LOW SCORE 1600.0 | HIGH SCORE 9800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 48847\n",
      "AGENT_48 | EPOCH 10/10 | BATCH 24/24 | CURRENT BATCH COUNT 871 | LOSS 1.5969 | AGE 4 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5477759630150265\n",
      "BEGIN RUNNING POPULATION | GENERATION 13\n",
      "AGENT_39 | STEP 2998 | SCORE 5300.0 | AGE 5 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5123.333333333333 | LOW SCORE 900.0 | HIGH SCORE 9700.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 50397\n",
      "AGENT_52 | EPOCH 10/10 | BATCH 25/25 | CURRENT BATCH COUNT 622 | LOSS 1.5298 | AGE 4 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.558269979317983\n",
      "BEGIN RUNNING POPULATION | GENERATION 14\n",
      "AGENT_38 | STEP 3337 | SCORE 8400.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5470.0 | LOW SCORE 2400.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52949\n",
      "AGENT_50 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 874 | LOSS 1.5781 | AGE 4 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.5964473636639422\n",
      "BEGIN RUNNING POPULATION | GENERATION 15\n",
      "AGENT_4 | STEP 3875 | SCORE 6500.0 | AGE 5 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4993.333333333333 | LOW SCORE 700.0 | HIGH SCORE 8900.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 49906\n",
      "AGENT_16 | EPOCH 10/10 | BATCH 25/25 | CURRENT BATCH COUNT 377 | LOSS 1.6054 | AGE 4 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6153687092781068\n",
      "BEGIN RUNNING POPULATION | GENERATION 16\n",
      "AGENT_31 | STEP 1692 | SCORE 1300.0 | AGE 5 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5491.666666666667 | LOW SCORE 1300.0 | HIGH SCORE 9800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 53690\n",
      "AGENT_31 | EPOCH 10/10 | BATCH 27/27 | CURRENT BATCH COUNT 221 | LOSS 1.5061 | AGE 5 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6287151740509787\n",
      "BEGIN RUNNING POPULATION | GENERATION 17\n",
      "AGENT_49 | STEP 2539 | SCORE 5500.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5288.333333333333 | LOW SCORE 1100.0 | HIGH SCORE 10200.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 51464\n",
      "AGENT_25 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 132 | LOSS 1.5483 | AGE 4 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6364645066933756\n",
      "BEGIN RUNNING POPULATION | GENERATION 18\n",
      "AGENT_12 | STEP 3531 | SCORE 8500.0 | AGE 4 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5335.0 | LOW SCORE 1600.0 | HIGH SCORE 9000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 53422\n",
      "AGENT_28 | EPOCH 10/10 | BATCH 27/27 | CURRENT BATCH COUNT 87 | LOSS 1.5508 | AGE 6 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6727248653953457\n",
      "BEGIN RUNNING POPULATION | GENERATION 19\n",
      "AGENT_24 | STEP 1705 | SCORE 1900.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5335.0 | LOW SCORE 1900.0 | HIGH SCORE 10600.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT_24 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 543 | LOSS 1.6796 | AGE 6 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.663368322115678\n",
      "BEGIN RUNNING POPULATION | GENERATION 20\n",
      "AGENT_42 | STEP 3429 | SCORE 7700.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5380.0 | LOW SCORE 1800.0 | HIGH SCORE 11300.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52333\n",
      "AGENT_45 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 566 | LOSS 1.7397 | AGE 6 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6899193093409905\n",
      "BEGIN RUNNING POPULATION | GENERATION 21\n",
      "AGENT_25 | STEP 1756 | SCORE 1700.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5193.333333333333 | LOW SCORE 1700.0 | HIGH SCORE 9500.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 53140\n",
      "AGENT_7 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 970 | LOSS 1.6544 | AGE 5 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.6912968534995347\n",
      "BEGIN RUNNING POPULATION | GENERATION 22\n",
      "AGENT_1 | STEP 2929 | SCORE 5000.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5111.666666666667 | LOW SCORE 1900.0 | HIGH SCORE 10000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 50231\n",
      "AGENT_20 | EPOCH 10/10 | BATCH 25/25 | CURRENT BATCH COUNT 539 | LOSS 1.7663 | AGE 7 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.7432451140403746\n",
      "BEGIN RUNNING POPULATION | GENERATION 23\n",
      "AGENT_14 | STEP 3582 | SCORE 6400.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5585.0 | LOW SCORE 2400.0 | HIGH SCORE 9300.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 51314\n",
      "AGENT_38 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 57 | LOSS 1.5983 | AGE 4 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.7351767432384004\n",
      "BEGIN RUNNING POPULATION | GENERATION 24\n",
      "AGENT_57 | STEP 1594 | SCORE 1500.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5196.666666666667 | LOW SCORE 1500.0 | HIGH SCORE 15000.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 53395\n",
      "AGENT_57 | EPOCH 10/10 | BATCH 27/27 | CURRENT BATCH COUNT 73 | LOSS 1.6074 | AGE 6 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.710571723219789\n",
      "BEGIN RUNNING POPULATION | GENERATION 25\n",
      "AGENT_33 | STEP 2086 | SCORE 3200.0 | AGE 8 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4846.666666666667 | LOW SCORE 1500.0 | HIGH SCORE 9700.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 48134\n",
      "AGENT_31 | EPOCH 10/10 | BATCH 24/24 | CURRENT BATCH COUNT 515 | LOSS 1.8755 | AGE 9 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.773225600057178\n",
      "BEGIN RUNNING POPULATION | GENERATION 26\n",
      "AGENT_56 | STEP 1798 | SCORE 2900.0 | AGE 8 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5228.333333333333 | LOW SCORE 1900.0 | HIGH SCORE 8600.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52530\n",
      "AGENT_50 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 665 | LOSS 1.8896 | AGE 8 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.755826867115803\n",
      "BEGIN RUNNING POPULATION | GENERATION 27\n",
      "AGENT_59 | STEP 2483 | SCORE 4400.0 | AGE 7 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5025.0 | LOW SCORE 2600.0 | HIGH SCORE 9800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 48831\n",
      "AGENT_32 | EPOCH 10/10 | BATCH 24/24 | CURRENT BATCH COUNT 863 | LOSS 1.7792 | AGE 8 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.817427745858828\n",
      "BEGIN RUNNING POPULATION | GENERATION 28\n",
      "AGENT_14 | STEP 2030 | SCORE 3500.0 | AGE 6 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5600.0 | LOW SCORE 2200.0 | HIGH SCORE 10100.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 51925\n",
      "AGENT_35 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 362 | LOSS 1.8754 | AGE 8 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.837667529674677\n",
      "BEGIN RUNNING POPULATION | GENERATION 29\n",
      "AGENT_8 | STEP 2360 | SCORE 4200.0 | AGE 7 | MEMBER 60/60 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5040.0 | LOW SCORE 2300.0 | HIGH SCORE 9300.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 48653\n",
      "AGENT_50 | EPOCH 10/10 | BATCH 24/24 | CURRENT BATCH COUNT 774 | LOSS 1.8607 | AGE 9 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.833737195034822\n",
      "BEGIN RUNNING POPULATION | GENERATION 30\n",
      "AGENT_32 | STEP 2659 | SCORE 4600.0 | AGE 9 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5173.333333333333 | LOW SCORE 1200.0 | HIGH SCORE 9500.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52357\n",
      "AGENT_18 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 578 | LOSS 1.6980 | AGE 6 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.8233898647626243\n",
      "BEGIN RUNNING POPULATION | GENERATION 31\n",
      "AGENT_34 | STEP 2210 | SCORE 3900.0 | AGE 11 | MEMBER 60/60 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5221.666666666667 | LOW SCORE 2200.0 | HIGH SCORE 9100.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52895\n",
      "AGENT_57 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 847 | LOSS 1.6556 | AGE 7 | MEMBER 15/15\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.7579684587625357\n",
      "BEGIN RUNNING POPULATION | GENERATION 32\n",
      "AGENT_46 | STEP 2891 | SCORE 8200.0 | AGE 7 | MEMBER 60/60 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5483.333333333333 | LOW SCORE 1000.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 52119\n",
      "AGENT_23 | EPOCH 10/10 | BATCH 26/26 | CURRENT BATCH COUNT 459 | LOSS 1.8109 | AGE 7 | MEMBER 15/15\t\t\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.8140965556487059\n",
      "BEGIN RUNNING POPULATION | GENERATION 33\n",
      "AGENT_48 | STEP 2409 | SCORE 4300.0 | AGE 11 | MEMBER 11/60 | GAME 1/1\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible next test, try on trajectories before labeling rather than after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
