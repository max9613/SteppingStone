{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Added relu end, Hypers:(40, 1, agent_hyper, 1/4, 1/4, 1000, 0.3)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsRating/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if train:\n",
    "                outputs.append(out(x))\n",
    "            else:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy rating network.\n",
    "class RATING_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.relu(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.relu(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.3\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(state_size * stack_size, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()#nn.MSELoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=50):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                #outputs = [torch.stack(o) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                # Peer inputs and outputs for peer loss function implementation.\n",
    "                peer_inputs = [b for b in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_outputs = [[a for a in o] for o in batches[i][1]]\n",
    "                for o in peer_outputs:\n",
    "                    rand.shuffle(o)\n",
    "                peer_outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs.to(self.t_device), train=True)\n",
    "                peer_outs = self.policy_map(peer_inputs.to(self.t_device), train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j]) - (self.alpha * self.loss_func(peer_outs[j], peer_outputs[j]))\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor)\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = observation / 255\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    groups.append((previous_state, action_queue))\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        tensor = torch.cat(tensors, 0)\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor)\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff, alpha):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        self.rating_map = RATING_NET((self.population[0].state_size * self.population[0].stack_size) + (self.population[0].action_size * self.population[0].step_size), 2, self.population[0].layer_count, self.population[0].t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.rating_map.params, lr=self.population[0].learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "    \n",
    "    # Trains rating map.\n",
    "    def train_rating_map(self, good_examples, bad_examples, batch_size=1024, epochs=50):\n",
    "        groups = []\n",
    "        eye = torch.eye(self.population[0].action_size).to(self.rating_map.t_device)\n",
    "        for example in good_examples:\n",
    "            tensors = [torch.Tensor(example[0])]\n",
    "            for a in example[1]:\n",
    "                tensors.append(eye[a])\n",
    "            tensor = torch.cat(tensors, 0)\n",
    "            groups.append((tensor, 1))\n",
    "        for example in bad_examples:\n",
    "            tensors = [torch.Tensor(example[0])]\n",
    "            for a in example[1]:\n",
    "                tensors.append(eye[a])\n",
    "            tensor = torch.cat(tensors, 0)\n",
    "            groups.append((tensor, 0))\n",
    "        rand.shuffle(groups)\n",
    "        batches = []\n",
    "        position = 0\n",
    "        batch = ([],[])\n",
    "        losses = []\n",
    "        while position < len(groups):\n",
    "            batch[0].append(groups[position][0])\n",
    "            batch[1].append(groups[position][1])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                peer_inputs = [t for t in batches[i][0]]\n",
    "                rand.shuffle(peer_inputs)\n",
    "                peer_inputs = torch.stack(peer_inputs)\n",
    "                peer_outputs = [t for t in batches[i][1]]\n",
    "                rand.shuffle(peer_outputs)\n",
    "                peer_outputs = torch.Tensor(peer_outputs).long()\n",
    "                outputs = torch.Tensor(batches[i][1]).long()\n",
    "                out = self.rating_map(inputs, train=True)\n",
    "                peer_out = self.rating_map(peer_inputs, train=True)\n",
    "                loss = self.loss_func(out, outputs) - (self.alpha * self.loss_func(peer_out, peer_outputs))\n",
    "                print('\\rRATING MAP | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f}\\t\\t'.format(e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy()), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        \n",
    "        # Train rating map.\n",
    "        good_examples = []\n",
    "        bad_examples = []\n",
    "        for exp in teach_pop:\n",
    "            good_examples += exp[1]\n",
    "        for exp in new_pop[int(len(new_pop) * self.teach_percent):]:\n",
    "            bad_examples += exp[1]\n",
    "        avg_loss = self.train_rating_map(good_examples, bad_examples)\n",
    "        print('')\n",
    "        manager.print('END TRAINING RATING MAP | AVG LOSS {}'.format(avg_loss))\n",
    "        \n",
    "        examples = good_examples\n",
    "        eye = torch.eye(self.population[0].action_size).to(self.rating_map.t_device)\n",
    "        for exps in new_pop:\n",
    "            for exp in exps[1]:\n",
    "                tensors = [torch.Tensor(exp[0]).to(self.rating_map.t_device)]\n",
    "                for a in exp[1]:\n",
    "                    tensors.append(eye[a])\n",
    "                tensor = torch.cat(tensors, 0)\n",
    "                out = self.rating_map(tensor).detach().cpu().numpy()\n",
    "                if np.argmax(out) == 1:\n",
    "                    examples.append(exp)\n",
    "        manager.print('BEGIN TRAINING POPULATION | TRAINING EXAMPLES {}'.format(len(examples)))\n",
    "        count = 0\n",
    "        losses = []\n",
    "        if len(examples) > 0:\n",
    "            for train in train_pop:\n",
    "                agent = train[0]\n",
    "                if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                    agent = self.replace_agent(agent)\n",
    "                trajectories = []\n",
    "                for _ in range(int(len(examples) / 2)):\n",
    "                    index = rand.randint(0, len(examples) - 1)\n",
    "                    trajectories.append(examples[index])\n",
    "                inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "                loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=1024, epochs=50)\n",
    "                losses.append(loss)\n",
    "                count += 1\n",
    "            print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses) if len(losses) > 0 else 'NA'))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 1, 0.001, 0.95, 5, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(40, 1, agent_hyper, 1/4, 1/4, 1000, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_38 | STEP 701 | SCORE 0.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 975.0 | LOW SCORE 0.0 | HIGH SCORE 6200.0\n",
      "RATING MAP | EPOCH 50/50 | BATCH 54/54 | CURRENT BATCH COUNT 674 | LOSS 0.4602\t\t\t\t\n",
      "END TRAINING RATING MAP | AVG LOSS 0.4782131953261517\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 19635\n",
      "AGENT_38 | EPOCH 50/50 | BATCH 10/10 | CURRENT BATCH COUNT 601 | LOSS 1.7068 | AGE 1 | MEMBER 10/10\t\t\n",
      "END TRAINING POPULATION | AVG LOSS 1.731669906258583\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_25 | STEP 3003 | SCORE 7800.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2180.0 | LOW SCORE 0.0 | HIGH SCORE 7800.0\n",
      "RATING MAP | EPOCH 50/50 | BATCH 68/68 | CURRENT BATCH COUNT 297 | LOSS 0.4738\t\t\t\n",
      "END TRAINING RATING MAP | AVG LOSS 0.47210765401230137\n",
      "BEGIN TRAINING POPULATION | TRAINING EXAMPLES 26347\n",
      "AGENT_22 | EPOCH 3/50 | BATCH 9/13 | CURRENT BATCH COUNT 1024 | LOSS 1.5625 | AGE 1 | MEMBER 3/10\t\t\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible next test, try on trajectories before labeling rather than after."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
