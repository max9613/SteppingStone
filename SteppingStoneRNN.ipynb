{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Set end activation to relu, Hypers: (40, 1, agent_hypers, 1/4, 1/4, 1000)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsRNN/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN policy map.\n",
    "class POLICY_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_hidden_count = pre_hidden_count\n",
    "        self.post_hidden_count = post_hidden_count\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        weights = generate_weights(input_size, hidden_size, pre_hidden_count)\n",
    "        self.pre_hidden_layers = []\n",
    "        prev_weight = input_size\n",
    "        for w in weights:\n",
    "            self.pre_hidden_layers.append(nn.Linear(prev_weight, w).to(t_device))\n",
    "            prev_weight = w\n",
    "        self.pre_hidden_layers.append(nn.Linear(prev_weight, hidden_size).to(t_device))\n",
    "        self.post_hidden_layers = []\n",
    "        for _ in range(post_hidden_count):\n",
    "            self.post_hidden_layers.append(nn.Linear(hidden_if cell_state is None:\n",
    "            hidden = torch.zeros(self.hidden_size).to(self.t_device)catsize, hidden_size).to(t_device))\n",
    "        self.policy_out = nn.Linear(hidden_size, class_count).to(t_device)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.parameters = []\n",
    "        for layer in self.pre_hidden_layers + self.post_hidden_layers + [self.policy_out]:\n",
    "            self.parameters += list(layer.parameters())\n",
    "        \n",
    "    # Forward propogation.\n",
    "    def forward(self, states, hidden=None, train=False):\n",
    "        outs = []\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.hidden_size).to(self.t_device)\n",
    "        for x in states:\n",
    "            for layer in self.pre_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            x += hidden\n",
    "            for layer in self.post_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            hidden = x\n",
    "            if train:\n",
    "                outs.append(self.policy_out(x))\n",
    "            else:\n",
    "                outs.append(self.relu(self.policy_out(x)))\n",
    "        if train:\n",
    "            return outs\n",
    "        else:\n",
    "            return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Agent that plays the game.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, learning_rate, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.learning_rate = learning_rate\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.policy_map = POLICY_MAP(input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.parameters, lr=learning_rate)\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.age = 0\n",
    "        \n",
    "    # Train the agent.\n",
    "    # Input data should be a list of trajectories, which should be of form [[states], [actions]].\n",
    "    def train(self, trajectories, unroll_depth, batch_size=32, epochs=50, extra_info=''):\n",
    "        batches = []\n",
    "        batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "        rand.shuffle(trajectories)\n",
    "        count = 0\n",
    "        for t in trajectories:\n",
    "            for i in range(unroll_depth):\n",
    "                batch[0][i].append(t[0][i])\n",
    "                batch[1][i].append(t[1][i])\n",
    "            count += 1\n",
    "            if count >= batch_size:\n",
    "                batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "                batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "                batches.append(batch)\n",
    "                batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "                count = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                inputs = batch[0]\n",
    "                targets = batch[1]\n",
    "                outputs = self.policy_map(inputs, train=True)\n",
    "                loss = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    loss += self.cross_loss(outputs[i], targets[i])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                print('\\rTRAINING {} | AGE {} | BATCH {}/{} | EPOCH {}/{} | LOSS {} | {}'.format(self.name, self.age, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "    \n",
    "    # Plays the game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        hidden_state = None\n",
    "        groups = []\n",
    "        group = []\n",
    "        env.reset()\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, step, score, self.age, extra_info), end = '')\n",
    "            score += reward\n",
    "            step += 1\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policies, hidden_state = self.policy_map([tensor], hidden_state)\n",
    "            policy = policies[0].to(self.s_device)\n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.class_count - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            group.append((tensor, action))\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                lives = info['ale.lives']\n",
    "                hidden_state = None\n",
    "                group = []\n",
    "            if render:\n",
    "                env.render()\n",
    "        return groups, score, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "    \n",
    "    # Sorts the trajectories into ones for the greedy head and non-greedy head.\n",
    "    def prepare_data(self, groups, unroll_depth):\n",
    "        trajectories = []\n",
    "        for total_run in groups:\n",
    "            index = unroll_depth\n",
    "            while index < len(total_run):\n",
    "                actions = []\n",
    "                states = []\n",
    "                for group in total_run[index-unroll_depth:index]:\n",
    "                    actions.append(group[1])\n",
    "                    states.append(group[0])\n",
    "                trajectories.append([states, actions])\n",
    "                index += 1\n",
    "        return trajectories\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, unroll_depth, epochs, batch_size, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score, step = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score, step))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "        print('')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        manager.save()\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        trajectories = self.prepare_data(examples, unroll_depth)\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            #if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "            #    agent = self.replace_agent(agent)\n",
    "            loss = agent.train(trajectories, unroll_depth, batch_size, epochs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}')\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        manager.save()\n",
    "        self.generation += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hypers = (10**-4, 128, 80, 4, 2, 14, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(40, 1, agent_hypers, 1/4, 1/4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_15 | STEP 894 | SCORE 0.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 805.0 | LOW SCORE 0.0 | HIGH SCORE 6200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_15 | AGE 0 | BATCH 63/63 | EPOCH 10/10 | LOSS 50.41295623779297 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 51.10423895578536\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_25 | STEP 1463 | SCORE 0.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1132.5 | LOW SCORE 0.0 | HIGH SCORE 6600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_25 | AGE 0 | BATCH 70/70 | EPOCH 10/10 | LOSS 55.759639739990234 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 49.95564568601336\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_6 | STEP 925 | SCORE 200.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1850.0 | LOW SCORE 0.0 | HIGH SCORE 9700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_0 | AGE 0 | BATCH 81/81 | EPOCH 10/10 | LOSS 59.43290710449219 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 51.731036084493\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_21 | STEP 2158 | SCORE 3300.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2182.5 | LOW SCORE 0.0 | HIGH SCORE 8500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_19 | AGE 1 | BATCH 82/82 | EPOCH 10/10 | LOSS 48.81602478027344 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 48.88493793627111\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_14 | STEP 1910 | SCORE 4800.0 | AGE 0 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 2977.5 | LOW SCORE 200.0 | HIGH SCORE 7800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_37 | AGE 0 | BATCH 82/82 | EPOCH 10/10 | LOSS 50.838375091552734 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 48.68076529386567\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_30 | STEP 1441 | SCORE 500.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3737.5 | LOW SCORE 400.0 | HIGH SCORE 11000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_27 | AGE 1 | BATCH 101/101 | EPOCH 10/10 | LOSS 44.90229797363281 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 52.11601566390236\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_33 | STEP 1476 | SCORE 2700.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3735.0 | LOW SCORE 400.0 | HIGH SCORE 12400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_4 | AGE 1 | BATCH 91/91 | EPOCH 10/10 | LOSS 41.319705963134766 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 49.21423804440341\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_24 | STEP 1351 | SCORE 2800.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 3650.0 | LOW SCORE 500.0 | HIGH SCORE 10200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_35 | AGE 1 | BATCH 88/88 | EPOCH 10/10 | LOSS 45.82925033569336 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 47.877561319524595\n",
      "BEGIN RUNNING POPULATION | GENERATION 8\n",
      "AGENT_13 | STEP 1903 | SCORE 7400.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4075.0 | LOW SCORE 700.0 | HIGH SCORE 9300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_3 | AGE 3 | BATCH 94/94 | EPOCH 10/10 | LOSS 51.55278778076172 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 47.183784184963145\n",
      "BEGIN RUNNING POPULATION | GENERATION 9\n",
      "AGENT_9 | STEP 1958 | SCORE 5200.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4722.5 | LOW SCORE 300.0 | HIGH SCORE 9900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_3 | AGE 4 | BATCH 98/98 | EPOCH 10/10 | LOSS 45.55733871459961 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 45.061049260898514\n",
      "BEGIN RUNNING POPULATION | GENERATION 10\n",
      "AGENT_27 | STEP 2580 | SCORE 8400.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5040.0 | LOW SCORE 800.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_23 | AGE 1 | BATCH 91/91 | EPOCH 10/10 | LOSS 51.23720932006836 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 46.8554115660112\n",
      "BEGIN RUNNING POPULATION | GENERATION 11\n",
      "AGENT_28 | STEP 1916 | SCORE 3400.0 | AGE 4 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5117.5 | LOW SCORE 1100.0 | HIGH SCORE 11200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_20 | AGE 2 | BATCH 99/99 | EPOCH 10/10 | LOSS 45.04788589477539 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 45.65332059185915\n",
      "BEGIN RUNNING POPULATION | GENERATION 12\n",
      "AGENT_24 | STEP 1956 | SCORE 4100.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5027.5 | LOW SCORE 1900.0 | HIGH SCORE 10200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_20 | AGE 3 | BATCH 107/107 | EPOCH 10/10 | LOSS 44.76964569091797 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 44.74054962051248\n",
      "BEGIN RUNNING POPULATION | GENERATION 13\n",
      "AGENT_3 | STEP 2526 | SCORE 6500.0 | AGE 6 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5052.5 | LOW SCORE 1300.0 | HIGH SCORE 11500.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_26 | AGE 3 | BATCH 97/97 | EPOCH 10/10 | LOSS 44.61616516113281 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 44.091450144974225\n",
      "BEGIN RUNNING POPULATION | GENERATION 14\n",
      "AGENT_37 | STEP 1753 | SCORE 2900.0 | AGE 5 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5002.5 | LOW SCORE 0.0 | HIGH SCORE 9800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_24 | AGE 2 | BATCH 91/91 | EPOCH 10/10 | LOSS 45.8278694152832 | | MEMBER 10/1010\n",
      "END TRAINING POPULATION | AVG LOSS 46.03837695530483\n",
      "BEGIN RUNNING POPULATION | GENERATION 15\n",
      "AGENT_16 | STEP 1547 | SCORE 700.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5020.0 | LOW SCORE 700.0 | HIGH SCORE 7900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_16 | AGE 2 | BATCH 88/88 | EPOCH 10/10 | LOSS 48.16283416748047 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 45.40246887727217\n",
      "BEGIN RUNNING POPULATION | GENERATION 16\n",
      "AGENT_39 | STEP 2243 | SCORE 4900.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4507.5 | LOW SCORE 1400.0 | HIGH SCORE 8400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_27 | AGE 6 | BATCH 89/89 | EPOCH 10/10 | LOSS 43.39940643310547 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 44.67076218594326\n",
      "BEGIN RUNNING POPULATION | GENERATION 17\n",
      "AGENT_29 | STEP 2373 | SCORE 5800.0 | AGE 4 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 4937.5 | LOW SCORE 1200.0 | HIGH SCORE 9200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_16 | AGE 3 | BATCH 91/91 | EPOCH 10/10 | LOSS 45.81319046020508 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 43.00306337754805\n",
      "BEGIN RUNNING POPULATION | GENERATION 18\n",
      "AGENT_6 | STEP 1824 | SCORE 4000.0 | AGE 5 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5147.5 | LOW SCORE 1500.0 | HIGH SCORE 10600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_27 | AGE 7 | BATCH 94/94 | EPOCH 10/10 | LOSS 43.230289459228516 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 43.6723861824198\n",
      "BEGIN RUNNING POPULATION | GENERATION 19\n",
      "AGENT_11 | STEP 1948 | SCORE 3700.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5377.5 | LOW SCORE 800.0 | HIGH SCORE 11100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_18 | AGE 4 | BATCH 92/92 | EPOCH 10/10 | LOSS 43.82181167602539 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.76828569536623\n",
      "BEGIN RUNNING POPULATION | GENERATION 20\n",
      "AGENT_7 | STEP 2216 | SCORE 2300.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5532.5 | LOW SCORE 2300.0 | HIGH SCORE 10600.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_7 | AGE 3 | BATCH 106/106 | EPOCH 10/10 | LOSS 41.327938079833984 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 43.73651960624839\n",
      "BEGIN RUNNING POPULATION | GENERATION 21\n",
      "AGENT_2 | STEP 2788 | SCORE 8800.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5242.5 | LOW SCORE 2500.0 | HIGH SCORE 9400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_16 | AGE 5 | BATCH 96/96 | EPOCH 10/10 | LOSS 44.378273010253906 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 43.70979374726613\n",
      "BEGIN RUNNING POPULATION | GENERATION 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT_18 | STEP 2071 | SCORE 5600.0 | AGE 5 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5855.0 | LOW SCORE 2600.0 | HIGH SCORE 9900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_6 | AGE 5 | BATCH 99/99 | EPOCH 10/10 | LOSS 40.568687438964844 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 44.27325942569309\n",
      "BEGIN RUNNING POPULATION | GENERATION 23\n",
      "AGENT_2 | STEP 2218 | SCORE 5500.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5240.0 | LOW SCORE 1000.0 | HIGH SCORE 9200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_39 | AGE 3 | BATCH 99/99 | EPOCH 10/10 | LOSS 43.50441360473633 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 43.4912950296113\n",
      "BEGIN RUNNING POPULATION | GENERATION 24\n",
      "AGENT_24 | STEP 2258 | SCORE 6300.0 | AGE 5 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5632.5 | LOW SCORE 2100.0 | HIGH SCORE 12100.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_13 | AGE 4 | BATCH 95/95 | EPOCH 10/10 | LOSS 44.61964797973633 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.44353300717003\n",
      "BEGIN RUNNING POPULATION | GENERATION 25\n",
      "AGENT_25 | STEP 2139 | SCORE 5000.0 | AGE 10 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5470.0 | LOW SCORE 1600.0 | HIGH SCORE 9800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_7 | AGE 5 | BATCH 95/95 | EPOCH 10/10 | LOSS 45.86328887939453 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 43.11047014979313\n",
      "BEGIN RUNNING POPULATION | GENERATION 26\n",
      "AGENT_36 | STEP 2235 | SCORE 4300.0 | AGE 6 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5210.0 | LOW SCORE 1900.0 | HIGH SCORE 10700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_34 | AGE 7 | BATCH 95/95 | EPOCH 10/10 | LOSS 42.146854400634766 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 43.13927285565828\n",
      "BEGIN RUNNING POPULATION | GENERATION 27\n",
      "AGENT_34 | STEP 2395 | SCORE 5200.0 | AGE 8 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5805.0 | LOW SCORE 0.0 | HIGH SCORE 11700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_15 | AGE 2 | BATCH 97/97 | EPOCH 10/10 | LOSS 40.465797424316406 | | MEMBER 10/10\n",
      "END TRAINING POPULATION | AVG LOSS 42.443777932235875\n",
      "BEGIN RUNNING POPULATION | GENERATION 28\n",
      "AGENT_19 | STEP 2450 | SCORE 5500.0 | AGE 9 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5600.0 | LOW SCORE 1800.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_29 | AGE 6 | BATCH 100/100 | EPOCH 10/10 | LOSS 42.9344482421875 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.49218467979431\n",
      "BEGIN RUNNING POPULATION | GENERATION 29\n",
      "AGENT_9 | STEP 2239 | SCORE 6900.0 | AGE 9 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5832.5 | LOW SCORE 2400.0 | HIGH SCORE 10800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_18 | AGE 8 | BATCH 102/102 | EPOCH 10/10 | LOSS 42.49312210083008 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.30315090815226\n",
      "BEGIN RUNNING POPULATION | GENERATION 30\n",
      "AGENT_4 | STEP 4091 | SCORE 10400.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 6312.5 | LOW SCORE 2200.0 | HIGH SCORE 10400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_29 | AGE 8 | BATCH 104/104 | EPOCH 10/10 | LOSS 41.52790069580078 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 43.20500721821418\n",
      "BEGIN RUNNING POPULATION | GENERATION 31\n",
      "AGENT_8 | STEP 2850 | SCORE 8800.0 | AGE 11 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5267.5 | LOW SCORE 700.0 | HIGH SCORE 8800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_12 | AGE 1 | BATCH 100/100 | EPOCH 10/10 | LOSS 48.56241226196289 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.69288464279175\n",
      "BEGIN RUNNING POPULATION | GENERATION 32\n",
      "AGENT_37 | STEP 1459 | SCORE 2500.0 | AGE 8 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5537.5 | LOW SCORE 1800.0 | HIGH SCORE 12300.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_27 | AGE 11 | BATCH 104/104 | EPOCH 10/10 | LOSS 41.37425231933594 | | MEMBER 10/100\n",
      "END TRAINING POPULATION | AVG LOSS 42.3587454194289\n",
      "BEGIN RUNNING POPULATION | GENERATION 33\n",
      "AGENT_22 | STEP 2270 | SCORE 6100.0 | AGE 10 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "END RUNNING POPULATION | AVERAGE SCORE 5607.5 | LOW SCORE 2800.0 | HIGH SCORE 10400.0\n",
      "BEGIN TRAINING POPULATION\n",
      "TRAINING AGENT_3 | AGE 10 | BATCH 5/103 | EPOCH 9/10 | LOSS 42.515377044677734 | | MEMBER 4/100000"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, 20, 10, 256, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
