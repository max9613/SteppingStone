{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "Switched end from softmax to relu, Hypers: (10**-4, 128, 64, 4, 2, 14, torch.device('cpu'), torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experimentsHydra/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action predictor based on two embedded states.\n",
    "class ACTION_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.sin = torch.sin\n",
    "        self.relu = F.relu\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        if train:\n",
    "            return self.output_layer(x)\n",
    "        else:\n",
    "            return self.relu(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Hydra map.\n",
    "class HYDRA_MAP(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, number_of_heads, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device):\n",
    "        super().__init__()\n",
    "        self.number_of_heads = number_of_heads\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pre_hidden_count = pre_hidden_count\n",
    "        self.post_hidden_count = post_hidden_count\n",
    "        self.class_count = class_count\n",
    "        self.t_device = t_device\n",
    "        weights = generate_weights(input_size, hidden_size, pre_hidden_count)\n",
    "        self.pre_hidden_layers = []\n",
    "        prev_weight = input_size\n",
    "        for w in weights:\n",
    "            self.pre_hidden_layers.append(nn.Linear(prev_weight, w).to(t_device))\n",
    "            prev_weight = w\n",
    "        self.pre_hidden_layers.append(nn.Linear(prev_weight, hidden_size).to(t_device))\n",
    "        self.heads = []\n",
    "        for _ in range(number_of_heads):\n",
    "            self.heads.append(ACTION_MAP(hidden_size, class_count, post_hidden_count, t_device))\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.params = []\n",
    "        for layer in self.pre_hidden_layers:\n",
    "            self.params += list(layer.parameters())\n",
    "        \n",
    "    # Forward propogation.\n",
    "    def forward(self, states, heads, hidden=None, train=False, train_rnn=True):\n",
    "        outs = [[] for _ in range(len(heads))]\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(self.hidden_size).to(self.t_device)\n",
    "        for x in states:\n",
    "            for layer in self.pre_hidden_layers:\n",
    "                x = self.sin(layer(x))\n",
    "            x += hidden\n",
    "            if not train_rnn:\n",
    "                x = x.detach()\n",
    "            for i in range(len(heads)):\n",
    "                outs[i].append(self.heads[heads[i]](x, train=train))\n",
    "            hidden = x\n",
    "        if train:\n",
    "            return outs\n",
    "        else:\n",
    "            return outs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Hydra that plays the game.\n",
    "class HYDRA:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, learning_rate, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device, s_device):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.policy_map = HYDRA_MAP(2, input_size, hidden_size, pre_hidden_count, post_hidden_count, class_count, t_device)\n",
    "        self.optimizers = [\n",
    "            torch.optim.Adam(self.policy_map.params + head.params, lr=learning_rate) for head in self.policy_map.heads\n",
    "        ]\n",
    "        self.cross_loss = nn.CrossEntropyLoss()\n",
    "        self.age = 0\n",
    "        \n",
    "    # Train the agent.\n",
    "    # Input data should be a list of trajectories, which should be of form [[states], [actions]].\n",
    "    def train_head(self, head, train_rnn, trajectories, unroll_depth, batch_size=32, epochs=50, extra_info=''):\n",
    "        optimizer = self.optimizers[head]\n",
    "        batches = []\n",
    "        batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "        rand.shuffle(trajectories)\n",
    "        count = 0\n",
    "        for t in trajectories:\n",
    "            for i in range(unroll_depth):\n",
    "                batch[0][i].append(t[0][i])\n",
    "                batch[1][i].append(t[1][i])\n",
    "            count += 1\n",
    "            if count >= batch_size:\n",
    "                batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "                batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "                batches.append(batch)\n",
    "                batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "                count = 0\n",
    "        if count > 0:\n",
    "            batch[0] = [torch.stack(state).to(self.t_device) for state in batch[0]]\n",
    "            batch[1] = [torch.Tensor(actions).long().to(self.t_device) for actions in batch[1]]\n",
    "            batches.append(batch)\n",
    "            batch = [[[] for _ in range(unroll_depth)], [[] for _ in range(unroll_depth)]]\n",
    "            count = 0\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            for b in range(len(batches)):\n",
    "                batch = batches[b]\n",
    "                inputs = batch[0]\n",
    "                targets = batch[1]\n",
    "                outputs = self.policy_map(inputs, heads=[head], train=True, train_rnn=train_rnn)[0]\n",
    "                loss = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    loss += self.cross_loss(outputs[i], targets[i])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "                print('\\rTRAINING HEAD {} | BATCH {}/{} | EPOCH {}/{} | LOSS {}'.format(head, b+1, len(batches), e+1, epochs, losses[-1], extra_info), end='')\n",
    "        self.age += 1\n",
    "        return sum(losses) / len(losses)\n",
    "    \n",
    "    # Plays the game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        action = 0\n",
    "        score = 0\n",
    "        step = 0\n",
    "        lives = 4\n",
    "        hidden_state = None\n",
    "        groups = []\n",
    "        group = []\n",
    "        env.reset()\n",
    "        tensor = None\n",
    "        while not done:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            print('\\rSTEP {} | SCORE {} | AGE {} {}\\t\\t'.format(step, score, self.age, extra_info), end = '')\n",
    "            score += reward\n",
    "            if tensor is not None:\n",
    "                group.append((tensor, action, reward))\n",
    "            step += 1\n",
    "            # Need to update to support multi-head\n",
    "            tensor = torch.Tensor.float(torch.from_numpy(observation / 255)).to(self.t_device)\n",
    "            policies, hidden_state = self.policy_map([tensor], hidden=hidden_state, heads=[0,1])\n",
    "            policy = policies[0][0].to(self.s_device) + policies[1][0].to(self.s_device)\n",
    "            if min(policy) < 0 or sum(policy) == 0:\n",
    "                action = rand.randint(0, self.class_count - 1)\n",
    "            else:\n",
    "                distribution = torch.distributions.categorical.Categorical(policy)\n",
    "                action = int(distribution.sample())\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                groups.append(group)\n",
    "                action = 0\n",
    "                tensor = None\n",
    "                lives = info['ale.lives']\n",
    "                hidden_state = None\n",
    "                group = []\n",
    "        return groups, score, step\n",
    "    \n",
    "    # Sorts the trajectories into ones for the greedy head and non-greedy head.\n",
    "    def prepare_data(self, groups, unroll_depth):\n",
    "        trajectories = []\n",
    "        for total_run in groups:\n",
    "            index = unroll_depth\n",
    "            total_reward = 0\n",
    "            while index < len(total_run):\n",
    "                actions = []\n",
    "                states = []\n",
    "                for group in total_run[index-unroll_depth:index]:\n",
    "                    total_reward += group[2]\n",
    "                    actions.append(group[1])\n",
    "                    states.append(group[0])\n",
    "                trajectories.append([states, actions])\n",
    "                index += 1\n",
    "        return trajectories\n",
    "    \n",
    "    # Collects data from n games and trains on it.\n",
    "    def collect_and_train(self, env, number_of_games, unroll_depth, current_head, render = False):\n",
    "        all_groups = []\n",
    "        total_score = 0\n",
    "        low_score = None\n",
    "        high_score = None\n",
    "        for g in range(number_of_games):\n",
    "            groups, score, step = self.play_game(env, render, f'| GAME {g+1}/{number_of_games}')\n",
    "            all_groups.append((groups, score, step))\n",
    "            total_score += score\n",
    "            if low_score is None or score < low_score:\n",
    "                low_score = score\n",
    "            if high_score is None or score > high_score:\n",
    "                high_score = score\n",
    "        all_groups.sort(key = lambda x: x[1], reverse=True)\n",
    "        final_groups = []\n",
    "        for g in all_groups[:int(0.3*len(all_groups))]:\n",
    "            final_groups += g[0]\n",
    "        print()\n",
    "        manager.print(f'FINISHED {number_of_games} GAMES | AVERAGE SCORE {total_score/number_of_games} | LOW SCORE {low_score} | HIGH SCORE {high_score}')\n",
    "        manager.save()\n",
    "        trajectory = self.prepare_data(final_groups, unroll_depth)\n",
    "        loss = self.train_head(current_head, True, trajectory, unroll_depth, batch_size=256, epochs=50, extra_info='')\n",
    "        print()\n",
    "        manager.print(f'FINISHED TRAINING HEAD {current_head} | LOSS {loss}')\n",
    "        manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra = HYDRA(10**-4, 128, 64, 4, 2, 14, torch.device('cpu'), torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1236 | SCORE 400.0 | AGE 0 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 335.0 | LOW SCORE 0.0 | HIGH SCORE 1200.0\n",
      "TRAINING HEAD 0 | BATCH 30/30 | EPOCH 50/50 | LOSS 49.744197845458984\n",
      "FINISHED TRAINING HEAD 0 | LOSS 50.28081092834473\n",
      "STEP 1329 | SCORE 200.0 | AGE 1 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 210.0 | LOW SCORE 0.0 | HIGH SCORE 900.0\n",
      "TRAINING HEAD 1 | BATCH 32/32 | EPOCH 50/50 | LOSS 45.352684020996094\n",
      "FINISHED TRAINING HEAD 1 | LOSS 47.01629961490631\n",
      "STEP 1393 | SCORE 0.0 | AGE 2 | GAME 20/20\t\t\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 30.0 | LOW SCORE 0.0 | HIGH SCORE 200.0\n",
      "TRAINING HEAD 0 | BATCH 26/26 | EPOCH 50/50 | LOSS 39.526718139648444\n",
      "FINISHED TRAINING HEAD 0 | LOSS 39.7252116394043\n",
      "STEP 1129 | SCORE 0.0 | AGE 3 | GAME 20/20\t\t\n",
      "FINISHED 20 GAMES | AVERAGE SCORE 0.0 | LOW SCORE 0.0 | HIGH SCORE 0.0\n",
      "TRAINING HEAD 1 | BATCH 22/22 | EPOCH 50/50 | LOSS 36.187240600585944\n",
      "FINISHED TRAINING HEAD 1 | LOSS 37.33968730579723\n",
      "STEP 588 | SCORE 0.0 | AGE 4 | GAME 17/20\t\t\t"
     ]
    }
   ],
   "source": [
    "heads = [0,1]\n",
    "index = 0\n",
    "while True:\n",
    "    hydra.collect_and_train(env, 20, 20, heads[index], True)\n",
    "    index += 1\n",
    "    if index >= len(heads):\n",
    "        index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
