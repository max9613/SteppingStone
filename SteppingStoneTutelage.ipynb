{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements.\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from ExperimentManager import Experiment\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as tdist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a brief description of this experiment:\n",
      "All sin, Hypers: (40, 1, agent_hyper, 1/8, 1/4, 5)\n"
     ]
    }
   ],
   "source": [
    "manager = Experiment.start_experiment('experiments2/', 'experiment', print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates network weights.\n",
    "def generate_weights(starting_size, ending_size, weights_needed):\n",
    "    difference = (starting_size - ending_size) / (weights_needed + 1)\n",
    "    weights = []\n",
    "    for i in range(weights_needed):\n",
    "        weights.append(int(starting_size - (difference * (i+1))))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy recommendor network.\n",
    "class POLICY_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, output_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layers = []\n",
    "        for i in range(output_count):\n",
    "            self.output_layers.append(nn.Linear(prev_weight, output_size).to(self.t_device))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        for o in self.output_layers:\n",
    "            self.params += list(o.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if train:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "            else:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy rating network.\n",
    "class RATING_NET(nn.Module):\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, input_size, output_size, layer_count, t_device):\n",
    "        super().__init__()\n",
    "        weights = generate_weights(input_size, output_size, layer_count)\n",
    "        prev_weight = input_size\n",
    "        self.t_device = t_device\n",
    "        self.hidden_layers = []\n",
    "        for w in weights:\n",
    "            self.hidden_layers.append(nn.Linear(prev_weight, w).to(self.t_device))\n",
    "            prev_weight = w\n",
    "        self.output_layer = nn.Linear(prev_weight, output_size).to(self.t_device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = F.relu\n",
    "        self.sin = torch.sin\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.params = []\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += list(h.parameters())\n",
    "        self.params += list(self.output_layer.parameters())\n",
    "            \n",
    "    # Forward propogate input.\n",
    "    def forward(self, x, train=False):\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = self.sin(hidden(x))\n",
    "        outputs = []\n",
    "        for out in self.output_layers:\n",
    "            if train:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "            else:\n",
    "                outputs.append(self.relu(out(x)))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory agent.\n",
    "class AGENT:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, name, state_size, action_size, layer_count, step_size, learning_rate, gamma, stack_size, t_device, s_device):\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.layer_count = layer_count\n",
    "        self.step_size = step_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.stack_size = stack_size\n",
    "        self.alpha = 0.1\n",
    "        self.t_device = t_device\n",
    "        self.s_device = s_device\n",
    "        self.age = 1\n",
    "        self.policy_map = POLICY_NET(state_size * stack_size, action_size, layer_count, step_size, t_device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_map.params, lr=learning_rate)\n",
    "        self.loss_func = nn.CrossEntropyLoss()#nn.MSELoss()\n",
    "    \n",
    "    # Train policy network.\n",
    "    def train_policy_network(self, inputs, outputs, extra_info='', batch_size=1024, epochs=50):\n",
    "        batches = []\n",
    "        position = 0\n",
    "        eye = torch.eye(self.action_size)\n",
    "        batch = ([],[[] for _ in range(self.step_size)])\n",
    "        losses = []\n",
    "        while position < len(inputs):\n",
    "            batch[0].append(inputs[position])\n",
    "            for i in range(len(batch[1])):\n",
    "                #batch[1][i].append(eye[outputs[i][position]])\n",
    "                batch[1][i].append(outputs[i][position])\n",
    "            position += 1\n",
    "            if len(batch[0]) >= batch_size:\n",
    "                batches.append(batch)\n",
    "                batch = ([],[[] for _ in range(self.step_size)])\n",
    "        if len(batch) > 0:\n",
    "            batches.append(batch)\n",
    "        for e in range(epochs):\n",
    "            for i in range(len(batches)):\n",
    "                inputs = torch.stack(batches[i][0])\n",
    "                #outputs = [torch.stack(o) for o in batches[i][1]]\n",
    "                outputs = [torch.Tensor(o).long() for o in batches[i][1]]\n",
    "                out = self.policy_map(inputs, train=True)\n",
    "                loss = None\n",
    "                for j in range(len(outputs)):\n",
    "                    if loss is None:\n",
    "                        loss = self.loss_func(out[j], outputs[j])\n",
    "                    else:\n",
    "                        loss += self.loss_func(out[j], outputs[j])\n",
    "                print('\\r{} | EPOCH {}/{} | BATCH {}/{} | CURRENT BATCH COUNT {} | LOSS {:0.4f} | AGE {} {}\\t\\t'.format(self.name, e+1, epochs, i+1, len(batches), len(batches[i][0]), loss.detach().cpu().numpy(), self.age, extra_info), end='')\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                losses.append(loss.detach().cpu().numpy())\n",
    "        self.age += 1\n",
    "        return sum(losses)/len(losses)\n",
    "        \n",
    "    # Plays a game.\n",
    "    def play_game(self, env, render=False, extra_info=''):\n",
    "        done = False\n",
    "        previous_state = None\n",
    "        action = 0\n",
    "        score = 0\n",
    "        inner_score = 0 # Score inside inner steps.\n",
    "        overall_step = 0\n",
    "        step = 0\n",
    "        depth = 0\n",
    "        first_step = True\n",
    "        lives = 4\n",
    "        action_queue = None\n",
    "        groups = []\n",
    "        env.reset()\n",
    "        frames = []\n",
    "        while not done:\n",
    "            if first_step:\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = observation\n",
    "                frames.append(state)\n",
    "                tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                previous_state = tensor.detach().cpu().numpy()\n",
    "                dists = self.policy_map(tensor)\n",
    "                action_queue = []\n",
    "                for d in dists:\n",
    "                    if rand.uniform(0,1) > self.gamma or min(d) < 0 or sum(d) == 0:\n",
    "                        action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                    else:\n",
    "                        distribution = torch.distributions.categorical.Categorical(d)\n",
    "                        action_queue.append(int(distribution.sample()))\n",
    "                first_step = False\n",
    "            else:\n",
    "                action = action_queue[step]\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                inner_score += reward\n",
    "                state = observation\n",
    "                frames.append(state)\n",
    "                step += 1\n",
    "                if step == self.step_size:\n",
    "                    groups.append((previous_state, action_queue))\n",
    "                    step = 0\n",
    "                    inner_score = 0\n",
    "                    if len(frames) < self.stack_size:\n",
    "                        tensor = torch.Tensor.float(torch.from_numpy(state))\n",
    "                        tensor = torch.cat([tensor for _ in range(self.stack_size)], 0)\n",
    "                    else:\n",
    "                        tensors = [torch.Tensor.float(torch.from_numpy(f)) for f in frames[-self.stack_size:]]\n",
    "                        tensor = torch.cat(tensors, 0)\n",
    "                    previous_state = tensor.detach().cpu().numpy()\n",
    "                    dists = self.policy_map(tensor)\n",
    "                    action_queue = []\n",
    "                    try_rand = rand.uniform(0,1)\n",
    "                    for d in dists:\n",
    "                        if try_rand > self.gamma or sum(d) == 0:\n",
    "                            action_queue.append(rand.randint(0, self.action_size - 1))\n",
    "                        else:\n",
    "                            if min(d) < 0:\n",
    "                                d += abs(min(d))\n",
    "                            distribution = torch.distributions.categorical.Categorical(d)\n",
    "                            action_queue.append(int(distribution.sample()))\n",
    "            print('\\r{} | STEP {} | SCORE {} | AGE {} {}\\t\\t'.format(self.name, overall_step, score, self.age, extra_info), end = '')\n",
    "            if render:\n",
    "                env.render()\n",
    "            if info['ale.lives'] != lives or done:\n",
    "                lives = info['ale.lives']\n",
    "                previous_state = None\n",
    "                action = 0\n",
    "                inner_score = 0 # Score inside inner steps.\n",
    "                step = 0\n",
    "                depth = 0\n",
    "                first_step = True\n",
    "                action_queue = None\n",
    "            overall_step += 1\n",
    "        return groups, score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population of agents that learn from each other.\n",
    "class POPULATION:\n",
    "    \n",
    "    # Constructor.\n",
    "    def __init__(self, population_size, number_of_attempts, agent_params, teach_percent, train_percent, age_cutoff):\n",
    "        self.population_size = population_size\n",
    "        self.number_of_attempts = number_of_attempts\n",
    "        self.teach_percent = teach_percent\n",
    "        self.train_percent = train_percent\n",
    "        self.population = []\n",
    "        self.agents_created = population_size\n",
    "        self.age_cutoff = age_cutoff\n",
    "        self.agent_params = agent_params\n",
    "        for i in range(population_size):\n",
    "            agent = AGENT(f'AGENT_{i}', agent_params[0], agent_params[1], agent_params[2], agent_params[3], agent_params[4], agent_params[5], agent_params[6], agent_params[7], agent_params[8])\n",
    "            self.population.append(agent)\n",
    "        self.generation = 0\n",
    "        \n",
    "    # Converts a list of trajectories into valid inputs and outputs for training.\n",
    "    def convert_to_training_data(self, trajectories):\n",
    "        step_size = len(trajectories[0][1])\n",
    "        inputs = []\n",
    "        outputs = [[] for _ in range(step_size)]\n",
    "        for t in trajectories:\n",
    "            inputs.append(torch.Tensor.float(torch.from_numpy(t[0])))\n",
    "            for i in range(step_size):\n",
    "                outputs[i].append(t[1][i])\n",
    "        return inputs, outputs\n",
    "    \n",
    "    # Replaces the given agent with a new agent that is returned.\n",
    "    def replace_agent(self, agent):\n",
    "        for i in range(len(self.population)):\n",
    "            if agent.name == self.population[i].name:\n",
    "                new_agent = AGENT(f'AGENT_{self.agents_created}', self.agent_params[0], self.agent_params[1], self.agent_params[2], self.agent_params[3], self.agent_params[4], self.agent_params[5], self.agent_params[6], self.agent_params[7], self.agent_params[8])\n",
    "                self.population[i] = new_agent\n",
    "                self.agents_created += 1\n",
    "                return new_agent\n",
    "        return agent\n",
    "        \n",
    "    # Runs and trains the agents.\n",
    "    def run_population(self, env, render=False):\n",
    "        new_pop = []\n",
    "        total_score = 0\n",
    "        high_score = None\n",
    "        low_score = None\n",
    "        manager.print('BEGIN RUNNING POPULATION | GENERATION {}'.format(self.generation))\n",
    "        rand.shuffle(self.population)\n",
    "        for agent in self.population:\n",
    "            candidate_runs = []\n",
    "            for g in range(self.number_of_attempts):\n",
    "                groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "                total_score += score\n",
    "                candidate_runs.append((groups, score))\n",
    "                if high_score is None or high_score < score:\n",
    "                    high_score = score\n",
    "                if low_score is None or low_score > score:\n",
    "                    low_score = score\n",
    "            candidate_runs.sort(key = lambda x: x[1], reverse=True)\n",
    "            new_pop.append((agent, candidate_runs[0][0], candidate_runs[0][1]))\n",
    "            \n",
    "            #all_groups = []\n",
    "            #agent_score = 0\n",
    "            #for g in range(self.number_of_attempts):\n",
    "            #    groups, score = agent.play_game(env, render, f'| MEMBER {len(new_pop) + 1}/{len(self.population)} | GAME {g+1}/{self.number_of_attempts}')\n",
    "            #    all_groups += groups\n",
    "            #    agent_score += score\n",
    "            #    total_score += score\n",
    "            #    if high_score is None or high_score < score:\n",
    "            #        high_score = score\n",
    "            #    if low_score is None or low_score > score:\n",
    "            #        low_score = score\n",
    "            #new_pop.append((agent, all_groups, agent_score / self.number_of_attempts))\n",
    "        print('\\n')\n",
    "        manager.print('END RUNNING POPULATION | AVERAGE SCORE {} | LOW SCORE {} | HIGH SCORE {}'.format(total_score / (len(new_pop) * self.number_of_attempts), low_score, high_score))\n",
    "        new_pop.sort(key = lambda x: x[2], reverse=True)\n",
    "        teach_pop = new_pop[:int(len(new_pop) * self.teach_percent)]\n",
    "        train_pop = new_pop[-int(len(new_pop) * self.train_percent):]\n",
    "        examples = []\n",
    "        for exp in teach_pop:\n",
    "            examples += exp[1]\n",
    "        manager.print('BEGIN TRAINING POPULATION')\n",
    "        count = 0\n",
    "        losses = []\n",
    "        for train in train_pop:\n",
    "            agent = train[0]\n",
    "            if agent.age > self.age_cutoff and rand.uniform(0,1) > 0.5:\n",
    "                agent = self.replace_agent(agent)\n",
    "            trajectories = []\n",
    "            for _ in range(int(len(examples) / 2)):\n",
    "                index = rand.randint(0, len(examples) - 1)\n",
    "                trajectories.append(examples[index])\n",
    "            inputs, outputs = self.convert_to_training_data(trajectories)\n",
    "            loss = agent.train_policy_network(inputs, outputs, extra_info=f'| MEMBER {count+1}/{len(train_pop)}', batch_size=1024, epochs=50)\n",
    "            losses.append(loss)\n",
    "            count += 1\n",
    "        print('\\n')\n",
    "        manager.print('END TRAINING POPULATION | AVG LOSS {}'.format(sum(losses)/len(losses)))\n",
    "        self.generation += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_hyper = (128, 14, 10, 2, 0.001, 0.95, 5, torch.device('cpu'), torch.device('cpu'))\n",
    "population = POPULATION(40, 1, agent_hyper, 1/8, 1/4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('KungFuMaster-ram-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN RUNNING POPULATION | GENERATION 0\n",
      "AGENT_8 | STEP 1414 | SCORE 700.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 775.0 | LOW SCORE 0.0 | HIGH SCORE 3800.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_21 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 205 | LOSS 5.2533 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.218355217933655\n",
      "BEGIN RUNNING POPULATION | GENERATION 1\n",
      "AGENT_13 | STEP 2614 | SCORE 3200.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1055.0 | LOW SCORE 0.0 | HIGH SCORE 3200.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_22 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 425 | LOSS 5.3631 | AGE 2 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.240445085207621\n",
      "BEGIN RUNNING POPULATION | GENERATION 2\n",
      "AGENT_0 | STEP 1513 | SCORE 1600.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1237.5 | LOW SCORE 0.0 | HIGH SCORE 4700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_15 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 403 | LOSS 5.1263 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.2079695854187005\n",
      "BEGIN RUNNING POPULATION | GENERATION 3\n",
      "AGENT_9 | STEP 1320 | SCORE 700.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1482.5 | LOW SCORE 300.0 | HIGH SCORE 3900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_20 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 530 | LOSS 5.3149 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.2914565041859944\n",
      "BEGIN RUNNING POPULATION | GENERATION 4\n",
      "AGENT_27 | STEP 1530 | SCORE 1200.0 | AGE 1 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1515.0 | LOW SCORE 200.0 | HIGH SCORE 3700.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_19 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 568 | LOSS 5.1594 | AGE 1 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.2563341566721595\n",
      "BEGIN RUNNING POPULATION | GENERATION 5\n",
      "AGENT_18 | STEP 1751 | SCORE 1400.0 | AGE 2 | MEMBER 40/40 | GAME 1/1\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1762.5 | LOW SCORE 500.0 | HIGH SCORE 4000.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_14 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 695 | LOSS 5.2660 | AGE 2 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.179624985059101\n",
      "BEGIN RUNNING POPULATION | GENERATION 6\n",
      "AGENT_3 | STEP 1405 | SCORE 200.0 | AGE 3 | MEMBER 40/40 | GAME 1/1\t\t\t\t\n",
      "\n",
      "END RUNNING POPULATION | AVERAGE SCORE 1512.5 | LOW SCORE 200.0 | HIGH SCORE 3900.0\n",
      "BEGIN TRAINING POPULATION\n",
      "AGENT_3 | EPOCH 50/50 | BATCH 3/3 | CURRENT BATCH COUNT 520 | LOSS 5.2161 | AGE 3 | MEMBER 10/10\t\t\t\n",
      "\n",
      "END TRAINING POPULATION | AVG LOSS 5.247429793039958\n",
      "BEGIN RUNNING POPULATION | GENERATION 7\n",
      "AGENT_10 | STEP 814 | SCORE 400.0 | AGE 2 | MEMBER 1/40 | GAME 1/1\t\t"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    population.run_population(env, True)\n",
    "    manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
